{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RAI introduction","text":"<p>RAI is a flexible agentic framework for developing and deploying Embodied AI features for your robots.</p> <p>RAI enables you to control your robots using natural language and helps them perceive and understand the world with AI. It integrates seamlessly with your current robotics stack to enhance your robots' capabilities.</p> <p></p>"},{"location":"#rai-framework","title":"RAI Framework","text":"<p>The RAI Framework provides a comprehensive, end-to-end solution for developing and deploying sophisticated AI-powered robotic systems. It supports the full lifecycle of embodied AI development, from initial configuration and testing to deployment and continuous improvement.</p> <p>Our suite of integrated packages enables developers to seamlessly transition from concept to production, offering:</p> <ul> <li>Complete Development Lifecycle: From initial agent development to deployment</li> <li>Modular Architecture: Choose and combine components based on your specific needs</li> <li>Production-Ready Tools: Enterprise-grade packages for simulation, testing, and deployment</li> <li>Extensible Platform: Easy integration with existing robotics infrastructure and custom solutions</li> <li>Advanced Human-Robot Interaction: Through text, speech, and multimodal interfaces</li> <li>Rich Multimodal Capabilities: Seamless integration of voice, vision, and sensor data with real-time processing of multiple input/output streams, native handling of diverse data types, and unified multi-sensory perception and action framework.</li> </ul> <p>The framework's components work in perfect harmony to deliver a robust foundation for your robotics projects:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive into RAI? Start with a quick-setup guide.</p> <p>Here are two ways to begin your journey:</p>"},{"location":"#option-1-try-our-demos","title":"Option 1: Try Our Demos","text":"<p>Experience RAI in action through our interactive demos. These showcase real-world applications across different robotic platforms:</p> <ul> <li>\ud83e\udd16 Manipulation Tasks - Watch RAI control a Franka Panda arm using natural language</li> <li>\ud83d\ude97 Autonomous Navigation - Explore RAI's capabilities with the ROSbot XL platform</li> <li>\ud83d\ude9c Agricultural Robotics - See how RAI handles complex decision-making in orchard environments</li> </ul>"},{"location":"#option-2-build-your-own-solution","title":"Option 2: Build Your Own Solution","text":"<p>Follow our comprehensive walkthrough to:</p> <ul> <li>Deploy RAI on your robot and enable natural language interactions</li> <li>Extend the framework with custom tools and capabilities</li> <li>Implement complex, multi-step tasks using RAI's advanced reasoning</li> </ul>"},{"location":"#communication-protocols","title":"Communication Protocols","text":"<p>RAI provides first-class support for ROS 2 Humble and Jazzy distributions. While ROS 2 serves as our Tier 1 communication protocol, RAI's architecture includes a powerful abstraction layer that:</p> <ul> <li>Simplifies communication across different networks and protocols</li> <li>Enables seamless integration with various communication backends</li> <li>Allows for future protocol extensions while maintaining a consistent interface</li> </ul> <p>This design philosophy means that while RAI is fully compatible with ROS 2, most of its features can be utilized independently of the ROS 2 environment. The framework's modular architecture makes it suitable not only for different robotic platforms but also for non-robotic applications, offering flexibility in deployment across various domains.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>You are welcome to contribute to RAI! Please see our Contribution Guide.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find our work helpful for your research, please consider citing the following BibTeX entry.</p> <pre><code>@misc{rachwa\u01422025raiflexibleagentframework,\n      title={RAI: Flexible Agent Framework for Embodied AI},\n      author={Kajetan Rachwa\u0142 and Maciej Majek and Bart\u0142omiej Boczek and Kacper D\u0105browski and Pawe\u0142 Liberadzki and Adam D\u0105browski and Maria Ganzha},\n      year={2025},\n      eprint={2505.07532},\n      archivePrefix={arXiv},\n      primaryClass={cs.MA},\n      url={https://arxiv.org/abs/2505.07532},\n}\n</code></pre> <p>Want to know more?</p> RAI at ROSCon 2024 <p></p> RAI demos at ROSCon 2024 <p></p>"},{"location":"#community","title":"Community","text":""},{"location":"#embodied-ai-community-group","title":"Embodied AI Community Group","text":"<p>RAI is one of the main projects in focus of the Embodied AI Community Group. If you would like to join the next meeting, look for it in the ROS Community Calendar.</p>"},{"location":"#rai-faq","title":"RAI FAQ","text":"<p>Please take a look at FAQ.</p>"},{"location":"api_design_considerations/","title":"API Usability Considerations for RAI Framework","text":"<p>This document synthesizes research on API usability and LLM-compatible API design, providing information and perspectives to RAI core developers and maintainers as they explore API designs that serve multiple audiences: Application Developers, Extension Developers, Core Developers, and LLM agents. This is an active exploration, not a finalized set of rules.</p>"},{"location":"api_design_considerations/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Classic Usability Dimensions</li> <li>LLM App Design Patterns for RAI</li> <li>RAI Framework Audience Analysis</li> <li>Tiered API Design for RAI<ul> <li>Three-Tier Structure</li> <li>Key Design Considerations</li> <li>Case Study: rai_perception Implementation</li> <li>Common Usability Concerns in RAI extension</li> </ul> </li> <li>Research Findings and Practical Takeaways</li> <li>Reading List</li> </ul>"},{"location":"api_design_considerations/#classic-usability-dimensions","title":"Classic Usability Dimensions","text":"<p>The Cognitive Dimensions Framework provides a systematic way to evaluate API design. Key dimensions relevant to RAI API evaluation:</p> <ul> <li>Abstraction Level: Range of abstraction exposed and usable by target developers</li> <li>Progressive Evaluation: Ability to test partially completed code</li> <li>Penetrability: Ease of exploring and understanding API components</li> <li>Consistency: How much can be inferred once part of the API is learned</li> <li>Domain Correspondence: How clearly API components map to the robotics domain</li> <li>Role Expressiveness: How apparent the relationship between components and the program is</li> </ul> <p>Key findings from empirical studies:</p> <ul> <li>48% of developers had to understand implementation details to use APIs (Piccioni et al., 2013)</li> <li>Method placement dramatically affects learnability\u2014developers were 2-11x faster when methods were on expected classes (Stylos &amp; Myers, 2008)</li> <li>High abstraction levels improve usability but reduce control\u2014the ideal level depends on task and audience (Diprose et al., 2016)</li> </ul> <p>Progressive Disclosure Pattern: Progressive disclosure (Nielsen, 1995) defers advanced features to reduce cognitive load. For APIs, this translates to tiered or layered API design: start with simple, intent-based methods, reveal complexity progressively as needed, and maintain consistent patterns across tiers. For technical contexts, \"layered API design\" or \"tiered abstraction\" is preferred over UX terminology.</p>"},{"location":"api_design_considerations/#llm-app-design-patterns-for-rai","title":"LLM App Design Patterns for RAI","text":"<p>From emerging LLM API design research (2023-2025), patterns that may be useful for evaluating RAI APIs across all extensions:</p> <p>Semantic Clarity:</p> <ul> <li>Use self-descriptive field names (e.g., <code>temperature_celsius</code> not <code>temp</code>)</li> <li>Provide rich metadata (units, data types, relationships)</li> <li>Make responses easily interpretable without complex parsing</li> </ul> <p>Error Handling:</p> <ul> <li>Return actionable error messages with suggestions</li> <li>Include context about what went wrong and how to fix it</li> <li>Enable self-correction and intelligent retry</li> </ul> <p>Granularity Balance:</p> <ul> <li>Avoid overly broad APIs (inefficient data transfer)</li> <li>Avoid overly fine-grained APIs (too many calls)</li> <li>Strike balance for single-call efficiency with composability</li> </ul> <p>Self-Descriptive Design:</p> <ul> <li>Use endpoint/method names that create natural language understanding</li> <li>Example: <code>dataProcessing/endpoint</code> rather than generic names</li> </ul> <p>Structured Responses:</p> <ul> <li>Provide structured, machine-readable data</li> <li>Minimize LLM parsing and inference overhead</li> <li>Focus LLM effort on reasoning, not data extraction</li> </ul> <p>These patterns may help evaluate whether RAI APIs are optimized for both human developers and LLM agents.</p>"},{"location":"api_design_considerations/#rai-framework-audience-analysis","title":"RAI Framework Audience Analysis","text":""},{"location":"api_design_considerations/#multiple-audiences-with-clear-separation","title":"Multiple Audiences with Clear Separation","text":"<p>The RAI framework paper (Rachwa\u0142 et al., 2025) states Tools are \"compatible with langchain, enabling seamless integration with tool-calling-enabled LLMs\" while also being \"used by Agents utilizing other decision-making mechanisms.\" Based on this paper, RAI serves four distinct roles at different architectural levels:</p> <ul> <li>Application Developers: Design and configure the system (choose Agents, Connectors, Tools)</li> <li>Extension Developers: Extend RAI by creating custom tools and components</li> <li>Core Developers: Implement new framework components (connectors, agents)</li> <li>LLM agents: Consume Tools at runtime via tool-calling mechanisms</li> </ul>"},{"location":"api_design_considerations/#api-similarities-differences","title":"API Similarities &amp; Differences","text":"<p>All roles interact with the Tools abstraction and access the same underlying capabilities (ROS 2 services, perception, navigation). However, their interaction patterns differ:</p> <ul> <li>Application Developers: Configure Agents (Conversational, StateBased), Connectors (ROS2Connector), and system architecture</li> <li>Extension Developers: Create custom tools extending <code>BaseROS2Tool</code>, implement domain-specific functionality</li> <li>Core Developers: Implement new connector types, agent types, framework infrastructure</li> <li>LLM agents: Invoke Tools at runtime (CallROS2Service, GetDistanceToObjects)</li> </ul> <p>Application Developers architect the system; LLM agents execute within it. Extension and Core Developers extend the framework itself. RAI maintains clear separation: Application Developers configure, LLM agents operate, Extension/Core Developers extend. However, APIs need to serve multiple audiences simultaneously, which benefits from tiered design that supports progressive disclosure.</p>"},{"location":"api_design_considerations/#use-case-differences","title":"Use Case Differences","text":"<p>Application Developers: System design and configuration, debugging, multi-agent setup decisions</p> <p>Extension Developers: Tool creation (extending <code>BaseROS2Tool</code>), component extension, custom aggregators</p> <p>Core Developers: Framework infrastructure, new connector/agent types, low-level integration</p> <p>LLM agents: Runtime tool invocation, task execution, decision-making during task performance</p>"},{"location":"api_design_considerations/#research-opportunities-for-rai","title":"Research Opportunities for RAI","text":"<p>RAI's multi-audience architecture and tiered API design present novel research opportunities:</p> <ol> <li>Tiered APIs for dual human/LLM audiences: No formal research exists on tiered APIs serving both human developers and LLM agents simultaneously</li> <li>Empirical studies of LLM agent API usage: Limited empirical work exists on how LLM agents interact with APIs</li> <li>Progressive disclosure patterns in APIs: While established in UI design, it hasn't been formalized for programmatic interfaces</li> <li>Frameworks for evaluating multi-audience robotics APIs: No established frameworks exist for evaluating robotics APIs that serve multiple audiences with different needs</li> </ol> <p>RAI could be among the first to empirically study how LLM agents navigate tiered APIs vs. how humans do, and whether a single design can serve both well across robotics domains.</p>"},{"location":"api_design_considerations/#tiered-api-design-for-rai","title":"Tiered API Design for RAI","text":"<p>A tiered API structure could organize code into three abstraction levels to support progressive disclosure and reduce cognitive load. This pattern might apply across RAI extensions (perception, navigation, manipulation, etc.).</p> <p>Note: The <code>rai_perception</code> extension has implemented a tiered API structure as an exploration of these design principles. See <code>rethinking_usability.md</code> for detailed analysis and implementation examples.</p>"},{"location":"api_design_considerations/#three-tier-structure","title":"Three-Tier Structure","text":"<p>High-level layer (Agent-friendly): Simple, intent-based tools with minimal required arguments. Tools hide pipeline complexity, algorithm selection, and configuration details. For example, <code>GetObjectGrippingPointsTool(object_name=\"cup\")</code> only requires the object name\u2014camera topics, filter configs, and estimation strategies are handled via ROS2 parameters or sensible defaults. Agents don't need to understand the pipeline or choose between <code>isolation_forest</code> vs <code>dbscan</code> strategies.</p> <p>Mid-level layer (Configurable): Configurable components that expose key parameters for tuning behavior. Components allow users to configure strategies and methods without implementing algorithms from scratch. For example, <code>PointCloudFilter</code> and <code>GrippingPointEstimator</code> with their Config classes (<code>PointCloudFilterConfig</code>, <code>GrippingPointEstimatorConfig</code>) allow configuring filtering strategies and estimation methods without understanding DBSCAN or Isolation Forest internals.</p> <p>Low-level layer (Expert control): Core algorithms providing direct access to model inference and processing stages. For example, detection algorithms (e.g., <code>GDBoxer</code>), segmentation algorithms (e.g., <code>GDSegmenter</code>), and processing functions (e.g., <code>depth_to_point_cloud</code>) for users who need full control over every parameter.</p>"},{"location":"api_design_considerations/#key-design-considerations","title":"Key Design Considerations","text":"<ol> <li> <p>Named presets over raw parameters: High-level tools could support semantic presets (e.g., <code>quality=\"high\"</code>, <code>approach=\"top_down\"</code>) that internally map to appropriate component configurations, rather than exposing all algorithm parameters.</p> </li> <li> <p>Semantic parameter names: Mid-level components might expose parameters that describe outcomes (e.g., <code>noise_handling=\"aggressive\"</code>) rather than algorithm names (e.g., <code>strategy=\"isolation_forest\"</code>).</p> </li> <li> <p>Rich result metadata: Tools could return confidence scores, strategy used, and alternative options to help LLM agents make better decisions about retrying or adjusting approaches.</p> </li> <li> <p>Progressive evaluation support: Users might benefit from being able to test individual pipeline stages without running the full pipeline, enabling incremental debugging and validation.</p> </li> </ol>"},{"location":"api_design_considerations/#case-study-rai_perception-implementation","title":"Case Study: rai_perception Implementation","text":"<p>The refactored <code>rai_perception</code> extension provides a concrete implementation of tiered API design principles, demonstrating how theoretical considerations translate to practice. Key implementations include:</p> <p>Progressive Disclosure:</p> <ul> <li>Three-tier structure: Tools (<code>tools/</code>), Components (<code>components/</code>), Algorithms (<code>algorithms/</code>)</li> <li>Semantic presets (<code>perception_presets.py</code>) with self-documenting names: <code>\"default_grasp\"</code>, <code>\"precise_grasp\"</code>, <code>\"top_grasp\"</code></li> </ul> <p>Configuration Management:</p> <ul> <li>Multi-tier configuration: algorithm configs (model registry), ROS2 parameters (deployment), component configs (Pydantic classes), presets (semantic mappings)</li> <li>Helper function <code>rai.communication.ros2.get_param_value()</code> for consistent parameter extraction with automatic type conversion</li> </ul> <p>Domain Correspondence:</p> <ul> <li>Semantic parameter names: <code>outlier_fraction</code> instead of <code>if_contamination</code>, <code>neighborhood_size</code> instead of <code>lof_n_neighbors</code></li> <li>Domain-oriented strategy names: <code>\"aggressive_outlier_removal\"</code> instead of <code>\"isolation_forest\"</code>, <code>\"density_based\"</code> instead of <code>\"dbscan\"</code></li> </ul> <p>Consistency:</p> <ul> <li>Input schema naming pattern: <code>{ToolName}Input</code> (e.g., <code>GetObjectGrippingPointsToolInput</code>)</li> <li>Standardized parameter handling: <code>_load_parameters()</code> method with consistent prefixes (<code>perception.gripping_points.*</code>)</li> </ul> <p>Role Expressiveness:</p> <ul> <li>Pipeline visibility: Tools expose <code>pipeline_stages</code> attributes and <code>get_pipeline_info()</code> methods</li> <li>Service dependency clarity: <code>required_services</code> attributes and <code>check_service_dependencies()</code> methods</li> </ul> <p>Progressive Evaluation:</p> <ul> <li>Debug mode (<code>debug=True</code>) publishes intermediate results to ROS2 topics for RVIZ visualization</li> <li>Enables inspection of pipeline stages without code modification</li> </ul> <p>Model Registry Pattern:</p> <ul> <li>Enables switching between detection models (e.g., GroundingDINO, YOLO) via ROS2 parameters</li> <li>No code changes required\u2014service reads <code>model_name</code> parameter and queries registry</li> </ul> <p>These implementations demonstrate how tiered API design addresses usability concerns identified in research while maintaining flexibility for expert users. The <code>rai_perception</code> exploration serves as a reference implementation for other RAI extensions considering similar refactoring.</p>"},{"location":"api_design_considerations/#common-usability-concerns-in-rai-extension","title":"Common Usability Concerns in RAI extension","text":"<p>Based on analysis of RAI extensions, common usability issues may include:</p> <ol> <li> <p>Configuration complexity: Tools requiring 6+ ROS2 parameters plus multiple configuration objects with 10+ parameters each create high cognitive load.</p> </li> <li> <p>Algorithm knowledge requirement: Mid-level users must understand domain-specific concepts (DBSCAN, RANSAC, path planning algorithms) to configure effectively.</p> </li> <li> <p>Hidden dependencies: Tool initialization depends on ROS2 parameters that must be set before tool creation\u2014no clear error if missing.</p> </li> <li> <p>Pipeline complexity: Tools orchestrate multi-stage pipelines (detection \u2192 processing \u2192 filtering \u2192 estimation) with no visibility into intermediate stages.</p> </li> <li> <p>Progressive evaluation difficulty: Cannot test individual pipeline stages\u2014must run full pipeline to see results.</p> </li> <li> <p>Parameter discovery: Configuration options scattered across multiple config classes\u2014no single source of truth for all parameters.</p> </li> <li> <p>Error messages: Algorithm-specific errors may not provide actionable guidance for LLM agents or application developers.</p> </li> <li> <p>Domain correspondence gap: Parameter names like <code>if_contamination</code>, <code>lof_n_neighbors</code> don't clearly map to domain concepts.</p> </li> <li> <p>Code duplication: Infrastructure-level duplication (service client creation, message retrieval, parameter handling) creates maintenance burden and inconsistent error handling.</p> </li> </ol>"},{"location":"api_design_considerations/#research-findings-and-practical-takeaways","title":"Research Findings and Practical Takeaways","text":""},{"location":"api_design_considerations/#practical-takeaways-for-rai-core-developers","title":"Practical Takeaways for RAI Core Developers","text":"<ol> <li> <p>Tiered approach aligns with research: High-level primitives work well but abstraction may need to preserve necessary control. This aligns with RAI's tiered API structure across extensions.</p> </li> <li> <p>Method placement matters: Place methods on classes where developers naturally start exploring. Research suggests this can make APIs 2-11x faster to learn.</p> </li> <li> <p>Documentation is critical: All major usability flaws trace to incomplete/unclear documentation. Method signatures, comments, and contracts should be comprehensive.</p> </li> <li> <p>Address abstraction gaps: If 48% of users need to peek at implementation details, the abstraction level needs adjustment. Provide clear paths between tiers without requiring implementation knowledge. The <code>rai_perception</code> implementation addresses this through semantic presets and discoverable component relationships.</p> </li> <li> <p>Enable progressive evaluation: High abstraction levels make progressive evaluation difficult. Allow testing of individual stages without running full pipelines. <code>rai_perception</code> implements debug mode that publishes intermediate results to ROS2 topics, enabling visualization without code changes.</p> </li> <li> <p>Semantic clarity for LLM agents: Use self-descriptive names, structured responses, and actionable error messages. <code>rai_perception</code> demonstrates this through semantic parameter names (<code>outlier_fraction</code> vs <code>if_contamination</code>) and preset names that are self-documenting.</p> </li> </ol>"},{"location":"api_design_considerations/#reading-list","title":"Reading List","text":"<ol> <li> <p>Diprose et al. (2016) - \"Designing an API at an appropriate abstraction level for programming social robot applications\" - https://www.cs.auckland.ac.nz/~beryl/publications/jvlc%202016%20Designing%20API.pdf    Abstraction/flexibility trade-off in robotics. High-level primitives work well but shouldn't remove necessary control.</p> </li> <li> <p>Piccioni, Furia &amp; Meyer (2013) - \"An Empirical Study of API Usability\" - https://bugcounting.net/pubs/esem13.pdf    Found 48% of participants had to understand implementation details to use APIs, revealing abstraction deficiencies.</p> </li> <li> <p>Stylos &amp; Myers (2008) - \"The implications of method placement on API learnability\" - https://www.cs.cmu.edu/~NatProg/papers/FSE2008-p105-stylos.pdf    Method placement dramatically affects learnability\u2014developers were 2-11x faster when methods were on expected classes.</p> </li> <li> <p>Clarke &amp; Becker (2003) - \"Using the Cognitive Dimensions Framework to evaluate the usability of a class library\" - https://www.ppig.org/files/2003-PPIG-15th-clarke.pdf</p> </li> <li> <p>Gravitee (2025) - \"Designing APIs for LLM Apps\" - https://www.gravitee.io/blog/designing-apis-for-llm-apps    AI-ready APIs require structured, semantically clear, and context-aware designs.</p> </li> <li> <p>Jakob Nielsen (2006) - \"Progressive disclosure\" - https://www.nngroup.com/articles/progressive-disclosure/    Defers advanced features to reduce cognitive load\u2014applies to tiered API design.</p> </li> <li> <p>Rachwa\u0142 et al. (2025) - \"RAI: Flexible Agent Framework for Embodied AI\" - https://arxiv.org/abs/2505.07532    Introduces the RAI framework for creating embodied Multi-Agent Systems for robotics, with integration for ROS 2, Large Language Models, and simulations. Describes the framework's architecture, tools, and mechanisms for agent embodiment.</p> </li> </ol>"},{"location":"API_documentation/overview/","title":"RAI API Documentation","text":""},{"location":"API_documentation/overview/#introduction","title":"Introduction","text":"<p>rai core provides a comprehensive set of tools and components for developing intelligent robotic systems powered by multimodal LLMs. This package bridges the gap between advanced AI capabilities and robotic platforms, enabling natural language understanding, reasoning, and multimodal interactions in robotic applications.</p>"},{"location":"API_documentation/overview/#core-components","title":"Core Components","text":"<p>RAI consists of several key components that work together to create intelligent robotic systems:</p> Component Description Agents Agents are the central components that encapsulate specific functionalities and behaviors. Connectors Connectors provide a unified way to interact with various communication systems e.g., ROS 2. Aggregators Aggregators collect and process messages from various sources, transforming them into summarized or analyzed information. Runners Manage the lifecycle of agents. <p>On top of that, RAI implements two major integrations: ROS 2 and LangChain.</p> Component Description ROS 2 Integration RAI provides a set of tools to interact with ROS 2. LangChain Integration RAI leverages LangChain to bridge the gap between large language models and robotic systems. RAI extends LangChain with seamless multimodal message support."},{"location":"API_documentation/overview/#getting-started","title":"Getting Started","text":"<p>For practical examples and tutorials on using RAI, refer to the tutorials section. The API documentation provides detailed information about each component, its purpose, and usage patterns.</p>"},{"location":"API_documentation/overview/#best-practices","title":"Best Practices","text":"<p>When working with RAI:</p> <ol> <li>Design agents with clear responsibilities and interfaces</li> <li>Use appropriate connectors for your target platforms</li> <li>Leverage aggregators to process complex sensor data</li> <li>Follow established patterns for tool development</li> <li>Consider performance implications for real-time robotic applications</li> </ol>"},{"location":"API_documentation/agents/overview/","title":"Agents","text":""},{"location":"API_documentation/agents/overview/#overview","title":"Overview","text":"<p>Agents in RAI are modular components that encapsulate specific functionalities and behaviors. They follow a consistent interface defined by the <code>BaseAgent</code> class and can be combined to create complex robotic systems.</p>"},{"location":"API_documentation/agents/overview/#baseagent","title":"BaseAgent","text":"<p><code>BaseAgent</code> is the abstract base class for all agent implementations in the RAI framework. It defines the minimal interface that all agents must implement while providing common functionality like logging.</p>"},{"location":"API_documentation/agents/overview/#class-definition","title":"Class Definition","text":"BaseAgent class definition"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent","title":"<code>rai.agents.base.BaseAgent</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>rai/agents/base.py</code> <pre><code>class BaseAgent(ABC):\n    def __init__(self):\n        \"\"\"Initializes a new agent instance and sets up logging with the class name.\"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def run(self):\n        \"\"\"Starts the agent's main execution loop.\n        In some cases, concrete run implementation may not be needed.\n        In that case use pass as a placeholder.\"\"\"\n        pass\n\n    @abstractmethod\n    def stop(self):\n        \"\"\"Gracefully terminates the agent's execution and cleans up resources.\"\"\"\n        pass\n</code></pre>"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new agent instance and sets up logging with the class name.</p> Source code in <code>rai/agents/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a new agent instance and sets up logging with the class name.\"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Starts the agent's main execution loop. In some cases, concrete run implementation may not be needed. In that case use pass as a placeholder.</p> Source code in <code>rai/agents/base.py</code> <pre><code>@abstractmethod\ndef run(self):\n    \"\"\"Starts the agent's main execution loop.\n    In some cases, concrete run implementation may not be needed.\n    In that case use pass as a placeholder.\"\"\"\n    pass\n</code></pre>"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Gracefully terminates the agent's execution and cleans up resources.</p> Source code in <code>rai/agents/base.py</code> <pre><code>@abstractmethod\ndef stop(self):\n    \"\"\"Gracefully terminates the agent's execution and cleans up resources.\"\"\"\n    pass\n</code></pre>"},{"location":"API_documentation/agents/overview/#purpose","title":"Purpose","text":"<p>The <code>BaseAgent</code> class serves as the cornerstone of RAI's agent architecture, establishing a uniform interface for various agent implementations. This enables:</p> <ul> <li>Consistent lifecycle management (starting/stopping)</li> <li>Standardized logging mechanisms</li> <li>Interoperability between different agent types</li> <li>Integration with management utilities like <code>AgentRunner</code></li> </ul>"},{"location":"API_documentation/agents/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Management: Always clean up resources in the <code>stop()</code> method</li> <li>Thread Safety: Use locks for shared resources when implementing multi-threaded agents</li> <li>Error Handling: Implement proper exception handling in long-running agent threads</li> <li>Logging: Use the provided <code>self.logger</code> for consistent logging</li> <li>Graceful Shutdown: Handle interruptions and cleanup properly</li> </ol>"},{"location":"API_documentation/agents/overview/#architecture","title":"Architecture","text":"<p>In the RAI framework, agents typically interact with:</p> <ul> <li>Connectors: For communication (ROS2, audio devices, etc.)</li> <li>Aggregators: For processing and summarizing input data</li> <li>Models: For AI capabilities (LLMs, vision models, speech recognition)</li> <li>Tools: For implementing specific actions an agent can take</li> </ul>"},{"location":"API_documentation/agents/overview/#see-also","title":"See Also","text":"<ul> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/","title":"ROS2 Aggregators","text":"<p>RAI provides several specialized aggregators for ROS2 messages:</p> Aggregator Description Example Usage <code>ROS2LogsAggregator</code> Processes ROS2 log messages, removing duplicates while maintaining order <code>aggregator = ROS2LogsAggregator()</code> <code>ROS2GetLastImageAggregator</code> Returns the most recent image from the buffer as a base64-encoded string <code>aggregator = ROS2GetLastImageAggregator()</code> <code>ROS2ImgVLMDescriptionAggregator</code> Uses a Vision Language Model to analyze and describe the most recent image <code>aggregator = ROS2ImgVLMDescriptionAggregator(llm=chat_model)</code> <code>ROS2ImgVLMDiffAggregator</code> Compares multiple images (first, middle, and last) to identify changes over time <code>aggregator = ROS2ImgVLMDiffAggregator(llm=chat_model)</code>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/#usage-in-state-based-agents","title":"Usage in State-Based Agents","text":"<p>Aggregators are typically used in state-based agents to maintain and update the agent's state:</p> <pre><code>config = StateBasedConfig(\n    aggregators={\n        (\"/camera/camera/color/image_raw\", \"sensor_msgs/msg/Image\"): [\n            ROS2ImgVLMDiffAggregator()\n        ],\n        \"/rosout\": [\n            ROS2LogsAggregator()\n        ]\n    }\n)\n\nagent = ROS2StateBasedAgent(\n    config=config,\n    target_connectors={\"to_human\": hri_connector},\n    tools=tools\n)\n</code></pre>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/#direct-registration-via-ros2connector","title":"Direct Registration via ROS2Connector","text":"<p>Aggregators can also be registered directly with a connector using the <code>register_callback</code> method. This allows for more flexible message processing outside of state-based agents:</p> <pre><code># Create a connector\nconnector = ROS2Connector()\n\n# Create an aggregator\nimage_aggregator = ROS2GetLastImageAggregator()\n\n# Register the aggregator as a callback for a specific topic\nconnector.register_callback(\n    topic=\"/camera/camera/color/image_raw\",\n    msg_type=\"sensor_msgs/msg/Image\",\n    callback=image_aggregator\n)\n\n# The aggregator will now process all messages received on the topic\n# You can retrieve the aggregated result at any time\naggregated_message = image_aggregator.get()\n</code></pre> <p>This approach is useful when you need to:</p> <ul> <li>Process messages from specific topics independently</li> <li>Combine multiple aggregators for the same topic</li> <li>Use aggregators in non-state-based agents</li> <li>Have more control over when aggregation occurs</li> </ul>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/#see-also","title":"See Also","text":"<ul> <li>Aggregators Overview: For more information on the base aggregator class</li> <li>Agents: For more information on the different types of agents in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/aggregators/overview/","title":"Aggregators","text":""},{"location":"API_documentation/aggregators/overview/#overview","title":"Overview","text":"<p>Aggregators in RAI are components that collect and process messages from various sources, transforming them into summarized or analyzed information. They are particularly useful in state-based agents where they help maintain and update the agent's state through periodic aggregation.</p>"},{"location":"API_documentation/aggregators/overview/#baseaggregator","title":"BaseAggregator","text":"<p><code>BaseAggregator</code> is the abstract base class for all aggregator implementations in the RAI framework. It provides a generic interface for collecting and processing messages of a specific type.</p>"},{"location":"API_documentation/aggregators/overview/#class-definition","title":"Class Definition","text":""},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator","title":"<code>rai.aggregators.base.BaseAggregator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Interface for aggregators.</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>class BaseAggregator(ABC, Generic[T]):\n    \"\"\"\n    Interface for aggregators.\n    \"\"\"\n\n    def __init__(self, max_size: int | None = None) -&gt; None:\n        super().__init__()\n        self._buffer: Deque[T] = deque()\n        self.max_size = max_size\n\n    def __call__(self, msg: T) -&gt; None:\n        if self.max_size is not None and len(self._buffer) &gt;= self.max_size:\n            self._buffer.popleft()\n        self._buffer.append(msg)\n\n    @abstractmethod\n    def get(self) -&gt; BaseMessage | None:\n        \"\"\"Returns the outcome of processing the aggregated message\"\"\"\n\n    def clear_buffer(self) -&gt; None:\n        \"\"\"Clears the buffer of messages\"\"\"\n        self._buffer.clear()\n\n    def get_buffer(self) -&gt; List[T]:\n        \"\"\"Returns a copy of the buffer of messages\"\"\"\n        return list(self._buffer)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(len={len(self._buffer)})\"\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator.clear_buffer","title":"<code>clear_buffer()</code>","text":"<p>Clears the buffer of messages</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>def clear_buffer(self) -&gt; None:\n    \"\"\"Clears the buffer of messages\"\"\"\n    self._buffer.clear()\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator.get","title":"<code>get()</code>  <code>abstractmethod</code>","text":"<p>Returns the outcome of processing the aggregated message</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>@abstractmethod\ndef get(self) -&gt; BaseMessage | None:\n    \"\"\"Returns the outcome of processing the aggregated message\"\"\"\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator.get_buffer","title":"<code>get_buffer()</code>","text":"<p>Returns a copy of the buffer of messages</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>def get_buffer(self) -&gt; List[T]:\n    \"\"\"Returns a copy of the buffer of messages\"\"\"\n    return list(self._buffer)\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#purpose","title":"Purpose","text":"<p>The <code>BaseAggregator</code> class serves as the foundation for message aggregation in RAI, providing:</p> <ul> <li>A buffer for collecting messages</li> <li>Size management to prevent memory overflow</li> <li>A consistent interface for processing and returning aggregated results</li> <li>Type safety through generics</li> </ul>"},{"location":"API_documentation/aggregators/overview/#usage-in-state-based-agents","title":"Usage in State-Based Agents","text":"<p>Aggregators are typically used in state-based agents to maintain and update the agent's state:</p> <pre><code>config = StateBasedConfig(\n    aggregators={\n        (\"/camera/camera/color/image_raw\", \"sensor_msgs/msg/Image\"): [\n            ROS2ImgVLMDiffAggregator()\n        ],\n        \"/rosout\": [\n            ROS2LogsAggregator()\n        ]\n    }\n)\n\nagent = ROS2StateBasedAgent(\n    config=config,\n    target_connectors={\"to_human\": hri_connector},\n    tools=tools\n)\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#direct-registration-via-connector","title":"Direct Registration via Connector","text":"<p>Aggregators can also be registered directly with a connector using the <code>register_callback</code> method. This allows for more flexible message processing outside of state-based agents:</p> <pre><code># Create a connector\nconnector = ROS2Connector()\n\n# Create an aggregator\nimage_aggregator = ROS2GetLastImageAggregator()\n\n# Register the aggregator as a callback for a specific topic\nconnector.register_callback(\n    topic=\"/camera/camera/color/image_raw\",\n    msg_type=\"sensor_msgs/msg/Image\",\n    callback=image_aggregator\n)\n\n# The aggregator will now process all messages received on the topic\n# You can retrieve the aggregated result at any time\naggregated_message = image_aggregator.get()\n</code></pre> <p>This approach is useful when you need to:</p> <ul> <li>Process messages from specific topics independently</li> <li>Combine multiple aggregators for the same topic</li> <li>Use aggregators in non-state-based agents</li> <li>Have more control over when aggregation occurs</li> </ul>"},{"location":"API_documentation/aggregators/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Buffer Management: Set appropriate max_size to prevent memory issues</li> <li>Resource Cleanup: Clear buffers when no longer needed</li> <li>Error Handling: Handle empty buffers and processing errors gracefully</li> <li>Type Safety: Use appropriate generic types for message types</li> <li>Performance: Consider the computational cost of aggregation operations</li> </ol>"},{"location":"API_documentation/aggregators/overview/#implementation-example","title":"Implementation Example","text":"<pre><code>class CustomAggregator(BaseAggregator[CustomMessage]):\n    def get(self) -&gt; HumanMessage | None:\n        msgs = self.get_buffer()\n        if not msgs:\n            return None\n\n        # Process messages\n        summary = process_messages(msgs)\n\n        # Clear buffer after processing\n        self.clear_buffer()\n\n        return HumanMessage(content=summary)\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#see-also","title":"See Also","text":"<ul> <li>ROS 2 Aggregators: For more information on the different types of ROS 2 aggregators in RAI</li> <li>Agents: For more information on the different types of agents in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/connectors/ROS_2_Connectors/","title":"ROS2 Connectors","text":"<p>RAI provides robust connectors for interacting with ROS 2 middleware, supporting both standard and human-robot interaction (HRI) message flows.</p> Connector Description Example Usage <code>ROS2Connector</code> Standard connector for generic ROS 2 topics, services, and actions. <code>connector = ROS2Connector()</code> <code>ROS2HRIConnector</code> Connector for multimodal HRI messages over ROS 2, combining ROS2BaseConnector and HRIConnector. <code>hri_connector = ROS2HRIConnector()</code>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#ros2connector","title":"<code>ROS2Connector</code>","text":"<p>The <code>ROS2Connector</code> is the main interface for publishing, subscribing, and calling services/actions in a ROS 2 system. It is a concrete implementation of <code>ROS2BaseConnector</code> for standard ROS 2 messages.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#class-definition","title":"Class definition","text":"ROS2BaseConnector class definition <p>ROS2Connector vs ROS2BaseConnector</p> <p><code>ROS2Connector</code> is a simple alias for <code>ROS2BaseConnector</code>. It exists mainly to provide a more intuitive and consistent class name for users, but does not add any new functionality.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector","title":"<code>rai.communication.ros2.connectors.base.ROS2BaseConnector</code>","text":"<p>               Bases: <code>ROS2ActionMixin</code>, <code>ROS2ServiceMixin</code>, <code>BaseConnector[T]</code></p> <p>ROS2-specific implementation of the BaseConnector.</p> <p>This connector provides functionality for ROS2 communication through topics, services, and actions, as well as TF (Transform) operations.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Name of the ROS2 node. If not provided, generates a unique name with UUID.</p> <code>None</code> <code>destroy_subscribers</code> <code>bool</code> <p>Whether to destroy subscribers after receiving a message, by default False.</p> <code>False</code> <code>executor_type</code> <code>Literal['single_threaded', 'multi_threaded']</code> <p>Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".</p> <code>'multi_threaded'</code> <code>use_sim_time</code> <code>bool</code> <p>Whether to use simulation time or system time, by default False.</p> <code>False</code> <p>Methods:</p> Name Description <code>get_topics_names_and_types</code> <p>Get list of available topics and their message types.</p> <code>get_services_names_and_types</code> <p>Get list of available services and their types.</p> <code>get_actions_names_and_types</code> <p>Get list of available actions and their types.</p> <code>send_message</code> <p>Send a message to a specified topic.</p> <code>receive_message</code> <p>Receive a message from a specified topic.</p> <code>wait_for_transform</code> <p>Wait for a transform to become available.</p> <code>get_transform</code> <p>Get the transform between two frames.</p> <code>create_service</code> <p>Create a ROS2 service.</p> <code>create_action</code> <p>Create a ROS2 action server.</p> <code>shutdown</code> <p>Clean up resources and shut down the connector.</p> Notes <p>Threading Model:     The connector creates an executor that runs in a dedicated thread.     This executor processes all ROS2 callbacks and operations asynchronously.</p> <p>Subscriber Lifecycle:     The <code>destroy_subscribers</code> parameter controls subscriber cleanup behavior:     - True: Subscribers are destroyed after receiving a message         - Pros: Better resource utilization         - Cons: Known stability issues (see: https://github.com/ros2/rclpy/issues/1142)     - False (default): Subscribers remain active after message reception         - Pros: More stable operation, avoids potential crashes         - Cons: May lead to memory/performance overhead from inactive subscribers</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>class ROS2BaseConnector(ROS2ActionMixin, ROS2ServiceMixin, BaseConnector[T]):\n    \"\"\"ROS2-specific implementation of the BaseConnector.\n\n    This connector provides functionality for ROS2 communication through topics,\n    services, and actions, as well as TF (Transform) operations.\n\n    Parameters\n    ----------\n    node_name : str, optional\n        Name of the ROS2 node. If not provided, generates a unique name with UUID.\n    destroy_subscribers : bool, optional\n        Whether to destroy subscribers after receiving a message, by default False.\n    executor_type : Literal[\"single_threaded\", \"multi_threaded\"], optional\n        Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".\n    use_sim_time : bool, optional\n        Whether to use simulation time or system time, by default False.\n\n    Methods\n    -------\n    get_topics_names_and_types()\n        Get list of available topics and their message types.\n    get_services_names_and_types()\n        Get list of available services and their types.\n    get_actions_names_and_types()\n        Get list of available actions and their types.\n    send_message(message, target, msg_type, auto_qos_matching=True, qos_profile=None, **kwargs)\n        Send a message to a specified topic.\n    receive_message(source, timeout_sec=1.0, msg_type=None, auto_topic_type=True, **kwargs)\n        Receive a message from a specified topic.\n    wait_for_transform(tf_buffer, target_frame, source_frame, timeout_sec=1.0)\n        Wait for a transform to become available.\n    get_transform(target_frame, source_frame, timeout_sec=5.0)\n        Get the transform between two frames.\n    create_service(service_name, on_request, on_done=None, service_type, **kwargs)\n        Create a ROS2 service.\n    create_action(action_name, generate_feedback_callback, action_type, **kwargs)\n        Create a ROS2 action server.\n    shutdown()\n        Clean up resources and shut down the connector.\n\n    Notes\n    -----\n    Threading Model:\n        The connector creates an executor that runs in a dedicated thread.\n        This executor processes all ROS2 callbacks and operations asynchronously.\n\n    Subscriber Lifecycle:\n        The `destroy_subscribers` parameter controls subscriber cleanup behavior:\n        - True: Subscribers are destroyed after receiving a message\n            - Pros: Better resource utilization\n            - Cons: Known stability issues (see: https://github.com/ros2/rclpy/issues/1142)\n        - False (default): Subscribers remain active after message reception\n            - Pros: More stable operation, avoids potential crashes\n            - Cons: May lead to memory/performance overhead from inactive subscribers\n    \"\"\"\n\n    def __init__(\n        self,\n        node_name: str | None = None,\n        destroy_subscribers: bool = False,\n        executor_type: Literal[\"single_threaded\", \"multi_threaded\"] = \"multi_threaded\",\n        use_sim_time: bool = False,\n    ):\n        \"\"\"Initialize the ROS2BaseConnector.\n\n        Parameters\n        ----------\n        node_name : str, optional\n            Name of the ROS2 node. If not provided, generates a unique name with UUID.\n        destroy_subscribers : bool, optional\n            Whether to destroy subscribers after receiving a message, by default False.\n        executor_type : Literal[\"single_threaded\", \"multi_threaded\"], optional\n            Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".\n\n        Raises\n        ------\n        ValueError\n            If an invalid executor type is provided.\n        \"\"\"\n        super().__init__()\n        if node_name is None:\n            node_name = f\"rai_ros2_connector_{str(uuid.uuid4())[-12:]}\"\n        if not rclpy.ok():\n            rclpy.init()\n            self.logger.warning(\n                \"Auto-initializing ROS2, but manual initialization is recommended. \"\n                \"For better control and predictability, call rclpy.init() or ROS2Context before creating this connector.\"\n            )\n        self._executor_type = executor_type\n        self._node = Node(node_name)\n        if use_sim_time:\n            self._node.set_parameters(\n                [Parameter(\"use_sim_time\", Parameter.Type.BOOL, True)]\n            )\n        self._topic_api = ROS2TopicAPI(self._node, destroy_subscribers)\n        self._service_api = ROS2ServiceAPI(self._node)\n        self._actions_api = ROS2ActionAPI(self._node)\n        self._tf_buffer = Buffer(node=self._node)\n        self._tf_listener = TransformListener(self._tf_buffer, self._node)\n\n        self._executor_performance_time_delta = 1.0\n        self._executor_performance_timer = self._node.create_timer(\n            self._executor_performance_time_delta, self._executor_performance_callback\n        )\n        self._performance_warning_threshold_multiplier: Final[float] = 1.1\n        self._available_executors: Final[set[str]] = {\n            \"MultiThreadedExecutor\",\n            \"SingleThreadedExecutor\",\n        }\n        if self._executor_type == \"multi_threaded\":\n            self._executor = MultiThreadedExecutor()\n        elif self._executor_type == \"single_threaded\":\n            self._executor = SingleThreadedExecutor()\n        else:\n            raise ValueError(f\"Invalid executor type: {self._executor_type}\")\n\n        self._executor.add_node(self._node)\n        self._thread = threading.Thread(target=self._executor.spin)\n        self._thread.start()\n        self.last_executor_performance_time = time.time()\n\n        # cache for last received messages\n        self.last_msg: Dict[str, T] = {}\n\n    def _executor_performance_callback(self) -&gt; None:\n        \"\"\"Monitor executor performance and log warnings if it falls behind schedule.\n\n        This callback checks if the executor is running slower than expected and logs\n        a warning with suggestions for alternative executors if performance issues\n        are detected.\n        \"\"\"\n        current_time = time.time()\n        time_behind = (\n            current_time\n            - self.last_executor_performance_time\n            - self._executor_performance_time_delta\n        )\n        threshold = (\n            self._executor_performance_time_delta\n            * self._performance_warning_threshold_multiplier\n        )\n\n        if time_behind &gt; threshold:\n            alternative_executors = self._available_executors - {\n                self._executor.__class__.__name__\n            }\n\n            self.logger.warning(\n                f\"{self._executor.__class__.__name__} is {time_behind:.2f} seconds behind. \"\n                f\"If you see this message frequently, consider switching to {', '.join(alternative_executors)}.\"\n            )\n            self.last_executor_performance_time = current_time\n        else:\n            self.last_executor_performance_time = current_time\n\n    def _last_message_callback(self, source: str, msg: T):\n        \"\"\"Store the last received message for a given source.\n\n        Parameters\n        ----------\n        source : str\n            The topic source identifier.\n        msg : T\n            The received message.\n        \"\"\"\n        self.last_msg[source] = msg\n\n    def get_topics_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n        \"\"\"Get list of available topics and their message types.\n\n        Returns\n        -------\n        List[Tuple[str, List[str]]]\n            List of tuples containing topic names and their corresponding message types.\n        \"\"\"\n        return self._topic_api.get_topic_names_and_types()\n\n    def get_services_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n        \"\"\"Get list of available services and their types.\n\n        Returns\n        -------\n        List[Tuple[str, List[str]]]\n            List of tuples containing service names and their corresponding types.\n        \"\"\"\n        return self._service_api.get_service_names_and_types()\n\n    def get_actions_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n        \"\"\"Get list of available actions and their types.\n\n        Returns\n        -------\n        List[Tuple[str, List[str]]]\n            List of tuples containing action names and their corresponding types.\n        \"\"\"\n        return self._actions_api.get_action_names_and_types()\n\n    def send_message(\n        self,\n        message: T,\n        target: str,\n        *,\n        msg_type: str,\n        auto_qos_matching: bool = True,\n        qos_profile: Optional[QoSProfile] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Send a message to a specified topic.\n\n        Parameters\n        ----------\n        message : T\n            The message to send.\n        target : str\n            The target topic name.\n        msg_type : str\n            The ROS2 message type.\n        auto_qos_matching : bool, optional\n            Whether to automatically match QoS profiles, by default True.\n        qos_profile : Optional[QoSProfile], optional\n            Custom QoS profile to use, by default None.\n        **kwargs : Any\n            Additional keyword arguments.\n        \"\"\"\n        self._topic_api.publish(\n            topic=target,\n            msg_content=message.payload,\n            msg_type=msg_type,\n            auto_qos_matching=auto_qos_matching,\n            qos_profile=qos_profile,\n        )\n\n    def general_callback_preprocessor(self, message: Any) -&gt; T:\n        \"\"\"Preprocess a raw ROS2 message into a connector message.\n\n        Parameters\n        ----------\n        message : Any\n            The raw ROS2 message.\n\n        Returns\n        -------\n        T\n            The preprocessed message.\n        \"\"\"\n        return self.T_class(payload=message, metadata={\"msg_type\": str(type(message))})\n\n    def register_callback(\n        self,\n        source: str,\n        callback: Callable[[T | Any], None],\n        raw: bool = False,\n        *,\n        msg_type: Optional[str] = None,\n        qos_profile: Optional[QoSProfile] = None,\n        auto_qos_matching: bool = True,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Register a callback for a topic.\n\n        Parameters\n        ----------\n        source : str\n            The topic to subscribe to.\n        callback : Callable[[T | Any], None]\n            The callback function to execute when a message is received.\n        raw : bool, optional\n            Whether to pass raw messages to the callback, by default False.\n        msg_type : Optional[str], optional\n            The ROS2 message type, by default None.\n        qos_profile : Optional[QoSProfile], optional\n            Custom QoS profile to use, by default None.\n        auto_qos_matching : bool, optional\n            Whether to automatically match QoS profiles, by default True.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The callback ID.\n        \"\"\"\n        exists = self._topic_api.subscriber_exists(source)\n        if not exists:\n            self._topic_api.create_subscriber(\n                topic=source,\n                msg_type=msg_type,\n                callback=partial(self.general_callback, source),\n                qos_profile=qos_profile,\n                auto_qos_matching=auto_qos_matching,\n            )\n        return super().register_callback(source, callback, raw=raw)\n\n    def receive_message(\n        self,\n        source: str,\n        timeout_sec: float = 1.0,\n        *,\n        msg_type: Optional[str] = None,\n        qos_profile: Optional[QoSProfile] = None,\n        auto_qos_matching: bool = True,\n        **kwargs: Any,\n    ) -&gt; T:\n        \"\"\"Receive a message from a topic.\n\n        Parameters\n        ----------\n        source : str\n            The topic to receive from.\n        timeout_sec : float, optional\n            Timeout in seconds, by default 1.0.\n        msg_type : Optional[str], optional\n            The ROS2 message type, by default None.\n        qos_profile : Optional[QoSProfile], optional\n            Custom QoS profile to use, by default None.\n        auto_qos_matching : bool, optional\n            Whether to automatically match QoS profiles, by default True.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        T\n            The received message.\n\n        Raises\n        ------\n        TimeoutError\n            If no message is received within the timeout period.\n        \"\"\"\n        if self._topic_api.subscriber_exists(source):\n            # trying to hit cache first\n            if source in self.last_msg:\n                if self.last_msg[source].timestamp &gt; time.time() - timeout_sec:\n                    return self.last_msg[source]\n        else:\n            self._topic_api.create_subscriber(\n                topic=source,\n                callback=partial(self.general_callback, source),\n                msg_type=msg_type,\n                qos_profile=qos_profile,\n                auto_qos_matching=auto_qos_matching,\n            )\n            self.register_callback(source, partial(self._last_message_callback, source))\n\n        start_time = time.time()\n        # wait for the message to be received\n        while time.time() - start_time &lt; timeout_sec:\n            if source in self.last_msg:\n                return self.last_msg[source]\n            time.sleep(0.1)\n        else:\n            raise TimeoutError(\n                f\"Message from {source} not received in {timeout_sec} seconds\"\n            )\n\n    @staticmethod\n    def wait_for_transform(\n        tf_buffer: Buffer,\n        target_frame: str,\n        source_frame: str,\n        timeout_sec: float = 1.0,\n    ) -&gt; bool:\n        \"\"\"Wait for a transform to become available.\n\n        Parameters\n        ----------\n        tf_buffer : Buffer\n            The TF buffer to check.\n        target_frame : str\n            The target frame.\n        source_frame : str\n            The source frame.\n        timeout_sec : float, optional\n            Timeout in seconds, by default 1.0.\n\n        Returns\n        -------\n        bool\n            True if the transform is available, False otherwise.\n        \"\"\"\n        start_time = time.time()\n        while time.time() - start_time &lt; timeout_sec:\n            if tf_buffer.can_transform(target_frame, source_frame, rclpy.time.Time()):\n                return True\n            time.sleep(0.1)\n        return False\n\n    def get_transform(\n        self,\n        target_frame: str,\n        source_frame: str,\n        timeout_sec: float = 5.0,\n    ) -&gt; TransformStamped:\n        \"\"\"Get the transform between two frames.\n\n        Parameters\n        ----------\n        target_frame : str\n            The target frame.\n        source_frame : str\n            The source frame.\n        timeout_sec : float, optional\n            Timeout in seconds, by default 5.0.\n\n        Returns\n        -------\n        TransformStamped\n            The transform between the frames.\n\n        Raises\n        ------\n        LookupException\n            If the transform is not available within the timeout period.\n        \"\"\"\n        transform_available = self.wait_for_transform(\n            self._tf_buffer, target_frame, source_frame, timeout_sec\n        )\n        if not transform_available:\n            raise LookupException(\n                f\"Could not find transform from {source_frame} to {target_frame} in {timeout_sec} seconds\"\n            )\n        transform: TransformStamped = self._tf_buffer.lookup_transform(\n            target_frame,\n            source_frame,\n            rclpy.time.Time(),\n            timeout=Duration(seconds=int(timeout_sec)),\n        )\n\n        return transform\n\n    def create_service(\n        self,\n        service_name: str,\n        on_request: Callable[[Any, Any], Any],\n        on_done: Optional[Callable[[Any, Any], Any]] = None,\n        *,\n        service_type: str,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Create a ROS2 service.\n\n        Parameters\n        ----------\n        service_name : str\n            The name of the service.\n        on_request : Callable[[Any, Any], Any]\n            Callback function to handle service requests.\n        on_done : Optional[Callable[[Any, Any], Any]], optional\n            Callback function called when service is terminated, by default None.\n        service_type : str\n            The ROS2 service type.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The service handle.\n        \"\"\"\n        return self._service_api.create_service(\n            service_name=service_name,\n            callback=on_request,\n            service_type=service_type,\n            **kwargs,\n        )\n\n    def create_action(\n        self,\n        action_name: str,\n        generate_feedback_callback: Callable,\n        *,\n        action_type: str,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Create a ROS2 action server.\n\n        Parameters\n        ----------\n        action_name : str\n            The name of the action.\n        generate_feedback_callback : Callable\n            Callback function to generate feedback during action execution.\n        action_type : str\n            The ROS2 action type.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The action handle.\n        \"\"\"\n        return self._actions_api.create_action_server(\n            action_name=action_name,\n            action_type=action_type,\n            execute_callback=generate_feedback_callback,\n            **kwargs,\n        )\n\n    @property\n    def node(self) -&gt; Node:\n        \"\"\"Get the ROS2 node.\n\n        Returns\n        -------\n        Node\n            The ROS2 node instance.\n        \"\"\"\n        return self._node\n\n    def shutdown(self):\n        \"\"\"Shutdown the connector and clean up resources.\n\n        This method:\n        1. Unregisters the TF listener\n        2. Destroys the ROS2 node\n        3. Shuts down the action API\n        4. Shuts down the topic API\n        5. Shuts down the executor\n        6. Joins the executor thread\n        \"\"\"\n        self._tf_listener.unregister()\n        self._node.destroy_node()\n        self._actions_api.shutdown()\n        self._topic_api.shutdown()\n        self._executor.shutdown()\n        self._thread.join()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.node","title":"<code>node</code>  <code>property</code>","text":"<p>Get the ROS2 node.</p> <p>Returns:</p> Type Description <code>Node</code> <p>The ROS2 node instance.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.__init__","title":"<code>__init__(node_name=None, destroy_subscribers=False, executor_type='multi_threaded', use_sim_time=False)</code>","text":"<p>Initialize the ROS2BaseConnector.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Name of the ROS2 node. If not provided, generates a unique name with UUID.</p> <code>None</code> <code>destroy_subscribers</code> <code>bool</code> <p>Whether to destroy subscribers after receiving a message, by default False.</p> <code>False</code> <code>executor_type</code> <code>Literal['single_threaded', 'multi_threaded']</code> <p>Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".</p> <code>'multi_threaded'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid executor type is provided.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def __init__(\n    self,\n    node_name: str | None = None,\n    destroy_subscribers: bool = False,\n    executor_type: Literal[\"single_threaded\", \"multi_threaded\"] = \"multi_threaded\",\n    use_sim_time: bool = False,\n):\n    \"\"\"Initialize the ROS2BaseConnector.\n\n    Parameters\n    ----------\n    node_name : str, optional\n        Name of the ROS2 node. If not provided, generates a unique name with UUID.\n    destroy_subscribers : bool, optional\n        Whether to destroy subscribers after receiving a message, by default False.\n    executor_type : Literal[\"single_threaded\", \"multi_threaded\"], optional\n        Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".\n\n    Raises\n    ------\n    ValueError\n        If an invalid executor type is provided.\n    \"\"\"\n    super().__init__()\n    if node_name is None:\n        node_name = f\"rai_ros2_connector_{str(uuid.uuid4())[-12:]}\"\n    if not rclpy.ok():\n        rclpy.init()\n        self.logger.warning(\n            \"Auto-initializing ROS2, but manual initialization is recommended. \"\n            \"For better control and predictability, call rclpy.init() or ROS2Context before creating this connector.\"\n        )\n    self._executor_type = executor_type\n    self._node = Node(node_name)\n    if use_sim_time:\n        self._node.set_parameters(\n            [Parameter(\"use_sim_time\", Parameter.Type.BOOL, True)]\n        )\n    self._topic_api = ROS2TopicAPI(self._node, destroy_subscribers)\n    self._service_api = ROS2ServiceAPI(self._node)\n    self._actions_api = ROS2ActionAPI(self._node)\n    self._tf_buffer = Buffer(node=self._node)\n    self._tf_listener = TransformListener(self._tf_buffer, self._node)\n\n    self._executor_performance_time_delta = 1.0\n    self._executor_performance_timer = self._node.create_timer(\n        self._executor_performance_time_delta, self._executor_performance_callback\n    )\n    self._performance_warning_threshold_multiplier: Final[float] = 1.1\n    self._available_executors: Final[set[str]] = {\n        \"MultiThreadedExecutor\",\n        \"SingleThreadedExecutor\",\n    }\n    if self._executor_type == \"multi_threaded\":\n        self._executor = MultiThreadedExecutor()\n    elif self._executor_type == \"single_threaded\":\n        self._executor = SingleThreadedExecutor()\n    else:\n        raise ValueError(f\"Invalid executor type: {self._executor_type}\")\n\n    self._executor.add_node(self._node)\n    self._thread = threading.Thread(target=self._executor.spin)\n    self._thread.start()\n    self.last_executor_performance_time = time.time()\n\n    # cache for last received messages\n    self.last_msg: Dict[str, T] = {}\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.create_action","title":"<code>create_action(action_name, generate_feedback_callback, *, action_type, **kwargs)</code>","text":"<p>Create a ROS2 action server.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>The name of the action.</p> required <code>generate_feedback_callback</code> <code>Callable</code> <p>Callback function to generate feedback during action execution.</p> required <code>action_type</code> <code>str</code> <p>The ROS2 action type.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The action handle.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def create_action(\n    self,\n    action_name: str,\n    generate_feedback_callback: Callable,\n    *,\n    action_type: str,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Create a ROS2 action server.\n\n    Parameters\n    ----------\n    action_name : str\n        The name of the action.\n    generate_feedback_callback : Callable\n        Callback function to generate feedback during action execution.\n    action_type : str\n        The ROS2 action type.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The action handle.\n    \"\"\"\n    return self._actions_api.create_action_server(\n        action_name=action_name,\n        action_type=action_type,\n        execute_callback=generate_feedback_callback,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.create_service","title":"<code>create_service(service_name, on_request, on_done=None, *, service_type, **kwargs)</code>","text":"<p>Create a ROS2 service.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the service.</p> required <code>on_request</code> <code>Callable[[Any, Any], Any]</code> <p>Callback function to handle service requests.</p> required <code>on_done</code> <code>Optional[Callable[[Any, Any], Any]]</code> <p>Callback function called when service is terminated, by default None.</p> <code>None</code> <code>service_type</code> <code>str</code> <p>The ROS2 service type.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The service handle.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def create_service(\n    self,\n    service_name: str,\n    on_request: Callable[[Any, Any], Any],\n    on_done: Optional[Callable[[Any, Any], Any]] = None,\n    *,\n    service_type: str,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Create a ROS2 service.\n\n    Parameters\n    ----------\n    service_name : str\n        The name of the service.\n    on_request : Callable[[Any, Any], Any]\n        Callback function to handle service requests.\n    on_done : Optional[Callable[[Any, Any], Any]], optional\n        Callback function called when service is terminated, by default None.\n    service_type : str\n        The ROS2 service type.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The service handle.\n    \"\"\"\n    return self._service_api.create_service(\n        service_name=service_name,\n        callback=on_request,\n        service_type=service_type,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.general_callback_preprocessor","title":"<code>general_callback_preprocessor(message)</code>","text":"<p>Preprocess a raw ROS2 message into a connector message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>The raw ROS2 message.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The preprocessed message.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def general_callback_preprocessor(self, message: Any) -&gt; T:\n    \"\"\"Preprocess a raw ROS2 message into a connector message.\n\n    Parameters\n    ----------\n    message : Any\n        The raw ROS2 message.\n\n    Returns\n    -------\n    T\n        The preprocessed message.\n    \"\"\"\n    return self.T_class(payload=message, metadata={\"msg_type\": str(type(message))})\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_actions_names_and_types","title":"<code>get_actions_names_and_types()</code>","text":"<p>Get list of available actions and their types.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str]]]</code> <p>List of tuples containing action names and their corresponding types.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_actions_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n    \"\"\"Get list of available actions and their types.\n\n    Returns\n    -------\n    List[Tuple[str, List[str]]]\n        List of tuples containing action names and their corresponding types.\n    \"\"\"\n    return self._actions_api.get_action_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_services_names_and_types","title":"<code>get_services_names_and_types()</code>","text":"<p>Get list of available services and their types.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str]]]</code> <p>List of tuples containing service names and their corresponding types.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_services_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n    \"\"\"Get list of available services and their types.\n\n    Returns\n    -------\n    List[Tuple[str, List[str]]]\n        List of tuples containing service names and their corresponding types.\n    \"\"\"\n    return self._service_api.get_service_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_topics_names_and_types","title":"<code>get_topics_names_and_types()</code>","text":"<p>Get list of available topics and their message types.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str]]]</code> <p>List of tuples containing topic names and their corresponding message types.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_topics_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n    \"\"\"Get list of available topics and their message types.\n\n    Returns\n    -------\n    List[Tuple[str, List[str]]]\n        List of tuples containing topic names and their corresponding message types.\n    \"\"\"\n    return self._topic_api.get_topic_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_transform","title":"<code>get_transform(target_frame, source_frame, timeout_sec=5.0)</code>","text":"<p>Get the transform between two frames.</p> <p>Parameters:</p> Name Type Description Default <code>target_frame</code> <code>str</code> <p>The target frame.</p> required <code>source_frame</code> <code>str</code> <p>The source frame.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds, by default 5.0.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>TransformStamped</code> <p>The transform between the frames.</p> <p>Raises:</p> Type Description <code>LookupException</code> <p>If the transform is not available within the timeout period.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_transform(\n    self,\n    target_frame: str,\n    source_frame: str,\n    timeout_sec: float = 5.0,\n) -&gt; TransformStamped:\n    \"\"\"Get the transform between two frames.\n\n    Parameters\n    ----------\n    target_frame : str\n        The target frame.\n    source_frame : str\n        The source frame.\n    timeout_sec : float, optional\n        Timeout in seconds, by default 5.0.\n\n    Returns\n    -------\n    TransformStamped\n        The transform between the frames.\n\n    Raises\n    ------\n    LookupException\n        If the transform is not available within the timeout period.\n    \"\"\"\n    transform_available = self.wait_for_transform(\n        self._tf_buffer, target_frame, source_frame, timeout_sec\n    )\n    if not transform_available:\n        raise LookupException(\n            f\"Could not find transform from {source_frame} to {target_frame} in {timeout_sec} seconds\"\n        )\n    transform: TransformStamped = self._tf_buffer.lookup_transform(\n        target_frame,\n        source_frame,\n        rclpy.time.Time(),\n        timeout=Duration(seconds=int(timeout_sec)),\n    )\n\n    return transform\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.receive_message","title":"<code>receive_message(source, timeout_sec=1.0, *, msg_type=None, qos_profile=None, auto_qos_matching=True, **kwargs)</code>","text":"<p>Receive a message from a topic.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The topic to receive from.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds, by default 1.0.</p> <code>1.0</code> <code>msg_type</code> <code>Optional[str]</code> <p>The ROS2 message type, by default None.</p> <code>None</code> <code>qos_profile</code> <code>Optional[QoSProfile]</code> <p>Custom QoS profile to use, by default None.</p> <code>None</code> <code>auto_qos_matching</code> <code>bool</code> <p>Whether to automatically match QoS profiles, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The received message.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If no message is received within the timeout period.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def receive_message(\n    self,\n    source: str,\n    timeout_sec: float = 1.0,\n    *,\n    msg_type: Optional[str] = None,\n    qos_profile: Optional[QoSProfile] = None,\n    auto_qos_matching: bool = True,\n    **kwargs: Any,\n) -&gt; T:\n    \"\"\"Receive a message from a topic.\n\n    Parameters\n    ----------\n    source : str\n        The topic to receive from.\n    timeout_sec : float, optional\n        Timeout in seconds, by default 1.0.\n    msg_type : Optional[str], optional\n        The ROS2 message type, by default None.\n    qos_profile : Optional[QoSProfile], optional\n        Custom QoS profile to use, by default None.\n    auto_qos_matching : bool, optional\n        Whether to automatically match QoS profiles, by default True.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    T\n        The received message.\n\n    Raises\n    ------\n    TimeoutError\n        If no message is received within the timeout period.\n    \"\"\"\n    if self._topic_api.subscriber_exists(source):\n        # trying to hit cache first\n        if source in self.last_msg:\n            if self.last_msg[source].timestamp &gt; time.time() - timeout_sec:\n                return self.last_msg[source]\n    else:\n        self._topic_api.create_subscriber(\n            topic=source,\n            callback=partial(self.general_callback, source),\n            msg_type=msg_type,\n            qos_profile=qos_profile,\n            auto_qos_matching=auto_qos_matching,\n        )\n        self.register_callback(source, partial(self._last_message_callback, source))\n\n    start_time = time.time()\n    # wait for the message to be received\n    while time.time() - start_time &lt; timeout_sec:\n        if source in self.last_msg:\n            return self.last_msg[source]\n        time.sleep(0.1)\n    else:\n        raise TimeoutError(\n            f\"Message from {source} not received in {timeout_sec} seconds\"\n        )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.register_callback","title":"<code>register_callback(source, callback, raw=False, *, msg_type=None, qos_profile=None, auto_qos_matching=True, **kwargs)</code>","text":"<p>Register a callback for a topic.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The topic to subscribe to.</p> required <code>callback</code> <code>Callable[[T | Any], None]</code> <p>The callback function to execute when a message is received.</p> required <code>raw</code> <code>bool</code> <p>Whether to pass raw messages to the callback, by default False.</p> <code>False</code> <code>msg_type</code> <code>Optional[str]</code> <p>The ROS2 message type, by default None.</p> <code>None</code> <code>qos_profile</code> <code>Optional[QoSProfile]</code> <p>Custom QoS profile to use, by default None.</p> <code>None</code> <code>auto_qos_matching</code> <code>bool</code> <p>Whether to automatically match QoS profiles, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The callback ID.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def register_callback(\n    self,\n    source: str,\n    callback: Callable[[T | Any], None],\n    raw: bool = False,\n    *,\n    msg_type: Optional[str] = None,\n    qos_profile: Optional[QoSProfile] = None,\n    auto_qos_matching: bool = True,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Register a callback for a topic.\n\n    Parameters\n    ----------\n    source : str\n        The topic to subscribe to.\n    callback : Callable[[T | Any], None]\n        The callback function to execute when a message is received.\n    raw : bool, optional\n        Whether to pass raw messages to the callback, by default False.\n    msg_type : Optional[str], optional\n        The ROS2 message type, by default None.\n    qos_profile : Optional[QoSProfile], optional\n        Custom QoS profile to use, by default None.\n    auto_qos_matching : bool, optional\n        Whether to automatically match QoS profiles, by default True.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The callback ID.\n    \"\"\"\n    exists = self._topic_api.subscriber_exists(source)\n    if not exists:\n        self._topic_api.create_subscriber(\n            topic=source,\n            msg_type=msg_type,\n            callback=partial(self.general_callback, source),\n            qos_profile=qos_profile,\n            auto_qos_matching=auto_qos_matching,\n        )\n    return super().register_callback(source, callback, raw=raw)\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.send_message","title":"<code>send_message(message, target, *, msg_type, auto_qos_matching=True, qos_profile=None, **kwargs)</code>","text":"<p>Send a message to a specified topic.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>T</code> <p>The message to send.</p> required <code>target</code> <code>str</code> <p>The target topic name.</p> required <code>msg_type</code> <code>str</code> <p>The ROS2 message type.</p> required <code>auto_qos_matching</code> <code>bool</code> <p>Whether to automatically match QoS profiles, by default True.</p> <code>True</code> <code>qos_profile</code> <code>Optional[QoSProfile]</code> <p>Custom QoS profile to use, by default None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def send_message(\n    self,\n    message: T,\n    target: str,\n    *,\n    msg_type: str,\n    auto_qos_matching: bool = True,\n    qos_profile: Optional[QoSProfile] = None,\n    **kwargs: Any,\n):\n    \"\"\"Send a message to a specified topic.\n\n    Parameters\n    ----------\n    message : T\n        The message to send.\n    target : str\n        The target topic name.\n    msg_type : str\n        The ROS2 message type.\n    auto_qos_matching : bool, optional\n        Whether to automatically match QoS profiles, by default True.\n    qos_profile : Optional[QoSProfile], optional\n        Custom QoS profile to use, by default None.\n    **kwargs : Any\n        Additional keyword arguments.\n    \"\"\"\n    self._topic_api.publish(\n        topic=target,\n        msg_content=message.payload,\n        msg_type=msg_type,\n        auto_qos_matching=auto_qos_matching,\n        qos_profile=qos_profile,\n    )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.shutdown","title":"<code>shutdown()</code>","text":"<p>Shutdown the connector and clean up resources.</p> <p>This method: 1. Unregisters the TF listener 2. Destroys the ROS2 node 3. Shuts down the action API 4. Shuts down the topic API 5. Shuts down the executor 6. Joins the executor thread</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def shutdown(self):\n    \"\"\"Shutdown the connector and clean up resources.\n\n    This method:\n    1. Unregisters the TF listener\n    2. Destroys the ROS2 node\n    3. Shuts down the action API\n    4. Shuts down the topic API\n    5. Shuts down the executor\n    6. Joins the executor thread\n    \"\"\"\n    self._tf_listener.unregister()\n    self._node.destroy_node()\n    self._actions_api.shutdown()\n    self._topic_api.shutdown()\n    self._executor.shutdown()\n    self._thread.join()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.wait_for_transform","title":"<code>wait_for_transform(tf_buffer, target_frame, source_frame, timeout_sec=1.0)</code>  <code>staticmethod</code>","text":"<p>Wait for a transform to become available.</p> <p>Parameters:</p> Name Type Description Default <code>tf_buffer</code> <code>Buffer</code> <p>The TF buffer to check.</p> required <code>target_frame</code> <code>str</code> <p>The target frame.</p> required <code>source_frame</code> <code>str</code> <p>The source frame.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds, by default 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transform is available, False otherwise.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>@staticmethod\ndef wait_for_transform(\n    tf_buffer: Buffer,\n    target_frame: str,\n    source_frame: str,\n    timeout_sec: float = 1.0,\n) -&gt; bool:\n    \"\"\"Wait for a transform to become available.\n\n    Parameters\n    ----------\n    tf_buffer : Buffer\n        The TF buffer to check.\n    target_frame : str\n        The target frame.\n    source_frame : str\n        The source frame.\n    timeout_sec : float, optional\n        Timeout in seconds, by default 1.0.\n\n    Returns\n    -------\n    bool\n        True if the transform is available, False otherwise.\n    \"\"\"\n    start_time = time.time()\n    while time.time() - start_time &lt; timeout_sec:\n        if tf_buffer.can_transform(target_frame, source_frame, rclpy.time.Time()):\n            return True\n        time.sleep(0.1)\n    return False\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#key-features","title":"Key Features","text":"<ul> <li>Manages ROS 2 node lifecycle and threading (via <code>MultiThreadedExecutor</code>, see ROS 2 Executors)</li> <li>Supports topic-based message passing (publish/subscribe, see ROS 2 Topics)</li> <li>Service calls (request/response, see ROS 2 Services)</li> <li>Actions (long-running operations with feedback, see ROS 2 Actions)</li> <li>TF (Transform) operations, see ROS 2 TF</li> <li>Callback registration for asynchronous notifications</li> </ul>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#example-usage","title":"Example Usage","text":"<pre><code>from rai.communication.ros2.connectors import ROS2Connector\n\nconnector = ROS2Connector()\n\n# Send a message to a topic\nconnector.send_message(\n    message=my_msg,  # ROS2Message\n    target=\"/my_topic\",\n    msg_type=\"std_msgs/msg/String\"\n)\n\n# Register a callback for a topic\nconnector.register_callback(\n    source=\"/my_topic\",\n    callback=my_callback,\n    msg_type=\"std_msgs/msg/String\"\n)\n\n# Call a service\nresponse = connector.service_call(\n    message=my_request_msg,\n    target=\"/my_service\",\n    msg_type=\"my_package/srv/MyService\"\n)\n\n# Start an action\nhandle = connector.start_action(\n    action_data=my_goal_msg,\n    target=\"/my_action\",\n    msg_type=\"my_package/action/MyAction\",\n    on_feedback=feedback_cb,\n    on_done=done_cb\n)\n\n# Get available topics\ntopics = connector.get_topics_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#node-lifecycle-and-threading","title":"Node Lifecycle and Threading","text":"<p>The connector creates a dedicated ROS 2 node and runs it in a background thread, using a <code>MultiThreadedExecutor</code> for asynchronous operations. This allows for concurrent message handling and callback execution.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#ros2hriconnector","title":"<code>ROS2HRIConnector</code>","text":"<p>The <code>ROS2HRIConnector</code> extends <code>ROS2BaseConnector</code> and implements the <code>HRIConnector</code> interface for multimodal human-robot interaction messages. It is specialized for exchanging <code>ROS2HRIMessage</code> objects, which can contain text, images, and audio.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#key-features_1","title":"Key Features","text":"<ul> <li>Publishes and subscribes to <code>rai_interfaces/msg/HRIMessage</code> topics</li> <li>Converts between ROS 2 HRI messages and the internal multimodal format</li> <li>Supports all standard connector operations (topics, services, actions)</li> <li>Suitable for integrating AI agents with human-facing ROS 2 interfaces</li> </ul>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#example-usage_1","title":"Example Usage","text":"<pre><code>from rai.communication.ros2.connectors import ROS2HRIConnector\n\nhri_connector = ROS2HRIConnector()\n\n# Send a multimodal HRI message to a topic\nhri_connector.send_message(\n    message=my_hri_msg,  # ROS2HRIMessage\n    target=\"/to_human\"\n)\n\n# Register a callback for incoming HRI messages\nhri_connector.register_callback(\n    source=\"/from_human\",\n    callback=on_human_message\n)\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#message-conversion","title":"Message Conversion","text":"<p>The <code>ROS2HRIConnector</code> automatically converts between ROS 2 <code>rai_interfaces/msg/HRIMessage</code> and the internal <code>ROS2HRIMessage</code> format for seamless multimodal communication.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#usage-in-agents","title":"Usage in Agents","text":"<p>Both connectors are commonly used in RAI agents to interface with ROS 2 environments. Example:</p> <pre><code>from rai.communication.ros2.connectors import ROS2Connector, ROS2HRIConnector\n\nros2_connector = ROS2Connector()\nhri_connector = ROS2HRIConnector()\n\n# Use with tools or agents\nagent = ReActAgent(\n    target_connectors={\"/to_human\": hri_connector},\n    tools=ROS2Toolkit(connector=ros2_connector).get_tools()\n)\n\n# Subscribe to human input\nagent.subscribe_source(\"/from_human\", hri_connector)\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#ros2-utilities","title":"ROS2 Utilities","text":"<p>RAI provides utilities for error handling and parameter access when working with ROS2 connectors:</p> <ul> <li> <p><code>ROS2ServiceError</code>: Exception raised for service-related errors (e.g., service unavailable, timeout). Includes service name, timeout duration, and diagnostic suggestions.</p> </li> <li> <p><code>ROS2ParameterError</code>: Exception raised for parameter-related errors (e.g., missing parameter, type mismatch). Includes parameter name, expected type/value, and suggestions for resolution.</p> </li> <li> <p><code>get_param_value()</code>: Utility function for simplified ROS2 parameter access with automatic type conversion and default value support. Reduces boilerplate compared to direct ROS2 parameter API usage.</p> </li> </ul> <pre><code>from rai.communication.ros2 import ROS2ServiceError, ROS2ParameterError, get_param_value\n\n# Access parameters with defaults\nservice_name = get_param_value(node, \"/my_service/name\", default=\"/default_service\")\n\n# Handle service errors\ntry:\n    response = connector.service_call(...)\nexcept ROS2ServiceError as e:\n    print(f\"Service {e.service_name} unavailable: {e.suggestion}\")\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#see-also","title":"See Also","text":"<ul> <li>Connectors Overview</li> <li>ROS 2 Aggregators</li> <li>ROS 2 Tools</li> </ul>"},{"location":"API_documentation/connectors/overview/","title":"Connectors","text":"<p>Connectors are a set of abstract interfaces and implementations designed to provide a unified way to interact with various communication systems, including robot middleware like ROS2, sound devices, and other I/O systems.</p>"},{"location":"API_documentation/connectors/overview/#connector-architecture","title":"Connector Architecture","text":"<p>The connector architecture is built on a hierarchy of abstract base classes and concrete implementations:</p>"},{"location":"API_documentation/connectors/overview/#base-classes","title":"Base Classes","text":""},{"location":"API_documentation/connectors/overview/#baseconnectort","title":"BaseConnector[T]","text":"BaseConnector class definition <p>The foundation interface that defines common communication patterns:</p> <ul> <li>Message passing (publish/subscribe)</li> <li>Service calls (request/response)</li> <li>Actions (long-running operations with feedback)</li> <li>Callback registration for asynchronous notifications</li> </ul>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector","title":"<code>rai.communication.base_connector.BaseConnector</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>class BaseConnector(Generic[T]):\n    def __init__(self, callback_max_workers: int = 4):\n        self.callback_max_workers = callback_max_workers\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.registered_callbacks: Dict[str, Dict[str, ParametrizedCallback[T]]] = (\n            defaultdict(dict)\n        )\n        self.callback_id_mapping: Dict[str, tuple[str, ParametrizedCallback[T]]] = {}\n        self.callback_executor = ThreadPoolExecutor(\n            max_workers=self.callback_max_workers\n        )\n\n        if not hasattr(self, \"__orig_bases__\"):\n            self.__orig_bases__ = {}\n            raise ConnectorException(\n                f\"Error while instantiating {str(self.__class__)}: \"\n                \"Message type T derived from BaseMessage needs to be provided\"\n                \" e.g. Connector[MessageType]()\"\n            )\n        self.T_class: Type[T] = get_args(self.__orig_bases__[-1])[0]\n\n    def _generate_handle(self) -&gt; str:\n        return str(uuid4())\n\n    def send_message(self, message: T, target: str, **kwargs: Optional[Any]) -&gt; None:\n        \"\"\"Implements publish pattern.\n\n        Sends a message to one or more subscribers. The target parameter\n        can be used to specify the destination or topic.\n\n        Parameters\n        ----------\n        message : T\n            The message to send.\n        target : str\n            The destination or topic to send the message to.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Raises\n        ------\n        ConnectorException\n            If the message cannot be sent.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def receive_message(\n        self, source: str, timeout_sec: float, **kwargs: Optional[Any]\n    ) -&gt; T:\n        \"\"\"Implements subscribe pattern.\n\n        Receives a message from a publisher. The source parameter\n        can be used to specify the source or topic to subscribe to.\n\n        Parameters\n        ----------\n        source : str\n            The source or topic to receive the message from.\n        timeout_sec : float\n            Timeout in seconds for receiving the message.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        T\n            The received message.\n\n        Raises\n        ------\n        ConnectorException\n            If the message cannot be received.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def register_callback(\n        self,\n        source: str,\n        callback: Callable[[T | Any], None],\n        raw: bool = False,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Implements register callback.\n\n        Registers a callback to be called when a message is received from a source.\n        If raw is False, the callback will receive a T object.\n        If raw is True, the callback will receive the raw message.\n\n        Parameters\n        ----------\n        source : str\n            The source to register the callback for.\n        callback : Callable[[T | Any], None]\n            The callback function to register.\n        raw : bool, optional\n            Whether to pass raw message to callback, by default False.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The ID of the registered callback.\n\n        Raises\n        ------\n        ConnectorException\n            If the callback cannot be registered.\n        \"\"\"\n        parametrized_callback = ParametrizedCallback[T](callback=callback, raw=raw)\n        self.registered_callbacks[source][parametrized_callback.id] = (\n            parametrized_callback\n        )\n        self.callback_id_mapping[parametrized_callback.id] = (\n            source,\n            parametrized_callback,\n        )\n        return parametrized_callback.id\n\n    def unregister_callback(self, callback_id: str) -&gt; None:\n        \"\"\"Unregisters a callback from a source.\n\n        Parameters\n        ----------\n        callback_id : str\n            The id of the callback to unregister.\n\n        Raises\n        ------\n        ConnectorException\n            If the callback cannot be unregistered.\n        \"\"\"\n        if callback_id not in self.callback_id_mapping:\n            raise ConnectorException(f\"Callback with id {callback_id} not found.\")\n\n        source, _ = self.callback_id_mapping[callback_id]\n        del self.registered_callbacks[source][callback_id]\n        del self.callback_id_mapping[callback_id]\n\n    def _safe_callback_wrapper(self, callback: Callable[[T], None], message: T) -&gt; None:\n        \"\"\"Safely execute a callback with error handling.\n\n        Parameters\n        ----------\n        callback : Callable[[T], None]\n            The callback function to execute.\n        message : T\n            The message to pass to the callback.\n        \"\"\"\n        try:\n            callback(message)\n        except Exception as e:\n            self.logger.error(f\"Error in callback: {str(e)}\")\n\n    def general_callback(self, source: str, message: Any) -&gt; None:\n        processed_message = self.general_callback_preprocessor(message)\n        for parametrized_callback in self.registered_callbacks.get(source, {}).values():\n            payload = message if parametrized_callback.raw else processed_message\n            self.callback_executor.submit(\n                self._safe_callback_wrapper, parametrized_callback.callback, payload\n            )\n\n    def general_callback_preprocessor(self, message: Any) -&gt; T:\n        \"\"\"Preprocessor for general callback used to transform any message to a BaseMessage.\n\n        Parameters\n        ----------\n        message : Any\n            The message to preprocess.\n\n        Returns\n        -------\n        T\n            The preprocessed message.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def call_service(\n        self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n    ) -&gt; BaseMessage:\n        \"\"\"\n        Alias for service_call method.\n        \"\"\"\n        return self.service_call(message, target, timeout_sec, **kwargs)\n\n    def service_call(\n        self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n    ) -&gt; BaseMessage:\n        \"\"\"Implements request-response pattern.\n\n        Sends a request and waits for a response. The target parameter\n        specifies the service endpoint to call.\n\n        Parameters\n        ----------\n        message : T\n            The request message to send.\n        target : str\n            The service endpoint to call.\n        timeout_sec : float\n            Timeout in seconds for the service call.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        BaseMessage\n            The response message.\n\n        Raises\n        ------\n        ConnectorException\n            If the service call cannot be made.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def create_service(\n        self,\n        service_name: str,\n        on_request: Callable,\n        on_done: Optional[Callable] = None,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Sets up a service endpoint for handling requests.\n\n        Creates a service that can receive and process requests.\n        The on_request callback handles incoming requests,\n        and on_done (if provided) is called when the service is terminated.\n\n        Parameters\n        ----------\n        service_name : str\n            Name of the service to create.\n        on_request : Callable\n            Callback function to handle incoming requests.\n        on_done : Optional[Callable], optional\n            Callback function called when service is terminated, by default None.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The handle of the created service.\n\n        Raises\n        ------\n        ConnectorException\n            If the service cannot be created.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def create_action(\n        self,\n        action_name: str,\n        generate_feedback_callback: Callable,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Sets up an action endpoint for long-running operations.\n\n        Creates an action that can be started and monitored.\n        The generate_feedback_callback is used to provide progress updates\n        during the action's execution.\n\n        Parameters\n        ----------\n        action_name : str\n            Name of the action to create.\n        generate_feedback_callback : Callable\n            Callback function to generate feedback during action execution.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The handle of the created action.\n\n        Raises\n        ------\n        ConnectorException\n            If the action cannot be created.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def start_action(\n        self,\n        action_data: Optional[T],\n        target: str,\n        on_feedback: Callable,\n        on_done: Callable,\n        timeout_sec: float,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Initiates a long-running operation with feedback.\n\n        Starts an action and provides callbacks for feedback and completion.\n        The on_feedback callback receives progress updates,\n        and on_done is called when the action completes.\n\n        Parameters\n        ----------\n        action_data : Optional[T]\n            Data to pass to the action.\n        target : str\n            The action endpoint to start.\n        on_feedback : Callable\n            Callback function to receive progress updates.\n        on_done : Callable\n            Callback function called when action completes.\n        timeout_sec : float\n            Timeout in seconds for the action.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The handle of the started action.\n\n        Raises\n        ------\n        ConnectorException\n            If the action cannot be started.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def terminate_action(self, action_handle: str, **kwargs: Optional[Any]) -&gt; Any:\n        \"\"\"Cancels an ongoing action.\n\n        Stops the execution of a previously started action.\n        The action_handle identifies which action to terminate.\n\n        Parameters\n        ----------\n        action_handle : str\n            The handle of the action to terminate.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        Any\n            Result of the termination operation.\n\n        Raises\n        ------\n        ConnectorException\n            If the action cannot be terminated.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def shutdown(self):\n        \"\"\"Shuts down the connector and releases all resources.\"\"\"\n        self.callback_executor.shutdown(wait=True)\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.call_service","title":"<code>call_service(message, target, timeout_sec, **kwargs)</code>","text":"<p>Alias for service_call method.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def call_service(\n    self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n) -&gt; BaseMessage:\n    \"\"\"\n    Alias for service_call method.\n    \"\"\"\n    return self.service_call(message, target, timeout_sec, **kwargs)\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.create_action","title":"<code>create_action(action_name, generate_feedback_callback, **kwargs)</code>","text":"<p>Sets up an action endpoint for long-running operations.</p> <p>Creates an action that can be started and monitored. The generate_feedback_callback is used to provide progress updates during the action's execution.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action to create.</p> required <code>generate_feedback_callback</code> <code>Callable</code> <p>Callback function to generate feedback during action execution.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The handle of the created action.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the action cannot be created.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def create_action(\n    self,\n    action_name: str,\n    generate_feedback_callback: Callable,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Sets up an action endpoint for long-running operations.\n\n    Creates an action that can be started and monitored.\n    The generate_feedback_callback is used to provide progress updates\n    during the action's execution.\n\n    Parameters\n    ----------\n    action_name : str\n        Name of the action to create.\n    generate_feedback_callback : Callable\n        Callback function to generate feedback during action execution.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The handle of the created action.\n\n    Raises\n    ------\n    ConnectorException\n        If the action cannot be created.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.create_service","title":"<code>create_service(service_name, on_request, on_done=None, **kwargs)</code>","text":"<p>Sets up a service endpoint for handling requests.</p> <p>Creates a service that can receive and process requests. The on_request callback handles incoming requests, and on_done (if provided) is called when the service is terminated.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Name of the service to create.</p> required <code>on_request</code> <code>Callable</code> <p>Callback function to handle incoming requests.</p> required <code>on_done</code> <code>Optional[Callable]</code> <p>Callback function called when service is terminated, by default None.</p> <code>None</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The handle of the created service.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the service cannot be created.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def create_service(\n    self,\n    service_name: str,\n    on_request: Callable,\n    on_done: Optional[Callable] = None,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Sets up a service endpoint for handling requests.\n\n    Creates a service that can receive and process requests.\n    The on_request callback handles incoming requests,\n    and on_done (if provided) is called when the service is terminated.\n\n    Parameters\n    ----------\n    service_name : str\n        Name of the service to create.\n    on_request : Callable\n        Callback function to handle incoming requests.\n    on_done : Optional[Callable], optional\n        Callback function called when service is terminated, by default None.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The handle of the created service.\n\n    Raises\n    ------\n    ConnectorException\n        If the service cannot be created.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.general_callback_preprocessor","title":"<code>general_callback_preprocessor(message)</code>","text":"<p>Preprocessor for general callback used to transform any message to a BaseMessage.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>The message to preprocess.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The preprocessed message.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def general_callback_preprocessor(self, message: Any) -&gt; T:\n    \"\"\"Preprocessor for general callback used to transform any message to a BaseMessage.\n\n    Parameters\n    ----------\n    message : Any\n        The message to preprocess.\n\n    Returns\n    -------\n    T\n        The preprocessed message.\n\n    Raises\n    ------\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.receive_message","title":"<code>receive_message(source, timeout_sec, **kwargs)</code>","text":"<p>Implements subscribe pattern.</p> <p>Receives a message from a publisher. The source parameter can be used to specify the source or topic to subscribe to.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source or topic to receive the message from.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds for receiving the message.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The received message.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the message cannot be received.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def receive_message(\n    self, source: str, timeout_sec: float, **kwargs: Optional[Any]\n) -&gt; T:\n    \"\"\"Implements subscribe pattern.\n\n    Receives a message from a publisher. The source parameter\n    can be used to specify the source or topic to subscribe to.\n\n    Parameters\n    ----------\n    source : str\n        The source or topic to receive the message from.\n    timeout_sec : float\n        Timeout in seconds for receiving the message.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    T\n        The received message.\n\n    Raises\n    ------\n    ConnectorException\n        If the message cannot be received.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.register_callback","title":"<code>register_callback(source, callback, raw=False, **kwargs)</code>","text":"<p>Implements register callback.</p> <p>Registers a callback to be called when a message is received from a source. If raw is False, the callback will receive a T object. If raw is True, the callback will receive the raw message.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source to register the callback for.</p> required <code>callback</code> <code>Callable[[T | Any], None]</code> <p>The callback function to register.</p> required <code>raw</code> <code>bool</code> <p>Whether to pass raw message to callback, by default False.</p> <code>False</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the registered callback.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the callback cannot be registered.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def register_callback(\n    self,\n    source: str,\n    callback: Callable[[T | Any], None],\n    raw: bool = False,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Implements register callback.\n\n    Registers a callback to be called when a message is received from a source.\n    If raw is False, the callback will receive a T object.\n    If raw is True, the callback will receive the raw message.\n\n    Parameters\n    ----------\n    source : str\n        The source to register the callback for.\n    callback : Callable[[T | Any], None]\n        The callback function to register.\n    raw : bool, optional\n        Whether to pass raw message to callback, by default False.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The ID of the registered callback.\n\n    Raises\n    ------\n    ConnectorException\n        If the callback cannot be registered.\n    \"\"\"\n    parametrized_callback = ParametrizedCallback[T](callback=callback, raw=raw)\n    self.registered_callbacks[source][parametrized_callback.id] = (\n        parametrized_callback\n    )\n    self.callback_id_mapping[parametrized_callback.id] = (\n        source,\n        parametrized_callback,\n    )\n    return parametrized_callback.id\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.send_message","title":"<code>send_message(message, target, **kwargs)</code>","text":"<p>Implements publish pattern.</p> <p>Sends a message to one or more subscribers. The target parameter can be used to specify the destination or topic.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>T</code> <p>The message to send.</p> required <code>target</code> <code>str</code> <p>The destination or topic to send the message to.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the message cannot be sent.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def send_message(self, message: T, target: str, **kwargs: Optional[Any]) -&gt; None:\n    \"\"\"Implements publish pattern.\n\n    Sends a message to one or more subscribers. The target parameter\n    can be used to specify the destination or topic.\n\n    Parameters\n    ----------\n    message : T\n        The message to send.\n    target : str\n        The destination or topic to send the message to.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Raises\n    ------\n    ConnectorException\n        If the message cannot be sent.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.service_call","title":"<code>service_call(message, target, timeout_sec, **kwargs)</code>","text":"<p>Implements request-response pattern.</p> <p>Sends a request and waits for a response. The target parameter specifies the service endpoint to call.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>T</code> <p>The request message to send.</p> required <code>target</code> <code>str</code> <p>The service endpoint to call.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds for the service call.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseMessage</code> <p>The response message.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the service call cannot be made.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def service_call(\n    self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n) -&gt; BaseMessage:\n    \"\"\"Implements request-response pattern.\n\n    Sends a request and waits for a response. The target parameter\n    specifies the service endpoint to call.\n\n    Parameters\n    ----------\n    message : T\n        The request message to send.\n    target : str\n        The service endpoint to call.\n    timeout_sec : float\n        Timeout in seconds for the service call.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    BaseMessage\n        The response message.\n\n    Raises\n    ------\n    ConnectorException\n        If the service call cannot be made.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.shutdown","title":"<code>shutdown()</code>","text":"<p>Shuts down the connector and releases all resources.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def shutdown(self):\n    \"\"\"Shuts down the connector and releases all resources.\"\"\"\n    self.callback_executor.shutdown(wait=True)\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.start_action","title":"<code>start_action(action_data, target, on_feedback, on_done, timeout_sec, **kwargs)</code>","text":"<p>Initiates a long-running operation with feedback.</p> <p>Starts an action and provides callbacks for feedback and completion. The on_feedback callback receives progress updates, and on_done is called when the action completes.</p> <p>Parameters:</p> Name Type Description Default <code>action_data</code> <code>Optional[T]</code> <p>Data to pass to the action.</p> required <code>target</code> <code>str</code> <p>The action endpoint to start.</p> required <code>on_feedback</code> <code>Callable</code> <p>Callback function to receive progress updates.</p> required <code>on_done</code> <code>Callable</code> <p>Callback function called when action completes.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds for the action.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The handle of the started action.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the action cannot be started.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def start_action(\n    self,\n    action_data: Optional[T],\n    target: str,\n    on_feedback: Callable,\n    on_done: Callable,\n    timeout_sec: float,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Initiates a long-running operation with feedback.\n\n    Starts an action and provides callbacks for feedback and completion.\n    The on_feedback callback receives progress updates,\n    and on_done is called when the action completes.\n\n    Parameters\n    ----------\n    action_data : Optional[T]\n        Data to pass to the action.\n    target : str\n        The action endpoint to start.\n    on_feedback : Callable\n        Callback function to receive progress updates.\n    on_done : Callable\n        Callback function called when action completes.\n    timeout_sec : float\n        Timeout in seconds for the action.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The handle of the started action.\n\n    Raises\n    ------\n    ConnectorException\n        If the action cannot be started.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.terminate_action","title":"<code>terminate_action(action_handle, **kwargs)</code>","text":"<p>Cancels an ongoing action.</p> <p>Stops the execution of a previously started action. The action_handle identifies which action to terminate.</p> <p>Parameters:</p> Name Type Description Default <code>action_handle</code> <code>str</code> <p>The handle of the action to terminate.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result of the termination operation.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the action cannot be terminated.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def terminate_action(self, action_handle: str, **kwargs: Optional[Any]) -&gt; Any:\n    \"\"\"Cancels an ongoing action.\n\n    Stops the execution of a previously started action.\n    The action_handle identifies which action to terminate.\n\n    Parameters\n    ----------\n    action_handle : str\n        The handle of the action to terminate.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    Any\n        Result of the termination operation.\n\n    Raises\n    ------\n    ConnectorException\n        If the action cannot be terminated.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.unregister_callback","title":"<code>unregister_callback(callback_id)</code>","text":"<p>Unregisters a callback from a source.</p> <p>Parameters:</p> Name Type Description Default <code>callback_id</code> <code>str</code> <p>The id of the callback to unregister.</p> required <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the callback cannot be unregistered.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def unregister_callback(self, callback_id: str) -&gt; None:\n    \"\"\"Unregisters a callback from a source.\n\n    Parameters\n    ----------\n    callback_id : str\n        The id of the callback to unregister.\n\n    Raises\n    ------\n    ConnectorException\n        If the callback cannot be unregistered.\n    \"\"\"\n    if callback_id not in self.callback_id_mapping:\n        raise ConnectorException(f\"Callback with id {callback_id} not found.\")\n\n    source, _ = self.callback_id_mapping[callback_id]\n    del self.registered_callbacks[source][callback_id]\n    del self.callback_id_mapping[callback_id]\n</code></pre>"},{"location":"API_documentation/connectors/overview/#hriconnectort","title":"HRIConnector[T]","text":"HRIConnector class definition <p>Extends BaseConnector with Human-Robot Interaction capabilities:</p> <ul> <li>Supports multimodal messages (text, images, audio)</li> <li>Provides conversion to/from Langchain message formats</li> <li>Handles message sequencing and conversation IDs</li> </ul>"},{"location":"API_documentation/connectors/overview/#rai.communication.hri_connector.HRIConnector","title":"<code>rai.communication.hri_connector.HRIConnector</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>BaseConnector[T]</code></p> <p>Base class for Human-Robot Interaction (HRI) connectors. Used for sending and receiving messages between human and robot from various sources.</p> Source code in <code>rai/communication/hri_connector.py</code> <pre><code>class HRIConnector(Generic[T], BaseConnector[T]):\n    \"\"\"\n    Base class for Human-Robot Interaction (HRI) connectors.\n    Used for sending and receiving messages between human and robot from various sources.\n    \"\"\"\n\n    def build_message(\n        self,\n        message: LangchainBaseMessage | RAIMultimodalMessage,\n        communication_id: Optional[str] = None,\n        seq_no: int = 0,\n        seq_end: bool = False,\n    ) -&gt; T:\n        \"\"\"\n        Build a new message object from a given input message.\n\n        Parameters\n        ----------\n        message : LangchainBaseMessage or RAIMultimodalMessage\n            The source message to transform into the target type.\n        communication_id : str, optional\n            An optional identifier for the communication session. Defaults to `None`.\n        seq_no : int, optional\n            The sequence number of the message in the communication stream. Defaults to `0`.\n        seq_end : bool, optional\n            Flag indicating whether this message is the final one in the sequence. Defaults to `False`.\n\n        Returns\n        -------\n        T\n            A message instance of type `T` compatible with the Connector, created from the provided input.\n\n        Notes\n        -----\n        This method uses `self.T_class.from_langchain` for conversion and assumes compatibility.\n        \"\"\"\n        return self.T_class.from_langchain(message, communication_id, seq_no, seq_end)  # type: ignore\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.hri_connector.HRIConnector.build_message","title":"<code>build_message(message, communication_id=None, seq_no=0, seq_end=False)</code>","text":"<p>Build a new message object from a given input message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>BaseMessage or MultimodalMessage</code> <p>The source message to transform into the target type.</p> required <code>communication_id</code> <code>str</code> <p>An optional identifier for the communication session. Defaults to <code>None</code>.</p> <code>None</code> <code>seq_no</code> <code>int</code> <p>The sequence number of the message in the communication stream. Defaults to <code>0</code>.</p> <code>0</code> <code>seq_end</code> <code>bool</code> <p>Flag indicating whether this message is the final one in the sequence. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>A message instance of type <code>T</code> compatible with the Connector, created from the provided input.</p> Notes <p>This method uses <code>self.T_class.from_langchain</code> for conversion and assumes compatibility.</p> Source code in <code>rai/communication/hri_connector.py</code> <pre><code>def build_message(\n    self,\n    message: LangchainBaseMessage | RAIMultimodalMessage,\n    communication_id: Optional[str] = None,\n    seq_no: int = 0,\n    seq_end: bool = False,\n) -&gt; T:\n    \"\"\"\n    Build a new message object from a given input message.\n\n    Parameters\n    ----------\n    message : LangchainBaseMessage or RAIMultimodalMessage\n        The source message to transform into the target type.\n    communication_id : str, optional\n        An optional identifier for the communication session. Defaults to `None`.\n    seq_no : int, optional\n        The sequence number of the message in the communication stream. Defaults to `0`.\n    seq_end : bool, optional\n        Flag indicating whether this message is the final one in the sequence. Defaults to `False`.\n\n    Returns\n    -------\n    T\n        A message instance of type `T` compatible with the Connector, created from the provided input.\n\n    Notes\n    -----\n    This method uses `self.T_class.from_langchain` for conversion and assumes compatibility.\n    \"\"\"\n    return self.T_class.from_langchain(message, communication_id, seq_no, seq_end)  # type: ignore\n</code></pre>"},{"location":"API_documentation/connectors/overview/#concrete-implementations","title":"Concrete Implementations","text":"Connector Description Documentation Link ROS 2 Connectors Robot Operating System 2 integration ROS2 Connectors"},{"location":"API_documentation/connectors/overview/#key-features","title":"Key Features","text":""},{"location":"API_documentation/connectors/overview/#message-types","title":"Message Types","text":"<p>Connectors are generic over message types derived from BaseMessage:</p> <ul> <li>BaseMessage: Foundation message type with payload and metadata</li> <li>ROS2Message: Message type for ROS 2 communication</li> <li>HRIMessage: Multimodal message type with text, images, and audio</li> <li>ROS2HRIMessage: HRIMessage specialized for ROS 2 transport</li> <li>SoundDeviceMessage: Specialized message for audio operations</li> </ul>"},{"location":"API_documentation/connectors/overview/#communication-patterns","title":"Communication Patterns","text":"<p>Connectors support multiple communication patterns:</p> <ol> <li> <p>Publish/Subscribe</p> <ul> <li><code>send_message(message, target, **kwargs)</code>: Send a message to a target</li> <li><code>receive_message(source, timeout_sec, **kwargs)</code>: Receive a message from a source</li> <li><code>register_callback(source, callback, **kwargs)</code>: Register for asynchronous notifications</li> </ul> </li> <li> <p>Request/Response</p> <ul> <li><code>service_call(message, target, timeout_sec, **kwargs)</code>: Make a synchronous service call</li> </ul> </li> <li> <p>Actions</p> <ul> <li><code>start_action(action_data, target, on_feedback, on_done, timeout_sec, **kwargs)</code>: Start a     long-running action</li> <li><code>terminate_action(action_handle, **kwargs)</code>: Cancel an ongoing action</li> <li><code>create_action(action_name, generate_feedback_callback, **kwargs)</code>: Create an action server</li> </ul> </li> </ol>"},{"location":"API_documentation/connectors/overview/#threading-model","title":"Threading Model","text":"<p>Connectors implement thread-safe operations:</p> <ul> <li>ROS 2 connectors use a dedicated thread with MultiThreadedExecutor</li> <li>Callbacks are executed in a ThreadPoolExecutor for concurrent processing</li> <li>Proper synchronization for shared resources</li> <li>Clean shutdown handling for all resources</li> </ul>"},{"location":"API_documentation/connectors/overview/#usage-examples","title":"Usage Examples","text":"Connector Example Usage Documentation ROS 2 ROS2 Connectors"},{"location":"API_documentation/connectors/overview/#error-handling","title":"Error Handling","text":"<p>Connectors implement robust error handling:</p> <ul> <li>All operations have appropriate timeout parameters</li> <li>Exceptions are properly propagated and documented</li> <li>Callbacks are executed in a protected manner to prevent crashes</li> <li>Resources are properly cleaned up during shutdown</li> </ul>"},{"location":"API_documentation/connectors/overview/#see-also","title":"See Also","text":"<ul> <li>Agents: For more information on the different types of agents in RAI</li> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/","title":"Tools","text":"<p>RAI provides various ROS 2 tools, both generic (mimics ros2cli) and specific (e.g., nav2, moveit2, etc.)</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#class-definition","title":"Class Definition","text":"<p><code>BaseROS2Tool</code> is the base class for all ROS 2 tools. It provides a common interface for all ROS 2 tools including name allowlist/blocklist for all the communication protocols (messages, services, actions)</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#class-definition_1","title":"Class Definition","text":""},{"location":"API_documentation/langchain_integration/ROS_2_tools/#rai.tools.ros2.base.BaseROS2Tool","title":"<code>rai.tools.ros2.base.BaseROS2Tool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>Base class for all ROS2 tools.</p> <p>Attributes:</p> Name Type Description <code>connector</code> <code>ROS2Connector</code> <p>The connector to the ROS 2 system.</p> <code>readable</code> <code>Optional[List[str]]</code> <p>The topics that can be read. If the list is not provided, all topics can be read.</p> <code>writable</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that can be written. If the list is not provided, all topics/actions/services can be written.</p> <code>forbidden</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that are forbidden to read and write.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>The description of the tool.</p> Source code in <code>rai/tools/ros2/base.py</code> <pre><code>class BaseROS2Tool(BaseTool):\n    \"\"\"\n    Base class for all ROS2 tools.\n\n    Attributes\n    ----------\n    connector : ROS2Connector\n        The connector to the ROS 2 system.\n    readable : Optional[List[str]]\n        The topics that can be read. If the list is not provided, all topics can be read.\n    writable : Optional[List[str]]\n        The names (topics/actions/services) that can be written. If the list is not provided, all topics/actions/services can be written.\n    forbidden : Optional[List[str]]\n        The names (topics/actions/services) that are forbidden to read and write.\n    name : str\n        The name of the tool.\n    description : str\n        The description of the tool.\n    \"\"\"\n\n    connector: ROS2Connector\n    readable: Optional[List[str]] = None\n    writable: Optional[List[str]] = None\n    forbidden: Optional[List[str]] = None\n\n    name: str = \"\"\n    description: str = \"\"\n\n    def is_readable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.readable is None:\n            return True\n        return topic in self.readable\n\n    def is_writable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.writable is None:\n            return True\n        return topic in self.writable\n\n    def _check_permission_and_include(\n        self, name: str, check_readable: bool = True\n    ) -&gt; Tuple[bool, bool, bool]:\n        \"\"\"\n        Check permissions and determine if resource should be included.\n\n        Args:\n            name: Resource name (topic/service/action)\n            check_readable: If False, only checks writable (for services/actions).\n                            If True, checks both readable and writable (for topics).\n\n        Returns:\n            (should_include, is_readable, is_writable)\n        \"\"\"\n        # Skip forbidden resources\n        if self.forbidden is not None and name in self.forbidden:\n            return (False, False, False)\n\n        is_readable_resource = self.is_readable(name) if check_readable else False\n        is_writable_resource = self.is_writable(name)\n\n        # Determine if resource should be included based on whitelist state\n        # If only readable is set: include only if resource is in readable list\n        # If only writable is set: include only if resource is in writable list\n        # If both are set: include if resource is in readable OR writable list\n        # If neither is set: include all resources (except forbidden)\n        should_include = True\n        if check_readable and self.readable is not None and self.writable is not None:\n            # Both whitelists are set, resource must be in at least one\n            should_include = is_readable_resource or is_writable_resource\n        elif check_readable and self.readable is not None:\n            # Only readable whitelist is set, resource must be readable\n            should_include = is_readable_resource\n        elif self.writable is not None:\n            # Only writable whitelist is set, resource must be writable\n            should_include = is_writable_resource\n\n        return (should_include, is_readable_resource, is_writable_resource)\n\n    def _categorize(self, is_readable: bool, is_writable: bool) -&gt; Optional[str]:\n        \"\"\"\n        Categorize resource into readable, writable, or both.\n\n        Returns:\n            Category name: \"readable_and_writable\", \"readable\", \"writable\", or None\n        \"\"\"\n        if is_readable and is_writable:\n            return \"readable_and_writable\"\n        elif is_readable:\n            return \"readable\"\n        elif is_writable:\n            return \"writable\"\n        return None\n</code></pre>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#security-model","title":"Security Model","text":"<p>ROS2 tools implement a security model using three access control parameters:</p> <ul> <li> <p><code>readable</code> (Allowlist for Reading): Whitelist of topics the agent can read/subscribe to</p> <ul> <li>If <code>None</code>: all topics are readable (permissive)</li> <li>If set: only topics in the list are readable (restrictive)</li> <li>Note: Only applies to topics; services and actions are not checked against this parameter</li> </ul> </li> <li> <p><code>writable</code> (Allowlist for Writing): Whitelist of topics/actions/services the agent can write/publish to</p> <ul> <li>If <code>None</code>: all topics/actions/services are writable (permissive)</li> <li>If set: only topics/actions/services in the list are writable (restrictive)</li> </ul> </li> <li> <p><code>forbidden</code> (Denylist): Blacklist of topics/actions/services that are always denied</p> <ul> <li>Highest priority - checked first and overrides both <code>readable</code> and <code>writable</code></li> <li>If a resource is forbidden, it cannot be accessed regardless of allowlists</li> </ul> </li> </ul> <p>Priority Order: <code>forbidden</code> &gt; <code>readable</code>/<code>writable</code> &gt; default (all allowed)</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#usage-example","title":"Usage example","text":"<pre><code>from rai.communication.ros2 import ROS2Connector\nfrom rai.tools.ros2.base import BaseROS2Tool\n\nconnector = ROS2Connector()\n\nBaseROS2Tool( # BaseROS2Tool cannot be used directly, this is just an example\n    connector=connector,\n    readable=[\"/odom\", \"/scan\"], # readable topics\n    writable=[\"/robot_position\"], # writable topics, services and actions\n    forbidden=[\"/cmd_vel\"], # forbidden topics, services and actions\n)\n</code></pre>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#generic-ros-2-tools","title":"Generic ROS 2 Tools","text":"<p>RAI provides a generic ROS 2 toolkit, which allows the Agent to interact with any ROS 2 topics, services and actions.</p> <pre><code>from rai.tools.ros2 import ROS2Toolkit\nfrom rai.communication.ros2 import ROS2Connector\n\nconnector = ROS2Connector()\ntools = ROS2Toolkit(connector=connector).get_tools()\n</code></pre> <p>Below is the list of tools provided by the generic ROS 2 toolkit that can also be used as standalone tools (except for the ROS 2 action tools, which should be used via the <code>ROS2ActionToolkit</code> as they share a state):</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#topics","title":"Topics","text":"Tool Name Description <code>PublishROS2MessageTool</code> Tool for publishing messages to ROS 2 topics <code>ReceiveROS2MessageTool</code> Tool for receiving messages from ROS 2 topics <code>GetROS2ImageTool</code> Tool for retrieving image data from topics <code>GetROS2TopicsNamesAndTypesTool</code> Tool for listing all available topics and their types <code>GetROS2MessageInterfaceTool</code> Tool for getting message interface information <code>GetROS2TransformTool</code> Tool for retrieving transform data"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#getros2topicsnamesandtypestool-behavior","title":"GetROS2TopicsNamesAndTypesTool Behavior","text":"<p><code>GetROS2TopicsNamesAndTypesTool</code> lists ROS2 topics and their types with filtering and categorization based on <code>readable</code>, <code>writable</code>, and <code>forbidden</code> parameters:</p> <p>Filtering Logic:</p> <ul> <li>Forbidden topics: Always excluded regardless of allowlists</li> <li>When only <code>readable</code> is set: Only topics in the readable list are included</li> <li>When only <code>writable</code> is set: Only topics in the writable list are included</li> <li>When both <code>readable</code> and <code>writable</code> are set: Topics in readable OR writable lists are included (OR logic)</li> <li>When neither is set: All topics are included (except forbidden)</li> </ul> <p>Output Format:</p> <ul> <li>No restrictions: Simple list of all topics</li> <li>With restrictions: Topics are categorized and displayed in sections:<ol> <li>Topics in both readable and writable lists (no section header)</li> <li>\"Readable topics:\" section (topics only in readable list)</li> <li>\"Writable topics:\" section (topics only in writable list)</li> </ol> </li> </ul> <p>Example:</p> <pre><code>tool = GetROS2TopicsNamesAndTypesTool(\n    connector=connector,\n    readable=[\"/odom\", \"/scan\", \"/camera/image\"],\n    writable=[\"/goal_pose\", \"/scan\"],\n    forbidden=[\"/cmd_vel\"]\n)\n# /scan appears in first section (both readable and writable)\n# /odom and /camera/image appear in \"Readable topics:\" section\n# /goal_pose appears in \"Writable topics:\" section\n# /cmd_vel is excluded (forbidden)\n</code></pre>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#services","title":"Services","text":"Tool Name Description <code>GetROS2ServicesNamesAndTypesTool</code> Tool for listing all available services and their types <code>CallROS2ServiceTool</code> Tool for calling ROS 2 services"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#actions","title":"Actions","text":"Tool Name Description <code>GetROS2ActionsNamesAndTypesTool</code> Tool for listing all available actions and their types <code>StartROS2ActionTool</code> Tool for starting ROS 2 actions <code>GetROS2ActionFeedbackTool</code> Tool for retrieving action feedback <code>GetROS2ActionResultTool</code> Tool for retrieving action results <code>CancelROS2ActionTool</code> Tool for canceling running actions <code>GetROS2ActionIDsTool</code> Tool for getting action IDs"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#specific-ros-2-tools","title":"Specific ROS 2 Tools","text":"<p>RAI provides specific ROS 2 tools for certain ROS 2 packages.</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#nav2","title":"Nav2","text":"Tool Name Description <code>NavigateToPoseTool</code> Tool for navigating to a pose <code>GetNavigateToPoseFeedbackTool</code> Tool for retrieving the feedback of a navigate to pose action <code>GetNavigateToPoseResultTool</code> Tool for retrieving the result of a navigate to pose action <code>CancelNavigateToPoseTool</code> Tool for canceling a navigate to pose action <code>GetOccupancyGridTool</code> Tool for retrieving the occupancy grid"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#custom-tools","title":"Custom Tools","text":"Tool Name Description <code>MoveToPointTool</code> Tool for moving to a point <code>MoveObjectFromToTool</code> Tool for moving an object from one point to another <code>GetObjectPositionsTool</code> Tool for retrieving the positions of objects"},{"location":"API_documentation/langchain_integration/multimodal_messages/","title":"Multimodality support","text":"<p>RAI implements <code>MultimodalMessage</code> that allows using image and audio* information in langchain.</p> <p>Audio is not fully supported yet</p> <p>Audio is currently added as a placeholder for future implementation.</p>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#class-definition","title":"Class Definition","text":"<p>LangChain supports multimodal data by default. This is done by expanding the content section from string to dictionary, containing specific keys. To make it easier to use, RAI implements a <code>MultimodalMessage</code> class, which is a wrapper around the <code>BaseMessage</code> class.</p>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#class-definition_1","title":"Class Definition","text":""},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.MultimodalMessage","title":"<code>rai.messages.multimodal.MultimodalMessage</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>Base class for multimodal messages.</p> <p>Attributes:</p> Name Type Description <code>images</code> <code>Optional[List[str]]</code> <p>List of base64 encoded images.</p> <code>audios</code> <code>Optional[Any]</code> <p>List of base64 encoded audios.</p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class MultimodalMessage(BaseMessage):\n    \"\"\"\n    Base class for multimodal messages.\n\n    Attributes\n    ----------\n    images : Optional[List[str]]\n        List of base64 encoded images.\n    audios : Optional[Any]\n        List of base64 encoded audios.\n    \"\"\"\n\n    images: Optional[List[str]] = None\n    audios: Optional[Any] = None\n\n    def __init__(\n        self,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)  # type: ignore\n\n        if self.audios not in [None, []]:\n            raise ValueError(\"Audio is not yet supported\")\n\n        _content: List[Union[str, Dict[str, Union[Dict[str, str], str]]]] = []\n\n        if isinstance(self.content, str):\n            _content.append({\"type\": \"text\", \"text\": self.content})\n        else:\n            raise ValueError(\"Content must be a string\")  # for now, to guarantee compat\n\n        if isinstance(self.images, list):\n            _image_content = [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{image}\",\n                    },\n                }\n                for image in self.images\n            ]\n            _content.extend(_image_content)\n        self.content = _content\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#subclasses","title":"Subclasses","text":""},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.HumanMultimodalMessage","title":"<code>rai.messages.multimodal.HumanMultimodalMessage</code>","text":"<p>               Bases: <code>HumanMessage</code>, <code>MultimodalMessage</code></p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class HumanMultimodalMessage(HumanMessage, MultimodalMessage):\n    def __repr_args__(self) -&gt; Any:\n        args = super().__repr_args__()\n        new_args = []\n        for k, v in args:\n            if k == \"content\":\n                v = [c for c in v if c[\"type\"] != \"image_url\"]\n            elif k == \"images\":\n                imgs_summary = [image[0:10] + \"...\" for image in v]\n                v = f\"{len(v)} base64 encoded images: [{', '.join(imgs_summary)}]\"\n            new_args.append((k, v))\n        return new_args\n\n    def _no_img_content(self):\n        return [c for c in self.content if c[\"type\"] != \"image_url\"]\n\n    def pretty_repr(self, html: bool = False) -&gt; str:\n        title = get_msg_title_repr(self.type.title() + \" Message\", bold=html)\n        # TODO: handle non-string content.\n        if self.name is not None:\n            title += f\"\\nName: {self.name}\"\n        return f\"{title}\\n\\n{self._no_img_content()}\"\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.AIMultimodalMessage","title":"<code>rai.messages.multimodal.AIMultimodalMessage</code>","text":"<p>               Bases: <code>AIMessage</code>, <code>MultimodalMessage</code></p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class AIMultimodalMessage(AIMessage, MultimodalMessage):\n    pass\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.SystemMultimodalMessage","title":"<code>rai.messages.multimodal.SystemMultimodalMessage</code>","text":"<p>               Bases: <code>SystemMessage</code>, <code>MultimodalMessage</code></p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class SystemMultimodalMessage(SystemMessage, MultimodalMessage):\n    pass\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.ToolMultimodalMessage","title":"<code>rai.messages.multimodal.ToolMultimodalMessage</code>","text":"<p>               Bases: <code>ToolMessage</code>, <code>MultimodalMessage</code></p> Note <p>When any subclass of this class is used with LangGraph agents, use <code>rai.agents.langchain.core import ToolRunner</code> as the tool runner, as it automatically handles multimodal ToolMessages as well as converts them to a format that is compatible with the vendor. </p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class ToolMultimodalMessage(ToolMessage, MultimodalMessage):\n    \"\"\"\n\n    Note\n    ----\n    When any subclass of this class is used with LangGraph agents, use\n    `rai.agents.langchain.core import ToolRunner` as the tool runner, as it automatically\n    handles multimodal ToolMessages as well as converts them to a format\n    that is compatible with the vendor.\n    ::: rai.agents.langchain.core.ToolRunner\n    \"\"\"\n\n    def postprocess(self, format: Literal[\"openai\", \"bedrock\"] = \"openai\"):\n        if format == \"openai\":\n            return self._postprocess_openai()\n        elif format == \"bedrock\":\n            return self._postprocess_bedrock()\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n    def _postprocess_openai(self):\n        \"\"\"OpenAI does not allow images in the tool message.\n        Functions dumps the message into human multimodal message and tool message.\n        \"\"\"\n        if isinstance(self.images, list):\n            human_message = HumanMultimodalMessage(\n                content=f\"Image returned by a tool call {self.tool_call_id}\",\n                images=self.images,\n                tool_call_id=self.tool_call_id,\n            )\n            # at this point self.content is a list of dicts\n            # we need to extract the text from each dict\n            tool_message = ToolMultimodalMessage(\n                tool_call_id=self.tool_call_id,\n                name=self.name,\n                content=\" \".join([part.get(\"text\", \"\") for part in self.content]),\n            )\n            return [tool_message, human_message]\n        else:\n            # TODO(maciejmajek): find out if content can be a list\n            return ToolMessage(tool_call_id=self.tool_call_id, content=self.content)\n\n    def _postprocess_bedrock(self):\n        return self._postprocess_openai()\n        # https://github.com/langchain-ai/langchain-aws/issues/75\n        # at this moment im not sure if bedrock supports images in the tool message\n        content = self.content\n        # bedrock expects image and not image_url\n        content[1][\"type\"] = \"image\"\n        content[1][\"image\"] = content[1].pop(\"image_url\")\n        content[1][\"image\"][\"source\"] = content[1][\"image\"].pop(\"url\")\n\n        return ToolMessage(tool_call_id=self.tool_call_id, content=content)\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.agents.langchain.core.ToolRunner","title":"<code>rai.agents.langchain.core.ToolRunner</code>","text":"<p>               Bases: <code>RunnableCallable</code></p> Source code in <code>rai/agents/langchain/core/tool_runner.py</code> <pre><code>class ToolRunner(RunnableCallable):\n    def __init__(\n        self,\n        tools: Sequence[Union[BaseTool, Callable]],\n        *,\n        name: str = \"tools\",\n        tags: Optional[list[str]] = None,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None:\n        super().__init__(self._func, name=name, tags=tags, trace=False)\n        self.logger = logger or logging.getLogger(__name__)\n        self.tools_by_name: Dict[str, BaseTool] = {}\n        for tool_ in tools:\n            if not isinstance(tool_, BaseTool):\n                tool_ = create_tool(tool_)\n            self.tools_by_name[tool_.name] = tool_\n\n    def get_messages(self, input: dict[str, Any]) -&gt; List:\n        \"\"\"Get fields from from input that will be processed.\"\"\"\n        return input.get(\"messages\", [])\n\n    def update_input_with_outputs(\n        self, input: dict[str, Any], outputs: List[Any]\n    ) -&gt; None:\n        \"\"\"Update input with tool outputs.\"\"\"\n        input[\"messages\"].extend(outputs)\n\n    def _func(self, input: dict[str, Any], config: RunnableConfig) -&gt; Any:\n        config[\"max_concurrency\"] = (\n            1  # TODO(maciejmajek): use better mechanism for task queueing\n        )\n        messages = self.get_messages(input)\n        if not messages:\n            raise ValueError(\"No message found in input\")\n\n        message = messages[-1]\n        if not isinstance(message, AIMessage):\n            raise ValueError(\"Last message is not an AIMessage\")\n\n        def run_one(call: ToolCall):\n            self.logger.info(f\"Running tool: {call['name']}, args: {call['args']}\")\n            artifact = None\n\n            try:\n                ts = time.perf_counter()\n                output = self.tools_by_name[call[\"name\"]].invoke(call, config)  # type: ignore\n                te = time.perf_counter() - ts\n                self.logger.info(\n                    f\"Tool {call['name']} completed in {te:.2f} seconds. Tool output: {str(output.content)[:100]}{'...' if len(str(output.content)) &gt; 100 else ''}\"\n                )\n                self.logger.debug(\n                    f\"Tool {call['name']} output: \\n\\n{str(output.content)}\"\n                )\n            except ValidationError as e:\n                errors = e.errors()\n                for error in errors:\n                    error.pop(\n                        \"url\"\n                    )  # get rid of the  https://errors.pydantic.dev/... url\n\n                error_message = f\"\"\"\n                                    Validation error in tool {call[\"name\"]}:\n                                    {e.title}\n                                    Number of errors: {e.error_count()}\n                                    Errors:\n                                    {json.dumps(errors, indent=2)}\n                                \"\"\"\n                self.logger.info(error_message)\n                output = ToolMessage(\n                    content=error_message,\n                    name=call[\"name\"],\n                    tool_call_id=call[\"id\"],\n                    status=\"error\",\n                )\n            except Exception as e:\n                self.logger.info(f'Error in \"{call[\"name\"]}\", error: {e}')\n                output = ToolMessage(\n                    content=f\"Failed to run tool. Error: {e}\",\n                    name=call[\"name\"],\n                    tool_call_id=call[\"id\"],\n                    status=\"error\",\n                )\n\n            if output.artifact is not None:\n                artifact = output.artifact\n                if not isinstance(artifact, dict):\n                    raise ValueError(\n                        \"Artifact must be a dictionary with optional keys: 'images', 'audios'\"\n                    )\n\n                artifact = cast(MultimodalArtifact, artifact)\n                store_artifacts(output.tool_call_id, [artifact])\n\n            if artifact is not None and (\n                len(artifact.get(\"images\", [])) &gt; 0\n                or len(artifact.get(\"audios\", [])) &gt; 0\n            ):  # multimodal case, we currently support images and audios artifacts\n                return ToolMultimodalMessage(\n                    content=msg_content_output(output.content),\n                    name=call[\"name\"],\n                    tool_call_id=call[\"id\"],\n                    images=artifact.get(\"images\", []),\n                    audios=artifact.get(\"audios\", []),\n                )\n\n            return output\n\n        with get_executor_for_config(config) as executor:\n            raw_outputs = [*executor.map(run_one, message.tool_calls)]\n            outputs: List[Any] = []\n            for raw_output in raw_outputs:\n                if isinstance(raw_output, ToolMultimodalMessage):\n                    outputs.extend(\n                        raw_output.postprocess()\n                    )  # openai please allow tool messages with images!\n                else:\n                    outputs.append(raw_output)\n\n            # because we can't answer an aiMessage with an alternating sequence of tool and human messages\n            # we sort the messages by type so that the tool messages are sent first\n            # for more information see implementation of ToolMultimodalMessage.postprocess\n            outputs.sort(key=lambda x: x.__class__.__name__, reverse=True)\n\n            self.update_input_with_outputs(input, outputs)\n            return input\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.agents.langchain.core.ToolRunner.get_messages","title":"<code>get_messages(input)</code>","text":"<p>Get fields from from input that will be processed.</p> Source code in <code>rai/agents/langchain/core/tool_runner.py</code> <pre><code>def get_messages(self, input: dict[str, Any]) -&gt; List:\n    \"\"\"Get fields from from input that will be processed.\"\"\"\n    return input.get(\"messages\", [])\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.agents.langchain.core.ToolRunner.update_input_with_outputs","title":"<code>update_input_with_outputs(input, outputs)</code>","text":"<p>Update input with tool outputs.</p> Source code in <code>rai/agents/langchain/core/tool_runner.py</code> <pre><code>def update_input_with_outputs(\n    self, input: dict[str, Any], outputs: List[Any]\n) -&gt; None:\n    \"\"\"Update input with tool outputs.\"\"\"\n    input[\"messages\"].extend(outputs)\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#usage","title":"Usage","text":"<p>Example:</p> <pre><code>from rai.messages import HumanMultimodalMessage, preprocess_image\nfrom rai import get_llm_model # initialize your model of choice defined in config.toml\n\nbase64_image = preprocess_image('https://raw.githubusercontent.com/RobotecAI/RobotecGPULidar/develop/docs/image/rgl-logo.png')\n\nllm = get_llm_model(model_type='complex_model') # initialize your vendor of choice in config.toml\nmsg = [HumanMultimodalMessage(content='Describe the image', images=[base64_image])]\nllm.invoke(msg).pretty_print()\n\n# ================================== Ai Message ==================================\n#\n# The image features the words \"Robotec,\" \"GPU,\" and \"Lidar\" displayed in a stylized,\n# multicolored font against a black background. The text has a wavy, striped pattern,\n# incorporating red, green, and blue colors that give it a vibrantly layered appearance.\n</code></pre> <p>Implementation of the following messages is identical: HumanMultimodalMessage, SystemMultimodalMessage, AIMultimodalMessage.</p> <p>ToolMultimodalMessage usage</p> <p>Most of the vendors, do not support multimodal tool messages. <code>ToolMultimodalMessage</code> has an addition of <code>postprocess</code> method, which converts the <code>ToolMultimodalMessage</code> into format that is compatible with a chosen vendor.</p>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#see-also","title":"See Also","text":"<ul> <li>Agents: For more information on the different types of agents in RAI</li> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/langchain_integration/overview/","title":"Langchain Integration","text":""},{"location":"API_documentation/langchain_integration/overview/#overview","title":"Overview","text":"<p>RAI integrates with Langchain to enable natural language, reasoning, and multimodal capabilities in robotic applications. This API documentation describes how Langchain is used and extended within RAI, and how to leverage it for agent and tool development.</p>"},{"location":"API_documentation/langchain_integration/overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Standardized Interfaces: Consistent APIs for LLMs and tools.</li> <li>Tool Ecosystem: Use and extend tools for text and multimodal operations.</li> <li>Agent-Based Reasoning: Build agents that combine LLMs, tools, and context/memory.</li> <li>Multimodal Communication: Support for images, audio, and sensor data within messages.</li> <li>Robotic Integration: ROS 2 communication and robotic toolsets.</li> </ul>"},{"location":"API_documentation/langchain_integration/overview/#getting-started","title":"Getting Started","text":"<pre><code>from rai import get_llm_model\n\n# Initialize LLM configured in config.toml\nllm = get_llm_model(model_type='complex_model')\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#agent-and-tool-patterns","title":"Agent and Tool Patterns","text":""},{"location":"API_documentation/langchain_integration/overview/#agent-based-systems","title":"Agent-Based Systems","text":"<pre><code>from rai.agents.langchain.runnables import create_react_runnable\n\nagent = create_react_runnable(\n    llm=llm,\n    tools=[ros2_topic, get_image]\n)\nagent.invoke({\"messages\": [HumanMessage(content=\"Analyze this image\")]})\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#multimodal-communication","title":"Multimodal Communication","text":"<pre><code>from rai.messages import HumanMultimodalMessage, preprocess_image\n\nmessage = HumanMultimodalMessage(\n    content=\"Analyze this image\",\n    images=[preprocess_image(image_uri)]\n)\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#tool-integration","title":"Tool Integration","text":"<pre><code>from langchain_core.tools import tool\n\n@tool\ndef custom_operation(input: str) -&gt; str:\n    # Tool implementation\n    return result\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#multimodal-tool-example","title":"Multimodal Tool Example","text":"<pre><code>from langchain_core.tools import tool\nfrom rai.messages import MultimodalArtifact\n\n@tool(response_format=\"content_and_artifact\")\ndef custom_operation(input: str) -&gt; str:\n    # Tool implementation\n    return result, MultimodalArtifact(images=[base64_encoded_png_image])\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#best-practices","title":"Best Practices","text":"<ul> <li>Use appropriate message types for text and media</li> <li>Follow Langchain tool patterns and document capabilities</li> <li>Keep agents focused and specialized</li> </ul>"},{"location":"API_documentation/langchain_integration/overview/#see-also","title":"See Also","text":"<ul> <li>Tool tutorial: For more information on how to create custom LangChain tools</li> <li>Agents</li> <li>Aggregators</li> <li>Connectors</li> <li>Multimodal messages</li> <li>Runners</li> </ul>"},{"location":"API_documentation/runners/overview/","title":"Runners","text":"<p>There are two ways to manage the agents in RAI:</p> <ol> <li><code>AgentRunner</code> - a class for starting and stopping the agents</li> <li><code>wait_for_shutdown</code> - a function for waiting for interruption signals</li> </ol> <p>Usage of <code>Runners</code> is optional</p> <p>You can start and stop the agents manually for more control over the agents lifecycle.</p>"},{"location":"API_documentation/runners/overview/#agentrunner-class-definition","title":"AgentRunner class definition","text":""},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner","title":"<code>rai.agents.runner.AgentRunner</code>","text":"<p>Runs the agents in the background.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>List[BaseAgent]</code> <p>List of agent instances</p> required Source code in <code>rai/agents/runner.py</code> <pre><code>class AgentRunner:\n    \"\"\"Runs the agents in the background.\n\n    Parameters\n    ----------\n    agents : List[BaseAgent]\n        List of agent instances\n    \"\"\"\n\n    def __init__(self, agents: List[BaseAgent]):\n        \"\"\"Initialize the AgentRunner with a list of agents.\n\n        Parameters\n        ----------\n        agents : List[BaseAgent]\n            List of agent instances to be managed by the runner.\n        \"\"\"\n        self.agents = agents\n        self.logger = logging.getLogger(__name__)\n\n    def run(self):\n        \"\"\"Run all agents in the background.\n\n        Notes\n        -----\n        This method starts all agents by calling their `run` method.\n        It is experimental; if agents do not run properly, consider running them in separate processes.\n        \"\"\"\n        self.logger.info(\n            f\"{self.__class__.__name__}.{self.run.__name__} is an experimental function. \\\n                            If you believe that your agents are not running properly, \\\n                            please run them separately (in different processes).\"\n        )\n        for agent in self.agents:\n            agent.run()\n\n    def run_and_wait_for_shutdown(self):\n        \"\"\"Run all agents and block until a shutdown signal is received.\n\n        Notes\n        -----\n        This method starts all agents and waits for a shutdown signal (SIGINT or SIGTERM), ensuring graceful termination.\n        \"\"\"\n        self.run()\n        self.wait_for_shutdown()\n\n    def wait_for_shutdown(self):\n        \"\"\"Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.\n\n        Notes\n        -----\n        Installs signal handlers to capture SIGINT and SIGTERM. On receiving a signal, stops all managed agents.\n        \"\"\"\n        shutdown_event = Event()\n\n        def signal_handler(signum: int, frame: Optional[FrameType]):\n            shutdown_event.set()\n\n        signal.signal(signal.SIGINT, signal_handler)\n        signal.signal(signal.SIGTERM, signal_handler)\n\n        try:\n            shutdown_event.wait()\n        finally:\n            for agent in self.agents:\n                agent.stop()\n\n    def stop(self):\n        \"\"\"Stop all managed agents by calling their `stop` method.\"\"\"\n        for agent in self.agents:\n            agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.__init__","title":"<code>__init__(agents)</code>","text":"<p>Initialize the AgentRunner with a list of agents.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>List[BaseAgent]</code> <p>List of agent instances to be managed by the runner.</p> required Source code in <code>rai/agents/runner.py</code> <pre><code>def __init__(self, agents: List[BaseAgent]):\n    \"\"\"Initialize the AgentRunner with a list of agents.\n\n    Parameters\n    ----------\n    agents : List[BaseAgent]\n        List of agent instances to be managed by the runner.\n    \"\"\"\n    self.agents = agents\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.run","title":"<code>run()</code>","text":"<p>Run all agents in the background.</p> Notes <p>This method starts all agents by calling their <code>run</code> method. It is experimental; if agents do not run properly, consider running them in separate processes.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def run(self):\n    \"\"\"Run all agents in the background.\n\n    Notes\n    -----\n    This method starts all agents by calling their `run` method.\n    It is experimental; if agents do not run properly, consider running them in separate processes.\n    \"\"\"\n    self.logger.info(\n        f\"{self.__class__.__name__}.{self.run.__name__} is an experimental function. \\\n                        If you believe that your agents are not running properly, \\\n                        please run them separately (in different processes).\"\n    )\n    for agent in self.agents:\n        agent.run()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.run_and_wait_for_shutdown","title":"<code>run_and_wait_for_shutdown()</code>","text":"<p>Run all agents and block until a shutdown signal is received.</p> Notes <p>This method starts all agents and waits for a shutdown signal (SIGINT or SIGTERM), ensuring graceful termination.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def run_and_wait_for_shutdown(self):\n    \"\"\"Run all agents and block until a shutdown signal is received.\n\n    Notes\n    -----\n    This method starts all agents and waits for a shutdown signal (SIGINT or SIGTERM), ensuring graceful termination.\n    \"\"\"\n    self.run()\n    self.wait_for_shutdown()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.stop","title":"<code>stop()</code>","text":"<p>Stop all managed agents by calling their <code>stop</code> method.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def stop(self):\n    \"\"\"Stop all managed agents by calling their `stop` method.\"\"\"\n    for agent in self.agents:\n        agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.wait_for_shutdown","title":"<code>wait_for_shutdown()</code>","text":"<p>Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.</p> Notes <p>Installs signal handlers to capture SIGINT and SIGTERM. On receiving a signal, stops all managed agents.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def wait_for_shutdown(self):\n    \"\"\"Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.\n\n    Notes\n    -----\n    Installs signal handlers to capture SIGINT and SIGTERM. On receiving a signal, stops all managed agents.\n    \"\"\"\n    shutdown_event = Event()\n\n    def signal_handler(signum: int, frame: Optional[FrameType]):\n        shutdown_event.set()\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    try:\n        shutdown_event.wait()\n    finally:\n        for agent in self.agents:\n            agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#wait_for_shutdown-function-definition","title":"wait_for_shutdown function definition","text":""},{"location":"API_documentation/runners/overview/#rai.agents.runner.wait_for_shutdown","title":"<code>rai.agents.runner.wait_for_shutdown(agents)</code>","text":"<p>Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>List[BaseAgent]</code> <p>List of running agents to be stopped on shutdown.</p> required Notes <p>This method ensures a graceful shutdown of both the agent and the ROS2 node upon receiving an interrupt signal (SIGINT, e.g., Ctrl+C) or SIGTERM. It installs signal handlers to capture these events and invokes the agent's <code>stop()</code> method as part of the shutdown process.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def wait_for_shutdown(agents: List[BaseAgent]):\n    \"\"\"Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.\n\n    Parameters\n    ----------\n    agents : List[BaseAgent]\n        List of running agents to be stopped on shutdown.\n\n    Notes\n    -----\n    This method ensures a graceful shutdown of both the agent and the ROS2 node upon receiving\n    an interrupt signal (SIGINT, e.g., Ctrl+C) or SIGTERM. It installs signal handlers to\n    capture these events and invokes the agent's ``stop()`` method as part of the shutdown process.\n    \"\"\"\n    shutdown_event = Event()\n\n    def signal_handler(signum, frame):\n        shutdown_event.set()\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    try:\n        shutdown_event.wait()\n    finally:\n        for agent in agents:\n            agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#usage-examples","title":"Usage examples","text":""},{"location":"API_documentation/runners/overview/#agentrunner","title":"AgentRunner","text":"<pre><code>from rai.agents import AgentRunner\nfrom rai.agents.ros2 import ROS2StateBasedAgent\nfrom rai.agents import ReActAgent\n\nstate_based_agent = ROS2StateBasedAgent()\nreact_agent = ReActAgent()\n\nrunner = AgentRunner([state_based_agent, react_agent])\n\nrunner.run_and_wait_for_shutdown() # starts the agents and blocks until the shutdown signal (Ctrl+C or SIGTERM)\n</code></pre>"},{"location":"API_documentation/runners/overview/#wait_for_shutdown","title":"wait_for_shutdown","text":"<pre><code>from rai.agents import wait_for_shutdown\nfrom rai.agents.ros2 import ROS2StateBasedAgent\nfrom rai.agents import ReActAgent\n\nstate_based_agent = ROS2StateBasedAgent()\nreact_agent = ReActAgent()\n\n# start the agents manually\nstate_based_agent.run()\nreact_agent.run()\n\n# blocks until the shutdown signal (Ctrl+C or SIGTERM)\nwait_for_shutdown([state_based_agent, react_agent])\n</code></pre>"},{"location":"API_documentation/runners/overview/#see-also","title":"See Also","text":"<ul> <li>Agents: For more information on the different types of agents in RAI</li> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> </ul>"},{"location":"ROS_2/ros_packages/","title":"ROS Packages","text":"<p>RAI includes multiple configurable ROS 2 packages.</p> Package Description Documentation rai_perception Object detection tools based on open-set models and machine learning techniques. Integrates GroundingDINO and GroundedSAM with ROS 2. rai_perception rai_nomad Package integrating NoMaD -- an exploration model with ROS2. rai_nomad rai_interfaces Definition of custom messages and services used in RAI. rai_bringup Launch files to run RAI."},{"location":"demos/agriculture/","title":"Autonomous Tractor Demo","text":"<p>This demo showcases autonomous tractors operating in an agricultural field using ROS 2. The tractor is controlled using a conventional navigation stack. Sometimes, due to the ever-changing environment, the tractor may encounter unexpected situations. The conventional stack is not designed to handle these situations and usually ends up replanning the path. The tractor can handle this optimally by calling the RAI agent to decide what to do.</p>"},{"location":"demos/agriculture/#quick-start","title":"Quick Start","text":"<p>Remain in sourced shell</p> <p>Ensure that every command is run in a sourced shell using <code>source setup_shell.sh</code> Ensure ROS 2 is sourced.</p> <ol> <li> <p>Follow the RAI setup instructions in the quick setup guide.</p> </li> <li> <p>Download the Latest Release</p> <pre><code>./scripts/download_demo.sh agriculture\n</code></pre> </li> <li> <p>Run the Simulation</p> <pre><code>./demo_assets/agriculture/RAIAgricultureDemo/RAIAgricultureDemo.GameLauncher -bg_ConnectToAssetProcessor=0\n</code></pre> </li> <li> <p>Start the Tractor Node</p> <pre><code>python examples/agriculture-demo.py --tractor_number 1\n</code></pre> </li> </ol> <p>You are now ready to run the demo and see the tractor in action!</p>"},{"location":"demos/agriculture/#running-the-demo","title":"Running the Demo","text":"<p>The demo simulates a scenario where the tractor stops due to an unexpected situation. The RAI Agent decides the next action based on the current state.</p>"},{"location":"demos/agriculture/#rai-agent-decisions","title":"RAI Agent decisions","text":"<p>RAI Agent's mission is to decide the next action based on the current state of the anomaly. There are three exposed services to control the tractor:</p> <ul> <li>continue</li> </ul> <p>Used when the anomaly is flagged as a false positive.</p> <ul> <li>flash</li> </ul> <p>Used to flash the lights on the tractor to e.g. get the attention of the animals</p> <ul> <li>replan</li> </ul> <p>Used to replan the path/skip the alley.</p>"},{"location":"demos/agriculture/#what-happens-in-the-demo","title":"What Happens in the Demo?","text":"<ul> <li>The node listens for the tractor's state and calls the RaiNode using ROS 2 action when an anomaly     is detected.</li> <li>The RaiNode decides the next action based on the current state.</li> <li>The tractor performs the action and the demo continues.</li> </ul> <p>For more details on configuring RAI for specific robots, refer to the API documentation.</p> <p>Building from source</p> <p>If you are having trouble running the binary, you can build it from source here.</p>"},{"location":"demos/debugging_assistant/","title":"ROS 2 Debugging Assistant","text":"<p>The ROS 2 Debugging Assistant is an interactive tool that helps developers inspect and troubleshoot their ROS 2 systems using natural language. It provides a chat-like interface powered by Streamlit where you can ask questions about your ROS 2 setup and execute common debugging commands.</p>"},{"location":"demos/debugging_assistant/#features","title":"Features","text":"<ul> <li>Interactive chat interface for debugging ROS 2 systems</li> <li>Real-time streaming of responses and tool executions</li> <li>Support for common ROS 2 debugging commands:<ul> <li><code>ros2 topic</code>: topic inspection and manipulation</li> <li><code>ros2 service</code>: service inspection and calling</li> <li><code>ros2 node</code>: node information</li> <li><code>ros2 action</code>: action server details and goal sending</li> <li><code>ros2 interface</code>: interface inspection</li> <li><code>ros2 param</code>: checking and setting parameters</li> </ul> </li> </ul>"},{"location":"demos/debugging_assistant/#running-the-assistant","title":"Running the Assistant","text":"<ol> <li> <p>Follow the RAI setup instructions in the quick setup guide.</p> </li> <li> <p>Launch the debugging assistant:</p> </li> </ol> <pre><code>source setup_shell.sh\nstreamlit run examples/debugging_assistant.py\n</code></pre>"},{"location":"demos/debugging_assistant/#usage-examples","title":"Usage Examples","text":"<p>Here are some example queries you can try:</p> <ul> <li>\"What topics are currently available?\"</li> <li>\"Show me the message type for /cmd_vel\"</li> <li>\"List all active nodes\"</li> <li>\"What services does the /robot_state_publisher node provide?\"</li> <li>\"Show me information about the /navigate_to_pose action\"</li> </ul>"},{"location":"demos/debugging_assistant/#how-it-works","title":"How it Works","text":"<p>The debugging assistant uses RAI's conversational agent capabilities combined with ROS 2 debugging tools. The key components are:</p> <ol> <li>Streamlit Interface: Provides the chat UI and displays tool execution results</li> <li>ROS 2 Tools: Collection of debugging tools that wrap common ROS 2 CLI commands</li> <li>Streaming Callbacks: Real-time updates of LLM responses and tool executions</li> </ol>"},{"location":"demos/debugging_assistant/#limitations","title":"Limitations","text":"<ul> <li>The assistant can only execute safe, read-only commands by default</li> <li>Some complex debugging scenarios may require manual intervention</li> <li>Performance depends on the chosen LLM vendor and model</li> </ul>"},{"location":"demos/manipulation/","title":"Manipulation tasks with natural language","text":"<p>This demo showcases the capabilities of RAI in performing manipulation tasks using natural language commands. The demo utilizes a robot arm (Franka Emika Panda) in a simulated environment, demonstrating how RAI can interpret complex instructions and execute them using advanced vision and manipulation techniques.</p> <p></p>"},{"location":"demos/manipulation/#setup","title":"Setup","text":"<p>LLM model</p> <p>The demo uses the <code>complex_model</code> LLM configured in <code>config.toml</code>. The model should be a multimodal, tool-calling model. See Vendors.</p> <p>ROS 2 Sourced</p> <p>Make sure ROS 2 is sourced. (e.g. <code>source /opt/ros/humble/setup.bash</code>)</p>"},{"location":"demos/manipulation/#local-setup","title":"Local Setup","text":""},{"location":"demos/manipulation/#setting-up-the-demo","title":"Setting up the demo","text":"<ol> <li>Follow the RAI setup instructions in the quick setup guide.</li> <li> <p>Download additional dependencies:</p> <pre><code>poetry install --with perception,simbench\nvcs import &lt; demos.repos\nrosdep install --from-paths src/examples/rai-manipulation-demo/ros2_ws/src --ignore-src -r -y\n</code></pre> </li> <li> <p>Download the latest binary release</p> <pre><code>./scripts/download_demo.sh manipulation\n</code></pre> </li> <li> <p>Build the ROS 2 workspace:</p> <pre><code>colcon build --symlink-install\n</code></pre> </li> </ol>"},{"location":"demos/manipulation/#running-the-demo","title":"Running the demo","text":"<p>Remain in sourced shell</p> <p>Ensure that every command is run in a sourced shell using <code>source setup_shell.sh</code> Ensure ROS 2 is sourced.</p> <ol> <li> <p>Run the Demo:</p> <pre><code>streamlit run examples/manipulation-demo-streamlit.py\n</code></pre> <p>Alternatively, you can run the simpler command-line version, which also serves as an example of how to use the RAI API for your own applications:</p> <ol> <li>Run Simulation</li> </ol> <pre><code>ros2 launch examples/manipulation-demo.launch.py game_launcher:=demo_assets/manipulation/RAIManipulationDemo/RAIManipulationDemo.GameLauncher\n</code></pre> <p>By default, perception services register both new model-agnostic service names (<code>/detection</code>, <code>/segmentation</code>) and legacy names (<code>/grounding_dino_classify</code>, <code>/grounded_sam_segment</code>) for backward compatibility. To disable legacy names, add the launch argument:</p> <pre><code>ros2 launch examples/manipulation-demo.launch.py game_launcher:=... enable_legacy_service_names:=false\n</code></pre> <ol> <li>Run cmd app</li> </ol> <pre><code>python examples/manipulation-demo.py\n</code></pre> <p>Note: <code>manipulation-demo.py</code> uses the new service names (<code>/detection</code>, <code>/segmentation</code>). Legacy examples like <code>manipulation-demo-v1.py</code> use legacy service names and require <code>enable_legacy_service_names:=true</code> (default).</p> </li> <li> <p>Interact with the robot arm using natural language commands. For example:</p> <pre><code>\"Place each apple on top of a cube\"\n\"Build a tower from cubes\"\n\"Arrange objects in a line\"\n\"Put two boxes closer to each other. Move only one box.\"\n\"Move cubes to the left side of the table\"\n</code></pre> </li> </ol> <p>Changing camera view</p> <p>To change camera in the simulation use 1-7 keys on your keyboard once it's window is focused.</p>"},{"location":"demos/manipulation/#docker-setup","title":"Docker Setup","text":""},{"location":"demos/manipulation/#1-setting-up-the-demo","title":"1. Setting up the demo","text":"<ol> <li> <p>Set up docker as outlined in the docker setup guide. During the setup, build the docker image with all dependencies (i.e., use the <code>--build-arg DEPENDENCIES=all_groups</code> argument)</p> </li> <li> <p>Enable X11 access for the docker container:</p> <pre><code>xhost +local:root\n</code></pre> </li> <li> <p>Run the docker container with the following command:</p> <pre><code>docker run --net=host --ipc=host --pid=host -e ROS_DOMAIN_ID=$ROS_DOMAIN_ID -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix --gpus all -it rai:jazzy # or rai:humble\n</code></pre> <p>NVIDIA Container Toolkit</p> <p>In order to use the <code>--gpus all</code> flag, the NVIDIA Container Toolkit must be installed on the host machine.</p> </li> <li> <p>(Inside the docker container) By default, RAI uses OpenAI as the vendor. Thus, it is necessary     to set the <code>$OPENAI_API_KEY</code> environmental variable. The command below may be utilized to set     the variable and add it to the container's <code>.bashrc</code> file:</p> <pre><code>export OPENAI_API_KEY=YOUR_OPEN_AI_API_KEY\necho \"export OPENAI_API_KEY=$OPENAI_API_KEY\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Note</p> <p>The default vendor can be changed to a different provider via the RAI configuration tool</p> </li> <li> <p>After this, follow the steps in the Local Setup from step 2 onwards.</p> <p>New terminal in docker</p> <p>In order to open a new terminal in the same docker container, you can use the following command:</p> <pre><code>docker exec -it &lt;container_id&gt; bash\n</code></pre> </li> </ol>"},{"location":"demos/manipulation/#how-it-works","title":"How it works","text":"<p>The manipulation demo utilizes several components:</p> <ol> <li>Vision processing using Grounded SAM 2 and Grounding DINO for object detection and segmentation via ROS2 services (<code>/detection</code>, <code>/segmentation</code>).</li> <li>RAI agent to process the request and plan the manipulation sequence.</li> <li>Robot arm control for executing the planned movements.</li> </ol> <p>The main logic of the demo is implemented in the <code>create_agent</code> function, which can be found in:</p> <pre><code>examples/manipulation-demo.py\n</code></pre>"},{"location":"demos/manipulation/#service-names-and-backward-compatibility","title":"Service Names and Backward Compatibility","text":"<p>Perception services support both new model-agnostic service names and legacy names for backward compatibility:</p> <ul> <li>New service names (default): <code>/detection</code>, <code>/segmentation</code></li> <li>Legacy service names: <code>/grounding_dino_classify</code>, <code>/grounded_sam_segment</code></li> </ul> <p>By default, both sets of names are registered. To disable legacy names (for new applications only), launch with:</p> <pre><code>ros2 launch examples/manipulation-demo.launch.py game_launcher:=... enable_legacy_service_names:=false\n</code></pre> <p>Existing applications using legacy service names will continue to work with the default configuration.</p>"},{"location":"demos/manipulation/#known-limitations","title":"Known Limitations","text":"<ul> <li><code>Grounding DINO</code> can't distinguish colors.</li> <li>VLMs tend to struggle with spatial understanding (for example left/right concepts).</li> </ul> <p>Building from source</p> <p>If you are having trouble running the binary, you can build it from source here.</p>"},{"location":"demos/overview/","title":"RAI Demos Overview","text":"<p>This document provides an overview of the available RAI demos. Each demo showcases different capabilities of the RAI platform in various robotics applications.</p> Demo Description Key Features Environment Manipulation Robot arm manipulation using natural language - Object detection and segmentation- Natural language command processing- Robotic manipulation planning ROS 2, Franka Emika Panda ROSbot XL Mobile robot operating in an apartment environment - Natural language interaction- Navigation and perception- Streamlit GUI interface ROS 2, Open 3D Engine Autonomous Tractor Autonomous agricultural tractor operating in a field - Path planning and navigation- Anomaly detection and handling- RAI agent decision making ROS 2, Agricultural field simulation ROS2 Debugging Assistant Interactive chat interface for debugging ROS 2 systems - Interactive chat interface for debugging ROS 2 systems ROS 2"},{"location":"demos/overview/#common-requirements","title":"Common Requirements","text":"<p>All demos require:</p> <ul> <li>ROS 2 (Humble or Jazzy)</li> <li>RAI platform setup</li> </ul> <p>Make sure to follow the installation instructions.</p> <p>For detailed setup and running instructions, please refer to the individual demo documentation linked above.</p>"},{"location":"demos/rosbot_xl/","title":"Husarion Robot XL demo","text":"<p>This demo utilizes Open 3D Engine simulation and allows you to work with RAI on a small mobile platform in a nice apartment.</p> <p></p>"},{"location":"demos/rosbot_xl/#quick-start","title":"Quick start","text":"<p>LLM model</p> <p>The demo uses the <code>complex_model</code> LLM configured in <code>config.toml</code>. The model should be a multimodal, tool-calling model. See Vendors.</p> <p>ROS 2 Sourced</p> <p>Make sure ROS 2 is sourced. (e.g. <code>source /opt/ros/humble/setup.bash</code>)</p> <ol> <li> <p>Follow the RAI setup instructions in the quick setup guide.</p> </li> <li> <p>Download the newest binary release:</p> <pre><code>./scripts/download_demo.sh rosbot\n</code></pre> </li> <li> <p>Install and download required packages</p> <pre><code>sudo apt install ros-${ROS_DISTRO}-navigation2 ros-${ROS_DISTRO}-nav2-bringup\nvcs import &lt; demos.repos\nrosdep install --from-paths src --ignore-src -r -y\npoetry install --with perception\ncolcon build\n</code></pre> </li> </ol> <p>Alternative: Demo source build</p> <p>If you would like more freedom to adapt the simulation to your needs, you can make changes using O3DE Editor and build the project yourself. Please refer to rai husarion rosbot xl demo for more details.</p>"},{"location":"demos/rosbot_xl/#running-rai","title":"Running RAI","text":"<ol> <li> <p>Running rai nodes and agents, navigation stack and O3DE simulation.</p> <pre><code>source setup_shell.sh\nros2 launch ./examples/rosbot-xl.launch.py game_launcher:=demo_assets/rosbot/RAIROSBotXLDemo/RAIROSBotXLDemo.GameLauncher\n</code></pre> </li> <li> <p>Run streamlit gui:</p> </li> </ol> <p>Choosing the right model</p> <p>Due to the multimodal requirements of the demo (such as processing camera images), the best results will be achieved with tool-calling LLMs that support vision, such as those from OpenAI, Anthropic, and similar providers. You can use local models (e.g., <code>gpt-oss-20b</code>, <code>qwen3</code>) configured in <code>config.toml</code>, but with these, image-based tasks (like interpreting the robot's camera feed) will not work\u2014the agent will have a reduced scope of capabilities.</p> <pre><code>source setup_shell.sh\nstreamlit run examples/rosbot-xl-demo.py\n</code></pre> <ol> <li> <p>Play with the demo, prompting the agent to perform tasks. Here are some examples:</p> <ul> <li>Where are you now?</li> <li>What do you see?</li> <li>What is the position of bed?</li> <li>Navigate to the kitchen.</li> <li>Please bring me coffee something from the kitchen (this one should be rejected thanks to robot embodiment module)</li> </ul> </li> </ol> <p>Changing camera view</p> <p>To change camera in the simulation use 1,2,3 keys on your keyboard once it's window is focused.</p>"},{"location":"demos/rosbot_xl/#how-it-works","title":"How it works","text":"<p>The rosbot demo utilizes several components:</p> <ol> <li>Vision processing using Grounded SAM 2 and Grounding DINO for object detection and segmentation. See RAI perception.</li> <li>RAI agent to process the request and interact with environment via tool-calling mechanism.</li> <li>Navigation is enabled via nav2 toolkit, which interacts with ROS 2 nav2 asynchronously by calling ros2 actions.</li> <li>Embodiment of the Rosbot is achieved using RAI Whoami module. This makes RAI agent aware of the hardware platform and its capabilities.</li> <li>Key informations can be provided to the agent in RAI Whoami. In this demo coordinates of <code>Kitchen</code> and <code>Living Room</code> are provided as <code>capabilities</code>.</li> </ol> <p>The details of the demo can be checked in <code>examples/rosbot-xl-demo.py</code>.</p>"},{"location":"extensions/nomad/","title":"RAI NoMaD","text":"<p>This package provides a ROS2 Node which loads and runs the NoMaD model, that can be dynamically activated and deactivated using ROS2 messages.</p>"},{"location":"extensions/nomad/#running-instructions","title":"Running instructions","text":""},{"location":"extensions/nomad/#1-setup-the-ros2-workspace","title":"1. Setup the ROS2 workspace","text":"<p>In the base directory of the <code>RAI</code> package install dependencies:</p> <pre><code>poetry install --with nomad\n</code></pre> <p>Source the ros installation</p> <pre><code>source /opt/ros/${ROS_DISTRO}/setup.bash\n</code></pre> <p>Run the build process:</p> <pre><code>colcon build\n</code></pre> <p>Setup your shell:</p> <pre><code>source ./setup_shell.sh\n</code></pre>"},{"location":"extensions/nomad/#2-run-the-node","title":"2. Run the node","text":"<p>Run the ROS2 node using <code>ros2 run</code>:</p> <pre><code>ros2 run rai_nomad nomad --ros-args -p image_topic:=&lt;your_image_topic&gt;\n</code></pre> <p>The model will be loaded and ready, but it will not run until you call the <code>/rai_nomad/start</code> service. Then it will start outputting velocity commands and your robot should start moving. You can then stop the model by calling the <code>/rai_nomad/stop</code> service.</p>"},{"location":"extensions/nomad/#parameters","title":"Parameters","text":"<ul> <li><code>model_path</code>: Path to where the model should be downloaded. By default it will be downloaded into the package's <code>share</code> directory.</li> <li><code>image_topic</code>: The topic where the camera images are being published.</li> <li><code>cmd_vel_topic</code>: The topic where the velocity commands are being published. Default: <code>/cmd_vel</code>.</li> <li><code>linear_vel</code>: Linear velocity scaling of the model output.</li> <li><code>angular_vel</code>: Angular velocity scaling of the model output.</li> <li><code>max_v</code> and <code>max_w</code>: Maximum linear and angular velocities that the model can output.</li> <li><code>rate</code>: The rate at which the model will run.</li> </ul>"},{"location":"extensions/perception/","title":"RAI Perception","text":"<p>RAI Perception brings powerful computer vision capabilities to your ROS2 applications. It integrates GroundingDINO and Grounded-SAM-2 to detect objects, create segmentation masks, and calculate gripping points.</p> <p>The package includes model-agnostic ROS2 service nodes (<code>DetectionService</code> and <code>SegmentationService</code>) that provide detection and segmentation capabilities. Legacy agents (<code>GroundedSamAgent</code> and <code>GroundingDinoAgent</code>) are deprecated in favor of these services. It also provides tools that work seamlessly with RAI LLM agents to build conversational robot scenarios, including <code>GetObjectPositionsTool</code> for general spatial reasoning and <code>GetObjectGrippingPointsTool</code> for advanced gripping strategies.</p>"},{"location":"extensions/perception/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>rai-perception</code>, ensure you have:</p> <ol> <li>ROS2 installed (Jazzy recommended, or Humble). If you don't have ROS2 yet, follow the official ROS2 installation guide for jazzy or humble.</li> <li>Python 3.8+ and <code>pip</code> installed (usually pre-installed on Ubuntu).</li> <li>NVIDIA GPU with CUDA support (required for optimal performance).</li> <li>wget installed (required for downloading model weights):     <pre><code>sudo apt install wget\n</code></pre></li> </ol>"},{"location":"extensions/perception/#installation","title":"Installation","text":"<p>Step 1: Source ROS2 in your terminal:</p> <pre><code># For ROS2 Jazzy (recommended)\nsource /opt/ros/jazzy/setup.bash\n\n# For ROS2 Humble\nsource /opt/ros/humble/setup.bash\n</code></pre> <p>Step 2: Install ROS2 dependencies. <code>rai-perception</code> requires its ROS2 packages that needs to be installed separately:</p> <pre><code># Update package lists first\nsudo apt update\n\n# Install rai_interfaces as a debian package\nsudo apt install ros-jazzy-rai-interfaces  # or ros-humble-rai-interfaces for Humble\n</code></pre> <p>Step 3: Install <code>rai-perception</code> via pip:</p> <pre><code>pip install rai-perception\n</code></pre> <p>[!TIP] It's recommended to install <code>rai-perception</code> in a virtual environment to avoid conflicts with other Python packages.</p> <p>[!TIP] To avoid sourcing ROS2 in every new terminal, add the source command to your <code>~/.bashrc</code> file:</p> <pre><code>echo \"source /opt/ros/jazzy/setup.bash\" &gt;&gt; ~/.bashrc  # or humble\n</code></pre>"},{"location":"extensions/perception/#getting-started","title":"Getting Started","text":"<p>This section provides a step-by-step guide to get you up and running with RAI Perception.</p>"},{"location":"extensions/perception/#quick-start","title":"Quick Start","text":"<p>After installing <code>rai-perception</code>, launch the perception services:</p> <p>Step 1: Open a terminal and source ROS2:</p> <pre><code>source /opt/ros/jazzy/setup.bash  # or humble\n</code></pre> <p>Step 2: Launch the perception services:</p> <pre><code>python -m rai_perception.scripts.run_perception_services\n</code></pre> <p>[!NOTE] The weights will be downloaded to <code>~/.cache/rai</code> directory on first use.</p> <p>[!NOTE] &gt; Legacy Service Names: The services register both new service names (<code>/detection</code>, <code>/segmentation</code>) and legacy service names (<code>/grounding_dino_classify</code>, <code>/grounded_sam_segment</code>) for backward compatibility. Legacy service names will be removed in a future release. Migrate your code to use the new service names.</p> <p>The services create two ROS 2 nodes: <code>detection_service</code> and <code>segmentation_service</code> using ROS2Connector.</p> <p>[!WARNING] &gt; Deprecated: The legacy script <code>run_perception_agents</code> and agents (<code>GroundedSamAgent</code>, <code>GroundingDinoAgent</code>) are deprecated. Use <code>run_perception_services</code> instead, which launches <code>DetectionService</code> and <code>SegmentationService</code>.</p>"},{"location":"extensions/perception/#testing-with-example-client","title":"Testing with Example Client","text":"<p>The <code>rai_perception/talker.py</code> example demonstrates how to use the perception services for object detection and segmentation. It shows the complete pipeline: GroundingDINO for object detection followed by GroundedSAM for instance segmentation, with visualization output.</p> <p>Step 1: Open a terminal and source ROS2:</p> <pre><code>source /opt/ros/jazzy/setup.bash  # or humble\n</code></pre> <p>Step 2: Launch the perception services:</p> <pre><code>python -m rai_perception.scripts.run_perception_services\n</code></pre> <p>Step 3: In a different terminal (remember to source ROS2 first), run the example client:</p> <pre><code>source /opt/ros/jazzy/setup.bash  # or humble\npython -m rai_perception.examples.talker --ros-args -p image_path:=\"&lt;path-to-image&gt;\"\n</code></pre> <p>You can use any image containing objects like dragons, lizards, or dinosaurs. For example, use the <code>sample.jpg</code> from the package's <code>images</code> folder. The client will detect these objects and save a visualization with bounding boxes and masks to <code>masks.png</code> in the current directory.</p> <p>[!TIP]</p> <p>If you wish to integrate open-set vision into your ros2 launch file, a premade launch file can be found in <code>rai/src/rai_bringup/launch/openset.launch.py</code></p>"},{"location":"extensions/perception/#ros2-service-interface","title":"ROS2 Service Interface","text":"<p>The services can be triggered by ROS2 services:</p> <p>New service names (recommended):</p> <ul> <li><code>/detection</code>: <code>rai_interfaces/srv/RAIGroundingDino</code></li> <li><code>/segmentation</code>: <code>rai_interfaces/srv/RAIGroundedSam</code></li> </ul> <p>Legacy service names (deprecated, will be removed):</p> <ul> <li><code>/grounding_dino_classify</code>: <code>rai_interfaces/srv/RAIGroundingDino</code></li> <li><code>/grounded_sam_segment</code>: <code>rai_interfaces/srv/RAIGroundedSam</code></li> </ul>"},{"location":"extensions/perception/#dive-deeper-tools-and-integration","title":"Dive Deeper: Tools and Integration","text":"<p>This section provides information for developers looking to integrate RAI Perception tools into their applications.</p>"},{"location":"extensions/perception/#rai-tools","title":"RAI Tools","text":"<p><code>rai_perception</code> package contains tools that can be used by RAI LLM agents to enhance their perception capabilities. For more information on RAI Tools see Tool use and development tutorial.</p> <p>The tools fall into two categories:</p> <ul> <li>Detection tools: <code>GetDetectionTool</code> and <code>GetDistanceToObjectsTool</code> for object detection and distance estimation</li> <li>Position and gripping tools: <code>GetObjectPositionsTool</code> for general spatial queries and <code>GetObjectGrippingPointsTool</code> for advanced gripping strategies</li> </ul>"},{"location":"extensions/perception/#getdetectiontool","title":"<code>GetDetectionTool</code>","text":"<p>This tool calls the GroundingDINO service to detect objects from a comma-separated prompt in the provided camera topic.</p> <p>[!TIP]</p> <p>you can try example below with rosbotxl demo binary. The binary exposes <code>/camera/camera/color/image_raw</code> and <code>/camera/camera/depth/image_rect_raw</code> topics.</p> <p>Example call</p> <pre><code>import time\nfrom rai_perception.tools import GetDetectionTool\nfrom rai.communication.ros2 import ROS2Connector, ROS2Context\n\nwith ROS2Context():\n    connector=ROS2Connector(node_name=\"test_node\")\n\n    # Wait for topic discovery to complete\n    print(\"Waiting for topic discovery...\")\n    time.sleep(3)\n\n    x = GetDetectionTool(connector=connector)._run(\n        camera_topic=\"/camera/camera/color/image_raw\",\n        object_names=[\"bed\", \"bed pillow\", \"table lamp\", \"plant\", \"desk\"],\n    )\n    print(x)\n</code></pre> <p>Example output</p> <pre><code>I have detected the following items in the picture plant, table lamp, table lamp, bed, desk\n</code></pre>"},{"location":"extensions/perception/#getdistancetoobjectstool","title":"<code>GetDistanceToObjectsTool</code>","text":"<p>This tool calls the GroundingDINO service to detect objects from a comma-separated prompt in the provided camera topic. Then it utilizes messages from the depth camera to estimate the distance to detected objects.</p> <p>Example call</p> <pre><code>from rai_perception.tools import GetDistanceToObjectsTool\nfrom rai.communication.ros2 import ROS2Connector, ROS2Context\nimport time\n\nwith ROS2Context():\n    connector=ROS2Connector(node_name=\"test_node\")\n    connector.node.declare_parameter(\"conversion_ratio\", 1.0)  # scale parameter for the depth map\n\n    # Wait for topic discovery to complete\n    print(\"Waiting for topic discovery...\")\n    time.sleep(3)\n\n    x = GetDistanceToObjectsTool(connector=connector)._run(\n        camera_topic=\"/camera/camera/color/image_raw\",\n        depth_topic=\"/camera/camera/depth/image_rect_raw\",\n        object_names=[\"desk\"],\n    )\n\n    print(x)\n</code></pre> <p>Example output</p> <pre><code>I have detected the following items in the picture desk: 2.43m away\n</code></pre>"},{"location":"extensions/perception/#getobjectpositionstool","title":"<code>GetObjectPositionsTool</code>","text":"<p>This tool retrieves the 3D positions (centroids) of all detected objects of a specified type in the target frame. It uses the <code>default_grasp</code> preset with centroid strategy, making it ideal for general spatial reasoning tasks like arranging objects or placing objects relative to each other.</p> <p>The tool name <code>get_object_positions</code> provides better spatial interpretation for agents, making it more likely to be used for spatial reasoning tasks compared to <code>get_object_gripping_points</code>.</p> <p>[!NOTE] For general position queries, use <code>GetObjectPositionsTool</code>. For advanced gripping strategies (top-down grasping, precise grasping), use <code>GetObjectGrippingPointsTool</code> instead.</p> <p>Example call</p> <pre><code>import rclpy\nfrom rai_perception.tools import GetObjectPositionsTool\nfrom rai.communication.ros2 import ROS2Connector\nfrom rai_perception.tools.gripping_points_tools import GRIPPING_POINTS_TOOL_PARAM_PREFIX\n\nrclpy.init()\nconnector = ROS2Connector(executor_type=\"single_threaded\")\nnode = connector.node\n\n# Set ROS2 parameters to match your robot/simulation setup\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.camera_topic\", \"/color_image5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.depth_topic\", \"/depth_image5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.camera_info_topic\", \"/color_camera_info5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.target_frame\", \"panda_link0\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.source_frame\", \"RGBDCamera5\"\n)\n\ntool = GetObjectPositionsTool(connector=connector)\nresult = tool._run(object_name=\"apple\")\nprint(result)\n</code></pre> <p>Example output</p> <pre><code>Centroids of detected apples in panda_link0 frame: [Centroid(x=0.51, y=0.391241, z=0.038238), Centroid(x=0.36, y=0.392357, z=0.037558)]. Sizes of the detected objects are unknown.\n</code></pre>"},{"location":"extensions/perception/#getobjectgrippingpointstool","title":"<code>GetObjectGrippingPointsTool</code>","text":"<p>This tool provides advanced gripping point strategies optimized for specific grasping scenarios. It supports multiple presets:</p> <ul> <li><code>default_grasp</code>: Centroid-based estimation (used internally by <code>GetObjectPositionsTool</code>)</li> <li><code>top_grasp</code>: Optimized for top-down grasping from above</li> <li><code>precise_grasp</code>: High-quality preset with aggressive outlier filtering and precise top-plane estimation</li> </ul> <p>Use this tool when you need advanced gripping strategies beyond simple position queries. For general spatial reasoning tasks, <code>GetObjectPositionsTool</code> is recommended.</p> <p>Example call</p> <pre><code>import rclpy\nfrom rai_perception import GetObjectGrippingPointsTool\nfrom rai.communication.ros2 import ROS2Connector\nfrom rai_perception.components.perception_presets import apply_preset\nfrom rai_perception.tools.gripping_points_tools import GRIPPING_POINTS_TOOL_PARAM_PREFIX\n\nrclpy.init()\nconnector = ROS2Connector(executor_type=\"single_threaded\")\nnode = connector.node\n\n# Set ROS2 parameters\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.camera_topic\", \"/color_image5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.depth_topic\", \"/depth_image5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.camera_info_topic\", \"/color_camera_info5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.target_frame\", \"panda_link0\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.source_frame\", \"RGBDCamera5\"\n)\n\n# Apply top_grasp preset for top-down grasping\nfilter_config, estimator_config = apply_preset(\"top_grasp\")\ntool = GetObjectGrippingPointsTool(\n    connector=connector,\n    filter_config=filter_config,\n    estimator_config=estimator_config,\n)\nresult = tool._run(object_name=\"cube\")\nprint(result)\n</code></pre>"},{"location":"extensions/perception/#debug-mode","title":"Debug Mode","text":"<p>Both <code>GetObjectPositionsTool</code> and <code>GetObjectGrippingPointsTool</code> support an optional <code>debug</code> parameter that enables progressive evaluation and debugging. When <code>debug=True</code>, the tool publishes intermediate pipeline results to ROS2 topics for visualization in RViz2 and logs detailed stage information including point counts and timing. This allows you to inspect individual pipeline stages (point cloud extraction, filtering, estimation) without running the full pipeline.</p> <p>Debug Topics Published:</p> <ul> <li><code>/debug/gripping_points/raw_point_clouds</code> (PointCloud2) - Raw point clouds from segmentation stage</li> <li><code>/debug/gripping_points/filtered_point_clouds</code> (PointCloud2) - Filtered point clouds after outlier removal</li> <li><code>/debug_gripping_points_pointcloud</code> (PointCloud2) - Final object point cloud</li> <li><code>/debug_gripping_points_markerarray</code> (MarkerArray) - Gripping points as red sphere markers</li> </ul> <p>[!WARNING] Debug mode adds computational overhead and network traffic. It is not suitable for production use. Debug topics publish for 5 seconds after each detection.</p>"},{"location":"extensions/perception/#visualizing-gripping-points-in-rviz2-with-manipulation-demo","title":"Visualizing Gripping Points in RViz2 with Manipulation Demo","text":"<p>[!NOTE] The manipulation demo must be started before running the debug visualization. Launch the demo using:</p> <pre><code>ros2 launch examples/manipulation-demo.launch.py game_launcher:=demo_assets/manipulation/RAIManipulationDemo/RAIManipulationDemo.GameLauncher\n</code></pre> <p>To debug gripping point locations using the manipulation demo:</p> <p>Step 1: Enable debug mode by modifying <code>examples/manipulation-demo.py</code> to pass <code>debug=True</code> when the tool is called, or call the tool directly:</p> <pre><code>import rclpy\nfrom rai_perception import GetObjectGrippingPointsTool\nfrom rai.communication.ros2 import ROS2Connector\nfrom rai_perception.tools.gripping_points_tools import GRIPPING_POINTS_TOOL_PARAM_PREFIX\n\nrclpy.init()\nconnector = ROS2Connector(executor_type=\"single_threaded\")\nnode = connector.node\n\n# Set ROS2 parameters to match your robot/simulation setup\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.camera_topic\", \"/color_image5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.depth_topic\", \"/depth_image5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.camera_info_topic\", \"/color_camera_info5\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.target_frame\", \"panda_link0\"\n)\nnode.declare_parameter(\n    f\"{GRIPPING_POINTS_TOOL_PARAM_PREFIX}.source_frame\", \"RGBDCamera5\"\n)\n\ntool = GetObjectGrippingPointsTool(connector=connector)\nresult = tool._run(object_name=\"box\", debug=True)\n</code></pre> <p>Alternatively, you can use the integration test which performs similar setup and includes additional visualization features:</p> <pre><code># Run the integration test with debug mode enabled\npytest tests/rai_perception/components/test_gripping_points_integration.py::test_gripping_points_manipulation_demo -m \"manual\" -s -v --grasp default_grasp\n</code></pre> <p>The test requires the manipulation demo to be running and will:</p> <ul> <li>Detect gripping points for the specified object</li> <li>Publish debug data to ROS2 topics for RViz2 visualization</li> <li>Save an annotated image showing gripping points projected onto the camera view</li> </ul> <p>Step 2: Launch RViz2 in a separate terminal:</p> <pre><code>rviz2\n</code></pre> <p>Step 3: In RViz2, configure the visualization:</p> <ol> <li>Set Fixed Frame to your target frame (default: <code>panda_link0</code> for manipulation demo)</li> <li>Add MarkerArray display (essential):<ul> <li><code>/debug_gripping_points_markerarray</code> (gripping points as red spheres, 0.04m radius)</li> </ul> </li> <li>Optionally add PointCloud2 displays for pipeline debugging:<ul> <li><code>/debug/gripping_points/raw_point_clouds</code> (raw point clouds from stage 1)</li> <li><code>/debug/gripping_points/filtered_point_clouds</code> (filtered point clouds from stage 2)</li> <li><code>/debug_gripping_points_pointcloud</code> (final object points)</li> </ul> </li> </ol> <p>Step 4: Trigger gripping point detection through the manipulation demo or tool call. The debug topics will publish for 5 seconds, showing the pipeline stages and final gripping point locations.</p>"},{"location":"extensions/rethinking_usability/","title":"Rethinking RAI Perception","text":"<p>Last updated: Jan 2026</p> <p>How not to make the API so hard to use</p> <p>Perception is a critical component in robotics applications, enabling object detection, segmentation, 3D gripping point estimation, and point cloud processing for ROS2-based systems. Within the RAI Framework, <code>rai_perception</code> serves as an extension providing vision capabilities through tools (<code>GetDetectionTool</code>, <code>GetObjectGrippingPointsTool</code>), ROS2 service nodes (<code>DetectionService</code>, <code>SegmentationService</code>), and utilities that integrate with other RAI components (e.g., <code>rai_semap</code>, <code>rai_bench</code>).</p> <p>As the codebase grows with recent work on 3D gripping point detection PR and new perception functionality in <code>rai_semap</code>, the current API design reveals significant usability challenges because most new APIs are designed for expert-level users, requiring deep understanding of algorithms, pipeline architecture, and domain-specific concepts. The configuration supporting these APIs is also complex. The existing APIs lack support for use cases where developers need to switch to different detection models.</p> <p>This document explores a potential redesign approach that addresses some of these challenges by organizing code into a tiered API structure that supports progressive disclosure, in addition to other exploration drawing on design principles from <code>api_design_considerations.md</code>. Mainly,</p> <ul> <li>Rather than focusing solely on API surface changes, the exploration considers cognitive load and provides clear paths from simple, agent-friendly tools to configurable components to expert-level algorithms, balancing accessibility with flexibility.</li> <li>Another goal is to lay out the foundation for enabling developers to switch between different detection models with no or little code changes (see the \"Switching Between Detection Models\" use case below).</li> </ul>"},{"location":"extensions/rethinking_usability/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Current State Analysis<ul> <li>Audiences and Roles</li> <li>Usability Concerns</li> </ul> </li> <li>Proposed Design<ul> <li>Tiered API Structure</li> <li>Folder Structure</li> <li>Usability Improvements Along Cognitive Dimensions<ul> <li>Progressive Disclosure</li> <li>Configuration Management</li> <li>Progressive Evaluation</li> <li>Penetrability</li> <li>Consistency</li> <li>Domain Correspondence</li> <li>Role Expressiveness</li> <li>Why Agents are Renamed as Services</li> </ul> </li> <li>Use Cases and Impact Evaluation<ul> <li>Existing Use Cases</li> <li>New Use Case: Switching Between Detection Models</li> <li>Adding New Models to the Registry</li> </ul> </li> </ul> </li> <li>Good to Have (deferred)</li> <li>Future Work<ul> <li>Data Collection for Fine-Tuning</li> <li>Observability</li> </ul> </li> </ul>"},{"location":"extensions/rethinking_usability/#current-state-analysis-as-of-jan-2026","title":"Current State Analysis (As of Jan 2026)","text":"<p>The analysis is based on the existing code from <code>rai_perception</code>, 3D gripping point detection PR and new perception functionality(1 and 2) in <code>rai_semap</code>.</p>"},{"location":"extensions/rethinking_usability/#audiences-and-roles","title":"Audiences and Roles","text":"<p><code>rai_perception</code> serves four roles within the RAI Framework:</p> <ul> <li>Application Developers (High-level): Use tools (<code>GetDetectionTool</code>, <code>GetObjectGrippingPointsTool</code>) in agents to build applications with minimal configuration</li> <li>LLM agents (Runtime consumers): Consume perception tools at runtime via tool-calling mechanisms (tools implement LangChain <code>BaseTool</code> with <code>name</code>, <code>description</code>, <code>args_schema</code> for LLM understanding)</li> <li>Extension Developers (Mid-level): Extend tools or create custom perception pipelines by working within existing framework layers</li> <li>Core Developers (Low-level): Implement new vision services (extending <code>BaseVisionAgent</code>, which should be renamed to avoid confusion with RAI's agent concept) or low-level algorithms from scratch</li> </ul>"},{"location":"extensions/rethinking_usability/#usability-concerns","title":"Usability Concerns","text":"<p>The current multi-tier abstraction (high-level tools, mid-level components, low-level algorithms) reveals significant usability challenges:</p> <ol> <li>Abstraction gap: Large jump from high-level tool usage (simple <code>object_name</code> parameter) to mid-level configuration (10+ algorithm parameters with domain-specific knowledge required)</li> <li>Configuration complexity: <code>GetObjectGrippingPointsTool</code> requires 6+ ROS2 parameters (<code>target_frame</code>, <code>source_frame</code>, <code>camera_topic</code>, <code>depth_topic</code>, <code>camera_info_topic</code>, <code>timeout_sec</code>) plus 3 configuration objects with 10+ parameters each</li> <li>Algorithm knowledge requirement: Mid-level users must understand ML/computer vision concepts (DBSCAN, RANSAC, Isolation Forest, percentiles) to configure effectively</li> <li>Hidden dependencies: Tool initialization depends on ROS2 parameters that must be set before tool creation, no clear error if missing</li> <li>Pipeline complexity: <code>GetObjectGrippingPointsTool</code> orchestrates a 3-stage pipeline (point cloud extraction \u2192 filtering \u2192 estimation) with no visibility into intermediate stages</li> <li>Progressive evaluation difficulty: Cannot test individual pipeline stages, must run full pipeline to see results. Debug mode (<code>debug=True</code>) partially addresses this by publishing intermediate results to ROS2 topics for visualization in RVIZ.</li> <li>Current error handling is inconsistent: tools catch <code>RaiTimeoutError</code> and return user-friendly messages, but generic <code>Exception</code> is re-raised without context. Parameter validation patterns vary across tools, and algorithm failures propagate as generic exceptions without actionable guidance.</li> </ol>"},{"location":"extensions/rethinking_usability/#proposed-design","title":"Proposed Design","text":"<p>The key part of the proposed design addresses the usability concerns identified above through a tiered API structure that supports progressive disclosure. This section outlines the architecture and explores how it improves usability along key cognitive dimensions.</p>"},{"location":"extensions/rethinking_usability/#tiered-api-structure","title":"Tiered API Structure","text":"<p>The tiered API structure organizes code into three abstraction levels:</p> <ul> <li> <p>High-level layer (Agent-friendly): Tools in <code>tools/</code> like <code>GetObjectGrippingPointsTool(object_name=\"cup\")</code> and <code>GetDetectionTool(camera_topic=\"/camera/rgb\", object_names=[\"cup\"])</code> with minimal required arguments, hiding pipeline details.</p> </li> <li> <p>Mid-level layer (Configurable): Components in <code>components/</code> like <code>PointCloudFilter</code> and <code>GrippingPointEstimator</code> with Config classes (<code>PointCloudFilterConfig</code>, <code>GrippingPointEstimatorConfig</code>) that expose semantic parameters (e.g., <code>strategy=\"aggressive_outlier_removal\"</code>, <code>outlier_fraction=0.05</code>) and support presets via <code>perception_presets.py</code>.</p> </li> <li> <p>Low-level layer (Expert control): Algorithms in <code>algorithms/</code> like <code>GDBoxer</code> and <code>GDSegmenter</code> that load their own configs and provide direct access to model inference and processing stages.</p> </li> </ul> <p>With this arrangement, users start with simple high-level tools, can configure mid-level components with semantic parameters (e.g., <code>strategy=\"aggressive_outlier_removal\"</code>), and access low-level algorithms only when needed.</p>"},{"location":"extensions/rethinking_usability/#folder-structure","title":"Folder Structure","text":"<p>The folder structure is now associated with abstraction level and each folder maps to a specific audience and tier:</p> <pre><code>rai_core/rai/\n\u2514\u2500\u2500 communication/ros2/        # ROS2 communication infrastructure\n    \u251c\u2500\u2500 parameters.py         # get_param_value() helper for extracting ROS2 parameter values\n    \u2514\u2500\u2500 exceptions.py         # ROS2ServiceError, ROS2ParameterError exception classes\n\nrai_perception/\n\u251c\u2500\u2500 models/                    # Model registry and interfaces\n\u251c\u2500\u2500 tools/                     # High-level LLM agent tools (BaseTool instances)\n\u251c\u2500\u2500 components/                # Mid-level configurable components and inter-package APIs\n\u251c\u2500\u2500 algorithms/                # Low-level core algorithms\n\u251c\u2500\u2500 services/                  # Model-agnostic ROS2 service nodes\n\u251c\u2500\u2500 configs/                   # Configuration files\n\u251c\u2500\u2500 scripts/                   # Utility scripts\n\u2514\u2500\u2500 examples/                  # Example code\n\nDeprecations, see later section for rationale\nrai_perception/\n\u251c\u2500\u2500 agents/                    # Legacy ROS2 service nodes (deprecated, use services/)\n\u2514\u2500\u2500 vision_markup/             # Vision markup utilities (deprecated, use algorithms/)\n</code></pre> <p>This structure aligns with cognitive dimensions: abstraction level (tools \u2192 components \u2192 algorithms). All configs are consolidated in <code>configs/</code> as user-facing runtime settings.</p>"},{"location":"extensions/rethinking_usability/#usability-improvements-along-cognitive-dimensions","title":"Usability Improvements Along Cognitive Dimensions","text":"<p>The following sections provide some examples on how the proposed design addresses usability concerns based on the refactored <code>rai_perception</code> codebase.</p>"},{"location":"extensions/rethinking_usability/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>The tiered API structure spans three abstraction levels: agent-friendly tools in <code>tools/</code>, configurable components in <code>components/</code>, and expert-level algorithms in <code>algorithms/</code>. Each layer exposes only the knobs that each audience needs, and enables experts to directly control core implementations without wrestling with irrelevant details.</p>"},{"location":"extensions/rethinking_usability/#configuration-management","title":"Configuration Management","text":"<p>The configuration system contains multiple components: algorithm configs (loaded from model registry), ROS2 parameters (deployment settings), component configs (Pydantic classes for algorithm tuning), and presets (semantic mappings for tools).</p> <p>Configuration Infrastructure Highlights:</p> <ul> <li><code>rai.communication.ros2.get_param_value()</code>: Helper function for extracting ROS2 parameter values with automatic type conversion</li> <li><code>components/perception_presets.py</code>: Semantic presets (\"default_grasp\", \"precise_grasp\", \"top_grasp\") that map to component configs, enabling high-level tool API simplification</li> </ul> <p>Note on External Dependencies: Hydra Configuration System</p> <p>Challenge: The <code>GDSegmenter</code> algorithm uses Hydra (required by SAM2) for configuration management. Hydra's config module discovery system requires proper package setup and can fail in ways that leak implementation details to high-level users, breaking API simplicity.</p> <p>Solution: For the default case (<code>config_path=None</code>), we use full file system paths instead of Hydra's config module discovery system. This preserves high-level API simplicity, application developers using tools don't encounter Hydra-specific errors or need to understand Hydra's package structure requirements. Advanced users can still use Hydra's module system for config composition and overrides when needed.</p>"},{"location":"extensions/rethinking_usability/#progressive-evaluation","title":"Progressive Evaluation","text":"<p>Current situation: Cannot test individual pipeline stages to see results. This makes debugging and incremental development difficult.</p> <p>Partial solution implemented: Debug mode (<code>debug=True</code>) added to <code>GetObjectGrippingPointsTool</code> publishes intermediate results to ROS2 topics and logs stage-level metadata, enabling visualization of pipeline stages in Rviz2. This allows users to inspect intermediate pipeline outputs without modifying code.</p> <p>Future improvements: Extend debug mode to other tools and consider exposing intermediate results as optional return values for programmatic access. This would enable users to test individual pipeline stages without running the full pipeline, supporting debugging and validation in isolation.</p>"},{"location":"extensions/rethinking_usability/#penetrability","title":"Penetrability","text":"<p>Penetrability refers to the ease of exploring and understanding API components without reading implementation details. The tiered design improves penetrability through discoverable presets and self-documenting component relationships.</p> <p>Presets improve penetrability: The <code>perception_presets.py</code> module demonstrates how presets make component relationships discoverable. Users can call <code>list_presets()</code> to see available options (<code>[\"default_grasp\", \"precise_grasp\", \"top_grasp\"]</code>), and preset names like <code>\"precise_grasp\"</code> are self-documenting\u2014no need to read implementation to understand what they do. The module-level docstring explicitly documents the component pipeline flow (<code>PointCloudFromSegmentation \u2192 PointCloudFilter \u2192 GrippingPointEstimator</code>), making relationships discoverable without inspecting code.</p> <p>High-level tools (<code>tools/</code>) use named presets over raw parameters. Instead of exposing all algorithm parameters, tools like <code>GetObjectGrippingPointsTool</code> should support semantic presets (e.g., <code>quality=\"precise_grasp\"</code>, <code>approach=\"top_grasp\"</code>) that internally map to appropriate component configurations.</p>"},{"location":"extensions/rethinking_usability/#consistency","title":"Consistency","text":"<p>Consistency refers to how much can be inferred once part of the API is learned. The tiered design establishes consistent patterns that reduce the need to read implementation details.</p> <p>Consistent naming pattern: Input schema classes follow the pattern <code>{ToolName}Input</code> (e.g., <code>GetObjectGrippingPointsToolInput</code>, <code>GetDetectionToolInput</code>, <code>GetDistanceToObjectsInput</code>). Once users learn this pattern, they can infer all input schema names without looking them up.</p> <p>Consistent parameter handling: Now tools use a standardized <code>_load_parameters()</code> method called in <code>model_post_init()</code> with parameter prefixes (e.g., <code>perception.gripping_points.*</code>, <code>perception.distance_to_objects.*</code>).</p>"},{"location":"extensions/rethinking_usability/#domain-correspondence","title":"Domain Correspondence","text":"<p>Domain correspondence refers to how clearly API components map to the robotics domain. The tiered design uses semantic naming that reflects robotics concepts rather than algorithm implementation details.</p> <p>Semantic parameter names: Mid-level components (<code>components/</code>) use semantic parameter names. Configuration classes like <code>PointCloudFilterConfig</code> and <code>GrippingPointEstimatorConfig</code> expose parameters that describe outcomes (e.g., <code>outlier_fraction=0.05</code>) rather than algorithm names (e.g., <code>if_contamination=0.05</code>).</p> <p>Domain Correspondence Changes (2025): The <code>PointCloudFilterConfig</code> has been updated to use domain-oriented parameter names and strategy names:</p> <ul> <li>Parameter names: Algorithm-specific names (<code>if_contamination</code>, <code>lof_n_neighbors</code>, <code>dbscan_eps</code>) replaced with semantic names (<code>outlier_fraction</code>, <code>neighborhood_size</code>, <code>cluster_radius_m</code>)</li> <li>Strategy names: Algorithm names (<code>\"isolation_forest\"</code>, <code>\"dbscan\"</code>, <code>\"lof\"</code>) replaced with domain-oriented names (<code>\"aggressive_outlier_removal\"</code>, <code>\"density_based\"</code>, <code>\"conservative_outlier_removal\"</code>)</li> </ul> <p>Trade-off: The tiered API design attempts to balance both needs:</p> <ul> <li>High-level tools: Use semantic presets (<code>\"precise_grasp\"</code>, <code>\"default_grasp\"</code>) that hide algorithm details</li> <li>Mid-level components: Use semantic parameter names (<code>outlier_fraction</code> instead of <code>if_contamination</code>) but document algorithm mapping</li> <li>Low-level algorithms: Direct algorithm access remains available for ML engineers who need full control</li> </ul> <p>We need to evaluate whether these increase the learning curve for Machine Learning engineers to understand the new mapping.</p>"},{"location":"extensions/rethinking_usability/#role-expressiveness","title":"Role Expressiveness","text":"<p>Role expressiveness refers to how apparent the relationship between components and the program is.</p> <p>Role Expressiveness Improvements (2025): Tools have been enhanced to make pipeline/data flow and service dependencies explicit:</p> <ul> <li> <p>Pipeline visibility: Tools now expose <code>pipeline_stages</code> class attributes and <code>get_pipeline_info()</code> methods that document the internal pipeline stages (e.g., <code>GetObjectGrippingPointsTool</code> documents its 3-stage pipeline: Point Cloud Extraction \u2192 Point Cloud Filtering \u2192 Gripping Point Estimation).</p> </li> <li> <p>Service dependency clarity: Tools now expose <code>required_services</code> class attributes, <code>get_service_info()</code> methods, and <code>check_service_dependencies()</code> methods that document which ROS2 services are required and their current availability status. This makes it clear that tools depend on services (e.g., <code>GetDetectionTool</code> requires <code>DetectionService</code>), helps users understand deployment requirements, and provides better error messages when services are unavailable.</p> </li> </ul> <p>These improvements address the role expressiveness dimension by making the relationship between tools, components, and services apparent without requiring users to read implementation details.</p> <p>Future improvements: Results should include confidence and metadata. Tools should return confidence scores, strategy used, and alternative options to help LLM agents make better decisions about retrying or adjusting approaches.</p>"},{"location":"extensions/rethinking_usability/#why-agents-are-renamed-as-services","title":"Why Agents are Renamed as Services","text":"<p>The classes previously named \"agents\" (e.g., <code>GroundingDinoAgent</code>, <code>GroundedSamAgent</code>) are being renamed to \"services\" (e.g., <code>DetectionService</code>, <code>SegmentationService</code>) for two key reasons:</p> <ol> <li> <p>Abstraction Confusion: The name \"agents\" is a misnomer because these are ROS2 service nodes, not RAI agents. The RAI framework has a distinct <code>rai.agents.BaseAgent</code> abstraction for high-level agent orchestration that uses connectors and tools. Calling ROS2 service nodes \"agents\" creates confusion about the architecture and makes it unclear what these classes actually do.</p> </li> <li> <p>ROS2-Specific Implementation: These classes are tightly coupled to ROS2 infrastructure\u2014they create <code>ROS2Connector</code> instances, use ROS2 parameters (<code>rclpy.parameter.Parameter</code>), expose ROS2 services, and cannot work with other connector types. They are ROS2 service nodes, not abstracted agents that could work with different communication backends.</p> </li> </ol> <p>The new naming clarifies that these are ROS2 service nodes that provide vision capabilities, while real RAI agents (if needed) would use these services as tools/resources rather than inheriting from them.</p>"},{"location":"extensions/rethinking_usability/#use-cases-and-impact-evaluation","title":"Use Cases and Impact Evaluation","text":"<p>This section evaluates how the proposed design impacts existing use cases and enables new capabilities.</p>"},{"location":"extensions/rethinking_usability/#existing-use-cases","title":"Existing Use Cases","text":"<p>1. Tools in RAI Agents</p> <p>Three tools are available for RAI agents: <code>GetDetectionTool</code>, <code>GetObjectGrippingPointsTool</code> and <code>GetDistanceToObjectsTool</code>.</p> <p>Impact of proposed design:</p> <ul> <li>Positive: No breaking changes for <code>GetDetectionTool</code>/<code>GetDistanceToObjectsTool</code> - tools read service_name from ROS2 params (backward compatible with defaults \"/detection\" and \"/segmentation\")</li> <li>Positive: <code>GetObjectGrippingPointsTool</code> requires no changes - already uses ROS2 params for configuration, config objects remain in <code>components/</code></li> <li>Positive: Model flexibility - can switch detection models via ROS2 param without code changes</li> <li>Positive: <code>wait_for_perception_dependencies()</code> automatically extracts service names from tools, eliminating need for hardcoded service names in most cases</li> <li>Issue: Legacy examples - some examples (e.g., <code>manipulation-demo-v1.py</code>) still hardcode old service names (<code>\"/grounding_dino_classify\"</code>, <code>\"/grounded_sam_segment\"</code>). Should be updated to use <code>wait_for_perception_dependencies()</code> or read from ROS2 params</li> <li>Issue: Backward compatibility - existing launch files/configs may assume old model-specific service names. Defaults are now generic (<code>\"/detection\"</code>, <code>\"/segmentation\"</code>), so migration requires setting ROS2 parameters or updating service names</li> <li>Resolved: Tool initialization - tools read ROS2 params at initialization with sensible defaults (<code>\"/detection\"</code> and <code>\"/segmentation\"</code>), matching current behavior</li> </ul> <p>2. ROS2 Service Nodes</p> <ul> <li><code>DetectionService</code>, <code>SegmentationService</code>: Model-agnostic ROS2 service nodes providing detection/segmentation services</li> <li>Launched via <code>rai_perception.scripts.run_perception_services</code></li> <li>Legacy agents (<code>GroundedSamAgent</code>, <code>GroundingDinoAgent</code>) are deprecated (emit deprecation warnings) and located in <code>agents/</code> folder</li> </ul> <p>3. Integration with Other RAI Components</p> <ul> <li><code>rai_semap</code>: Uses <code>rai_perception.components.perception_utils.extract_pointcloud_from_bbox</code> for semantic mapping; detection results flow from <code>rai_perception</code> services into <code>rai_semap</code> for map annotation</li> <li><code>rai_bench</code>: Used for tool-calling agent evaluation and manipulation benchmarks</li> </ul> <p>Impact of proposed design:</p> <ul> <li>No breaking changes: Tools remain in <code>tools/</code>, import paths unchanged; <code>perception_utils.py</code> is already in <code>components/</code> and <code>rai_semap</code> uses the correct import path</li> <li>Service dependencies: Service name defaults changed from model-specific (<code>\"/grounding_dino_classify\"</code>, <code>\"/grounded_sam_segment\"</code>) to generic (<code>\"/detection\"</code>, <code>\"/segmentation\"</code>). Migration guide available in <code>MIGRATION.md</code>. Tools and services can be configured via ROS2 parameters to maintain compatibility</li> <li>Model flexibility: Can test different models via ROS2 params without code changes</li> </ul>"},{"location":"extensions/rethinking_usability/#new-use-case-switching-between-detection-models","title":"New Use Case: Switching Between Detection Models","text":"<p>The tiered API structure enables a new capability: switching detection models with minimal code changes through the model registry pattern. This addresses the limitation identified in the current state where APIs lack support for model switching.</p> <p>Workflow:</p> <pre><code>1. list_available_models() \u2192 [\"grounding_dino\", \"yolo\", ...]\n2. Set ROS2 param: /detection_service/model_name = \"yolo\"\n3. Service startup:\n   \u251c\u2500 Reads model_name param\n   \u251c\u2500 get_model(\"yolo\") \u2192 (AlgorithmClass, config_path)\n   \u2514\u2500 AlgorithmClass(weights_path, config_path)\n4. Tools read service_name param (default: \"/detection\")\n5. Use tools \u2192 no code changes\n</code></pre> <p>Remaining Issues:</p> <ul> <li>Error handling: No validation at parameter declaration time; invalid <code>model_name</code> only fails at runtime</li> <li>Multiple instances: No documented pattern for running multiple models simultaneously</li> <li>Model registration: Process exists but lacks documentation</li> </ul>"},{"location":"extensions/rethinking_usability/#adding-new-models-to-the-registry","title":"Adding New Models to the Registry","text":"<p>The tiered architecture supports extensibility by enabling new models to be added following the registry pattern. This section illustrates the process with concrete examples:</p> <p>YOLO (Detection Model):</p> <ul> <li>Design compatibility: Fits tiered architecture\u2014add <code>YOLOBoxer</code> in <code>algorithms/</code>, register in <code>models/detection.py</code>. Follows same pattern as <code>GDBoxer</code>.</li> <li>Key differences: Uses class IDs (closed vocabulary) instead of text prompts (open vocabulary). Config loading typically uses YAML or no config (vs Python config for GroundingDINO).</li> <li>Implementation steps: Create <code>algorithms/yolo_boxer.py</code>, implement <code>get_boxes(image, class_ids, confidence_threshold)</code>, add COCO class ID mapping utility, register in registry, export in <code>algorithms/__init__.py</code>.</li> </ul> <p>Florence-2 (Unified Vision-Language Model):</p> <ul> <li>Design compatibility: Fits tiered architecture\u2014add <code>Florence2Algorithm</code> in <code>algorithms/</code>. Can register in both detection and segmentation registries.</li> <li>Key differences: Uses task prompts (<code>\"OD\"</code>, <code>\"SEG\"</code>) instead of task-specific models. Current separate services (<code>DetectionService</code>, <code>SegmentationService</code>) work but don't leverage unified model efficiently.</li> <li>Implementation steps: Create <code>algorithms/florence2.py</code>, implement both <code>get_boxes()</code> and <code>get_segmentation()</code> methods, handle Hugging Face model loading, parse location tokens to bounding boxes/masks, register in both registries.</li> <li>Future considerations: Capability-based registry (<code>capability=\"detection+segmentation\"</code>) and unified service architecture would be more efficient for multi-task models.</li> </ul>"},{"location":"extensions/rethinking_usability/#good-to-have-deferred","title":"Good to Have (deferred)","text":"<p>Config Utilities (<code>rai.config</code>):</p> <ul> <li><code>load_yaml_config()</code>: Unified YAML loading with ROS2 parameter extraction; reduces boilerplate but current manual approach is explicit</li> <li><code>get_config_path()</code>: Standardizes ROS2 parameter path resolution; only used in one place currently</li> <li><code>merge_nested_configs()</code>: Handles nested dict merging; current flat presets work fine with manual updates</li> <li>Trade-off: Utilities reduce duplication but add abstraction; current approach is explicit and maintainable</li> </ul> <p>Multi-Stage Pipeline Service Failures:</p> <ul> <li>Multi-stage tools (e.g., <code>GetObjectGrippingPointsTool</code>) don't indicate which pipeline stage failed</li> <li>Solution: Raise <code>ROS2ServiceError</code> with pipeline stage info for stage-specific recovery</li> <li>Benefit: Enables stage-specific retry, clearer diagnostics, better LLM agent error understanding</li> </ul>"},{"location":"extensions/rethinking_usability/#future-work","title":"Future Work","text":""},{"location":"extensions/rethinking_usability/#data-collection-for-fine-tuning","title":"Data Collection for Fine-Tuning","text":"<p>Design compatibility:</p> <ul> <li>No blocking points: Current tiered structure (tools \u2192 components \u2192 algorithms) provides clear instrumentation points at each level</li> <li>ROS2 parameter helpers (<code>rai.communication.ros2.get_param_value()</code>) can be extended for data collection configuration</li> <li>Component-based design allows adding collection hooks without breaking existing APIs</li> <li>Consideration: Need to design collection API that works across all abstraction tiers without adding overhead to high-level tools</li> </ul>"},{"location":"extensions/rethinking_usability/#observability","title":"Observability","text":"<p>Design compatibility:</p> <ul> <li>No blocking points: Tiered structure naturally supports instrumentation at each level</li> <li>Service layer (<code>services/</code>) provides centralized point for service-level metrics</li> <li>Component abstraction allows adding observability decorators/wrappers without changing core logic</li> <li>Parameter registry can include observability configuration (enable/disable, verbosity levels)</li> <li>Consideration: Need to ensure observability doesn't leak into high-level tool APIs (keep tools simple for LLM agents)</li> <li>Consideration: Design should support optional observability - not required for basic usage</li> </ul>"},{"location":"faq/ROS_2_Overview/","title":"Overview","text":"<p>RAI provides an abstraction layer for ROS 2 which streamlines the process of integrating ROS 2 with LLMs and other AI components. This integration bridge is essential for modern robotics systems that need to leverage artificial intelligence capabilities alongside traditional robotics frameworks.</p> <p>At the heart of this integration is the <code>rai.communication.ros2.ROS2Connector</code>, which provides a unified interface to ROS 2's subscription, service, and action APIs. This abstraction layer makes it straightforward to build ROS 2 agents and LangChain tools that can seamlessly communicate with both robotics and AI components.</p> Why is RAI not a ROS 2 package? <p>RAI was initially developed as a ROS 2 package, but this approach proved problematic for several key reasons:</p> <ol> <li> <p>Dependency Management</p> <ul> <li>ROS 2's dependency system (rosdep) is too vague and inflexible for RAI's needs</li> <li>RAI heavily relies on the Python ecosystem, particularly for AI/LLM integration</li> <li>Poetry provides more precise version control and dependency resolution, crucial for AI/ML components</li> </ul> </li> <li> <p>Architectural Separation</p> <ul> <li>The initial ROS 2 package approach made it difficult to maintain clear separation between:<ul> <li>Core AI/LLM functionality</li> <li>Communication layer</li> <li>Robot-specific implementations</li> </ul> </li> <li>This separation is crucial for maintainability and extensibility</li> </ul> </li> <li> <p>Development Workflow</p> <ul> <li>ROS 2's build system adds unnecessary complexity for Python-based AI development</li> <li>The mixed C++/Python ecosystem of ROS 2 doesn't align well with RAI's Python-first approach</li> <li>Faster development cycles are possible with a Python-centric architecture</li> </ul> </li> <li> <p>Flexibility and Portability</p> <ul> <li>RAI needs to work both with and without ROS 2 which allows vast amount of use cases which contribute to the RAI ecosystem</li> <li>The framework should be deployable in various environments</li> <li>Different communication protocols should be easily supported</li> </ul> </li> </ol> <p>The current architecture, with RAI as a Python framework that works with ROS 2 rather than as ROS 2, provides the best of both worlds:  - Full ROS 2 compatibility through the <code>ROS2Connector</code> abstraction  - Clean separation of concerns  - Flexible dependency management  - Better maintainability and extensibility</p>"},{"location":"faq/ROS_2_Overview/#key-features","title":"Key Features","text":"<ul> <li>Unified Communication Interface: Simple, generic interface for ROS 2 topics, services, and actions</li> <li>Automatic QoS Matching: Handles Quality of Service profiles automatically</li> <li>Thread-Safe Operations: Built-in thread safety for concurrent operations</li> </ul>"},{"location":"faq/ROS_2_Overview/#supported-ros-2-features","title":"Supported ROS 2 Features","text":"<ul> <li>Topics (publish/subscribe)</li> <li>Services (request/response)</li> <li>Actions (long-running operations with feedback)</li> <li>TF2 transforms</li> </ul>"},{"location":"faq/ROS_2_Overview/#integration-examples","title":"Integration Examples","text":"<pre><code>from rai.communication.ros2 import ROS2Connector, ROS2Context, ROS2Message\n\n\n@ROS2Context()\ndef main():\n    connector = ROS2Connector()\n\n    # Subscribe to a topic\n    def my_custom_callback(message: ROS2Message):\n        message.payload  # actual ROS 2 message\n\n    connector.register_callback(\"/topic\", my_custom_callback)\n\n    # Receive a message\n    message = connector.receive_message(\"/topic\")\n    message.payload  # actual ROS 2 message\n\n    # Publish a message\n    message = ROS2Message(payload={\"data\": \"Hello, ROS 2!\"})\n    connector.send_message(message, \"/topic\", msg_type=\"std_msgs/msg/String\")\n\n    # Call a service\n    request = ROS2Message(payload={\"data\": True})\n    response = connector.service_call(\n        message=request, target=\"/service\", msg_type=\"std_srvs/msg/SetBool\"\n    )\n\n    # Start an action\n    def my_custom_feedback_callback(feedback: ROS2Message):\n        feedback.payload  # actual ROS 2 feedback\n\n    message = ROS2Message(\n        payload={\"pose\": {\"position\": {\"x\": 1.0, \"y\": 2.0, \"z\": 0.0}}}\n    )\n    action_id = connector.start_action(\n        action_data=message,\n        target=\"/navigate_to_pose\",\n        msg_type=\"nav2_msgs/action/NavigateToPose\",\n        feedback_callback=my_custom_feedback_callback,\n    )\n\n    # Cancel an action\n    connector.terminate_action(action_id)\n</code></pre>"},{"location":"faq/ROS_2_Overview/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Context Management</p> <ul> <li>Always use <code>ROS2Context</code> as a decorator or context manager</li> <li>Ensures proper initialization and cleanup</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Use the connector's shutdown method when done</li> <li>Handle exceptions appropriately</li> </ul> </li> <li> <p>Performance Considerations</p> <ul> <li>Use appropriate QoS profiles</li> <li>Consider deregistering callbacks when not needed</li> </ul> </li> </ol>"},{"location":"faq/ROS_2_Overview/#common-use-cases","title":"Common Use Cases","text":"<ol> <li> <p>Robot Control</p> <ul> <li>Navigation commands</li> <li>Manipulation tasks</li> <li>Sensor data processing</li> </ul> </li> <li> <p>System Integration</p> <ul> <li>Connecting AI components to ROS 2</li> <li>Multi-agent coordination</li> </ul> </li> </ol>"},{"location":"faq/ROS_2_Overview/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Connection Problems</p> <ul> <li>Check ROS 2 network configuration</li> <li>Verify topic/service names</li> <li>Ensure proper QoS matching</li> </ul> </li> <li> <p>Performance Issues</p> <ul> <li>Monitor thread usage</li> <li>Check QoS settings</li> <li>Verify message sizes</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Proper cleanup of resources</li> <li>Handling of node lifecycle</li> <li>Memory management</li> </ul> </li> </ol>"},{"location":"faq/ROS_2_Overview/#future-developments","title":"Future Developments","text":"<ul> <li>Performance optimizations</li> </ul>"},{"location":"faq/faq/","title":"RAI FAQ","text":"What is RAI and how does it work? What is RAI? <p>RAI is a flexible AI agent framework to develop and deploy Embodied AI features for your robots.</p> How does RAI work? <p></p> <p>RAI connects human instructions to robot actions in four simple steps:</p> <ul> <li>Human gives a command: The user interacts with the robot using speech or text.</li> <li>RAI understands and reasons: RAI interprets the request, plans the response, and decides on the best action.</li> <li>RAI controls the robot: It sends commands to the robot through ROS 2 or the robot's SDK.</li> <li>Robot responds: The robot performs the action and can communicate back to the user, closing the loop.</li> </ul> Who is RAI for? <p>RAI is for robotics developers, researchers, educators, startups, companies, and hobbyists who want to add AI and natural language capabilities to robots, whether using ROS 2 or other robotics stacks. And anyone who\u2019s ever looked at a robot and thought, \u201cI wish you understood me.\u201d</p> What can I do with RAI? <p>With RAI, you can control robots using voice or text, integrate vision, speech, and sensor data, build and test complex robot behaviors, use ready-made demos, develop multi-agent systems, create custom tools, implement real-time decision making, and design interactive scenarios for human-robot interaction. Or just teach your robot to bring you snacks. We don\u2019t judge.</p> What demos are available for RAI? <p>RAI offers demos for manipulation tasks, autonomous navigation with ROSbot XL and agricultural robotics. See the documentation for details on each demo.</p> <p>Demos from ROSCon 2024</p> <p></p> How do I build my own solution with RAI? <p>Follow the step-by-step walkthrough to deploy RAI on your robot, add custom tools and features, and create advanced multi-agent systems for complex tasks.</p> Does using RAI cost money? <p>RAI is free and open source software. However, if you use RAI with cloud-based models or third-party services, those may incur costs depending on the provider, model, and your usage. Please review the terms and pricing of any external services you choose to use with RAI. To use RAI cost-free, make sure to use local models and services.</p> How can I get started with RAI? How do I install RAI? <p>Check out the Quick Setup Guide to install RAI, try a demo, or follow the walkthrough to build your own solution.</p> How do I try a demo? <p>Check out the Quick Setup Guide to install RAI, try a demo, or follow the walkthrough to build your own solution.</p> How do I build my own solution? <p>Follow the step-by-step walkthrough to deploy RAI on your robot, add custom tools and features, and create advanced multi-agent systems for complex tasks.</p> How do I extend RAI? Missing a feature? <p>Start by checking our API documentation to see if the feature you need already exists. If not, you're welcome to open a new issue or contribute your own solution! The step-by-step walkthrough is a great way to get familiar with RAI's structure. With these resources, you'll be well-equipped to add new features or customize RAI for your needs. If you have questions, don't hesitate to reach out to the community!</p> How do I integrate a new type of sensor? <p>Integration depends on what you want to achieve with your sensor data. If you'd like to use the data with an LLM or multimodal model, you'll need to write a small adapter that converts the sensor output into a format the model can understand (such as text or images). For other use cases, you can create a custom tool or agent that processes the sensor data as needed. If you need guidance, our community is happy to help\u2014just ask on Discord or open a discussion!</p> <p>Note</p> <p>At the moment, RAI does not implement a module for converting sensor data into text or images. These conversions are done on the application level/user side.</p> How do I contribute to RAI? Contributtion guide <p>To develop RAI, you just need to know how to run <code>pre-commit</code> (spoiler alert: just type <code>pre-commit</code>), and you\u2019re basically a core developer. For anything more complicated (or if you like reading rules), see the Contribution Guide.</p> Existing issues/bugs/something is missings <p>If you find an issue/bug/feature missing, please check the existing issues and if it is not already reported, create a new one.</p> I would like to contribute, but I don't know what <p>If you would like to contribute, but don't know what to do, please join our Discord and we will help you find something to work on. Alternatively, browse the issues for inspiration.</p> I've never contributed to open source before, how do I start? <p>Welcome! We love helping newcomers get started with open source. The RAI community is friendly and supportive\u2014no prior experience with RAI required. Check out our Contribution Guide for step-by-step instructions, and feel free to join our Discord to ask questions or get guidance. We're here to help you every step of the way!</p> How do I get support for RAI? <p>Where can I learn more about RAI?</p> <p>You can find support by joining our Discord, or visiting the Embodied AI Community Group. For talks and demos, see the ROSCon 2024 links in the documentation.</p> <p>Where can I get support?</p> <p>You can find support and more information in the Contribution Guide, on the Q&amp;A forum, issues, by joining our Discord, or visiting the Embodied AI Community Group. For talks and demos, see the ROSCon 2024 links in the documentation.</p> What do I need to get started with RAI? <p>What are the requirements?</p> <ul> <li>ROS 2 Humble or Jazzy</li> <li>Python 3.10 or 3.12</li> <li>Ubuntu 22.04 or 24.04</li> </ul> <p>How about a robot?</p> <p>RAI is compatible with any robot that has a ROS 2 interface or exposes another type of interface. The important thing is that your robot must already have a robotic stack set up. RAI builds on top of your existing robotic stack, but does not include the stack itself.</p> <p>Do I even need a robot?</p> <p>No, you can also run the demos on your computer or in custom-made simulations. Or just pretend. We won\u2019t tell.</p> RAI outside of robotics <p>Can I use RAI outside of robotics?</p> <p>Yes, you can use RAI in non-robotic applications. RAI supports ROS 2, but is not limited to it.</p> <p>How do I use RAI outside of robotics?</p> <p>RAI is designed to be flexible. There are multiple abstractions, that make working with multimodal data, multi-agent system, various communication protocols easy. For more information see the RAI API documentation</p> Is RAI limited to LLMs? <p>Can I use models other than LLMs in RAI?</p> <p>No, RAI is not limited to large language models. You can integrate any type of model\u2014such as vision models, speech recognition, classical AI, or custom algorithms\u2014into your RAI agents. The BaseAgent abstraction is designed to make it easy to plug in any form of \"intelligence\" or decision logic.</p> <p>How do I use other models in RAI?</p> <p>To use a different model, simply implement your logic within a custom agent using the BaseAgent abstraction. This allows you to connect vision, speech, or any other AI model to your robot or application. For more details and code examples, see the RAI API documentation.</p> Licensing &amp; Commercial Use <p>Can I use RAI commercially?</p> <p>Yes, RAI is licensed under the Apache 2.0 license.</p> <p>Are there any restrictions on commercial use?</p> <p>No, RAI is licensed under the Apache 2.0 license, which allows for commercial use. Make sure to follow the license terms and give credits to the original authors.</p> Security &amp; Privacy <p>How does RAI handle security and privacy?</p> <p>RAI can use both local and cloud models. For maximum privacy, we recommend using the most performing local models. For cloud models, we recommend using tested and trusted providers.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/","title":"RAI Contributors Code of Conduct","text":"<p>RAI adopts the ROS Code of Conduct maintained by the OSRF. We expand the scope of the Code of Conduct to all official RAI communication channels, including, but not limited to:</p> <ul> <li>RAI source code repositories</li> <li>RAI discord and other communication channels</li> </ul> <p>What follows is the adopted ROS Code of Conduct (retrieved: 2024-08-22). It also includes contact information to RAI internal Conduct Team.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#ros-code-of-conduct","title":"ROS Code of Conduct","text":"<ul> <li>ROS Code of Conduct</li> <li>Python Software Foundation Code of Conduct</li> <li>The RUST language Code of Conduct</li> <li>Contributor Covenant</li> <li>Frame Shift Consulting Code of Conduct Book</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#code-of-conduct-for-ros","title":"Code of Conduct for ROS","text":"<p>View the Project on GitHub</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#ros-community-code-of-conduct","title":"ROS Community Code of Conduct","text":""},{"location":"faq/contributing/CODE_OF_CONDUCT/#preamble-attribution","title":"Preamble / Attribution","text":"<p>The ROS Code of Conduct draws heavily from the work of other open source software communities and tries to synthesize their efforts to address the specific needs of the ROS community. In particular, we draw heavily from the following prior work:</p> <ul> <li>Python Software Foundation Code of Conduct</li> <li>The RUST Language Code of Conduct</li> <li>Contributor Covenant</li> <li>Frame Shift Consulting Code of Conduct Book</li> </ul> <p>It is worth noting that this is a living document to be used to protect community members. It should not be interpreted as a hard and fast legal guide for community behavior. Instead, it should be interpreted as a broad outline of acceptable and unacceptable behavior, how to report unacceptable behavior, and how it will be dealt with.</p> <p>The ROS Code of Conduct is released under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#scope-of-the-code-of-conduct","title":"Scope of the Code of Conduct","text":"<p>This guide applies to all people, events, web properties, communication media, and source code repositories administered by Open Robotics and the ROS 2 Technical Steering Committee. This includes but is not limited to:</p> <ul> <li>ROS source code repositories</li> <li>ROS Discourse</li> <li>ROS Wiki</li> <li>ROS Documentation</li> <li>ROS Answers</li> <li>ROSCon</li> <li>ROS.org</li> <li>All ROS 2 TSC working groups</li> <li>All Open Robotics administered e-mail lists</li> <li>Comments made on event video hosting services</li> <li>Comments made on the official event or ROS hashtags</li> </ul> <p>If you administer a ROS affiliated organization outside of the organizations listed above and would like to adopt this code of conduct for your own project, please contact us at conduct@robotec.ai. We will require the contact information for at least one administrator for your project.</p> <p>Outside organizations that have adopted this code of conduct include the following organizations:</p> <ul> <li>None at this time.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#code-of-conduct","title":"Code of Conduct","text":"<p>The ROS community is made up of members from around the globe with a diverse set of skills, personalities, and experiences. It is through these differences that our community experiences great successes and continued growth. When you\u2019re working with members of the community, this Code of Conduct will help steer your interactions and keep ROS a positive, successful, and growing community.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#our-community","title":"Our Community","text":"<p>Members of the ROS community are open, considerate, and respectful. Behaviors that reinforce these values contribute to a positive environment, and include:</p> <ul> <li>Being open. Members of the community are open to collaboration, whether it\u2019s on REPs, patches, problems, or otherwise.</li> <li>Focusing on what is best for the community. We\u2019re respectful of the processes set forth in the community, and we work within them.</li> <li>Acknowledging time and effort. We\u2019re respectful of the volunteer efforts that permeate the ROS community. We\u2019re thoughtful when addressing the efforts of others, keeping in mind that oftentimes the labor was completed simply for the good of the community.</li> <li>Being respectful of differing viewpoints and experiences. We\u2019re receptive to constructive comments and criticism, as the experiences and skill sets of other members contribute to the whole of our efforts.</li> <li>Showing empathy towards other community members. We\u2019re attentive in our communications, whether in person or online, and we\u2019re tactful when approaching differing views.</li> <li>Being considerate. Members of the community are considerate of their peers \u2013 other ROS users.</li> <li>Being respectful. We\u2019re respectful of others, their positions, their skills, their commitments, and their efforts.</li> <li>Gracefully accepting constructive criticism. When we disagree, we are courteous in raising our issues.</li> <li>Using welcoming and inclusive language. We\u2019re accepting of all who wish to take part in our activities, fostering an environment where anyone can participate and everyone can make a difference.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Every member of our community has the right to have their identity respected. The ROS community is dedicated to providing a positive experience for everyone, regardless of age, gender identity and expression, sexual orientation, disability, physical appearance, body size, ethnicity, nationality, race, or religion (or lack thereof), education, or socio-economic status.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#inappropriate-behavior","title":"Inappropriate Behavior","text":"<p>Examples of unacceptable behavior by participants include:</p> <ul> <li>Harassment of any participants in any form.</li> <li>Deliberate intimidation, stalking, or following.</li> <li>Logging or taking screenshots of online activity for harassment purposes.</li> <li>Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission.</li> <li>Violent threats or language directed against another person.</li> <li>Incitement of violence or harassment towards any individual, including encouraging a person to commit suicide or to engage in self-harm.</li> <li>Creating additional online accounts in order to harass another person or circumvent a ban.</li> <li>Sexual language and imagery in online communities or in any conference venue, including talks.</li> <li>Insults, put downs, or jokes that are based upon stereotypes, that are exclusionary, or that hold others up for ridicule.</li> <li>Excessive swearing.</li> <li>Unwelcome sexual attention or advances.</li> <li>Unwelcome physical contact, including simulated physical contact (e.g., textual descriptions like \u201chug\u201d or \u201cbackrub\u201d) without consent or after a request to stop.</li> <li>Pattern of inappropriate social contact, such as requesting/assuming inappropriate levels of intimacy with others.</li> <li>Sustained disruption of online community discussions, in-person presentations, or other in-person events.</li> <li>Continued one-on-one communication after requests to cease.</li> <li>Other conduct that is inappropriate for a professional audience, including people of many different backgrounds.</li> </ul> <p>Community members asked to stop any inappropriate behavior are expected to comply immediately.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#weapons-policy","title":"Weapons Policy","text":"<p>No weapons are allowed at ROS physical events. Weapons include but are not limited to explosives (including fireworks), guns, and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately and will only be allowed to return without the weapon.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>The Code of Conduct Team is responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>The Code of Conduct Team has the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#current-code-of-conduct-team","title":"Current Code of Conduct Team","text":"<p>The Code of Conduct Team consists of three volunteers from the ROS community. Optimally, these members are located at multiple locations across the globe to provide for a timely response to conduct questions and violations, and provide native language support as best we can. The Conduct Team members serve a two-year term with replacements nominated by the ROS 2 Technical Steering Committee. The Conduct Team works to adjudicate conduct violations using a consensus model. For most situations, that is those that don\u2019t require an immediate response, the Conduct Team will issue reports and enforcement actions representing the consensus of the team.</p> <p>The current Code of Conduct Team consists of:</p> <ul> <li>Wiktoria Siekierska wiktoria.siekierska@robotec.ai</li> <li>Adam Kuty\u0142owski adam.kutylowski@robotec.ai</li> <li>Maciej Majek maciej.majek@robotec.ai</li> </ul> <p>The entire team can be contacted using conduct@robotec.ai. The team can arrange for other means of communications after the initial contact. We recommend you use the conduct@robotec.ai address unless you wish to report a Conduct Team member, or you feel uncomfortable communicating with a certain team member.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the Conduct Team responsible for enforcement at conduct@robotec.ai. All complaints will be reviewed and responded to within 48 hours. The Conduct Team is obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>The Conduct Team will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#correction","title":"Correction","text":"<ul> <li>Your contact info (so we can get in touch with you if we need to follow up)</li> <li>Date and time of the incident</li> <li>Any links, screen shots, videos, or other media that may help</li> <li>Location of the incident</li> <li>Whether the incident is ongoing</li> <li>Description of the incident</li> <li>Identifying information of the reported person</li> <li>Additional circumstances surrounding the incident</li> <li> <p>Other people involved in or witnesses to the incident and their contact information or description</p> </li> <li> <p>Example Behavior: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> </li> <li>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#warning","title":"Warning","text":"<ul> <li>Example Behavior: A violation through a single incident or series of actions.</li> <li>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#temporary-ban","title":"Temporary Ban","text":"<ul> <li>Example Behavior: A serious violation of community standards, including sustained inappropriate behavior.</li> <li>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#permanent-ban","title":"Permanent Ban","text":"<ul> <li>Example Behavior: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</li> <li>Consequence: A permanent ban from any sort of public interaction within the community.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#how-to-report-a-conduct-violation","title":"How To Report A Conduct Violation","text":"<p>If you believe someone is in physical danger, including from themselves, the most important thing is to get that person help. Please contact the appropriate crisis number, non-emergency number, or police number as appropriate. If you are at a ROS event, you can consult with a volunteer or staff member to help find an appropriate number.</p> <p>If you believe someone has violated the ROS Code of Conduct, we encourage you to report it. If you are unsure whether the incident is a violation, or whether the space where it happened is covered by the Code of Conduct, we encourage you to still report it. We are fine with receiving reports where we decide to take no action for the sake of creating a safer space.</p> <p>The ROS-related forums and events listed above should have a designated moderator or Code of Conduct point of contact. Larger gatherings, like conferences, may have several people to contact. Specific information should be available for each listed gathering, online or off.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#report-template","title":"Report Template","text":"<p>When you make a report via email or phone, please provide as much information as possible to help us make a fair and accurate decision about the appropriate response. The following template should serve as a guide for reporting:</p> <ul> <li>Your contact info (so we can get in touch with you if we need to follow up)</li> <li>Date and time of the incident</li> <li>Any links, screenshots, videos, or other media that may help</li> <li>Location of the incident</li> <li>Whether the incident is ongoing</li> <li>Description of the incident</li> <li>Identifying information of the reported person</li> <li>Additional circumstances surrounding the incident</li> <li>Other people involved in or witnesses to the incident and their contact information or description</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#confidentiality","title":"Confidentiality","text":"<p>All reports will be kept confidential. When we discuss incidents with people who are reported, we will anonymize details as much as we can to protect reporter privacy.</p> <p>However, some incidents happen in one-on-one interactions, and even if the details are anonymized, the reported person may be able to guess who made the report. If you have concerns about retaliation or your personal safety, please note those in your report. In some cases, we can compile several anonymized reports into a pattern of behavior and take action on that pattern.</p> <p>In some cases, we may determine that a public statement will need to be made. If that\u2019s the case, the identities of all victims and reporters will remain confidential unless those individuals instruct us otherwise.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#report-handling-procedure","title":"Report Handling Procedure","text":"<p>When you make a report to the Conduct Team, they will gather information about the incident. If the incident is ongoing and needs to be immediately addressed, any Conduct Team member may take appropriate action to ensure the safety of everyone involved. If the situation requires it, this may take the form of a referral to an appropriate agency, including the local police. The Conduct Team is not equipped to handle emergency situations.</p> <p>If the incident is less urgent, the report will be discussed by the Conduct Team to determine an appropriate response. You should receive a response from the Conduct Team within 48 hours confirming the receipt of the report, and potentially asking for follow-up information. Within one week of an incident report, a member of the Conduct Team will follow up with the person who made the report. The follow-up may include:</p> <ul> <li>An acknowledgment that the Code of Conduct responders discussed the situation</li> <li>Whether or not the report was determined to be a violation of the Code of Conduct</li> <li>What actions (if any) were taken to correct the reported behavior</li> </ul>"},{"location":"faq/contributing/CONTRIBUTING/","title":"Contributing","text":"<p>We are very happy you want to contribute to RAI, and we welcome all input. This document outlines guidelines for contributors to follow.</p> <p>Our philosophy is strongly aligned with the philosophy of the ROS development process. These guidelines have been therefore strongly influenced by the ROS2 Contributing document.</p>"},{"location":"faq/contributing/CONTRIBUTING/#tenets","title":"Tenets","text":"<ul> <li> <p>Engage Robotec.ai as early as possible</p> <ul> <li>Start discussions with Robotec.ai and the community early. Long time RAI contributors may have a     clearer vision of the big picture. If you implement a feature and send a pull request without     discussing with the community first, you are taking the risk of it being rejected, or you may be     asked to largely rethink your design.</li> <li>Opening issues or using Discourse to socialize an idea before starting the implementation is     generally preferable.</li> </ul> </li> <li> <p>Adopt community best-practices whenever possible instead of ad-hoc processes</p> <p>Think about the end-users experience when developing and contributing. Features accessible to a larger amount of potential users, utilising widely available solutions are more likely to be accepted.</p> </li> <li> <p>Think about the community as a whole</p> <p>Think about the bigger picture. There are developers building different robots with different constraints. The landscape of available AI models is rapidly changing, coming with different capabilities and constraints. RAI wants to accommodate requirements of the whole community.</p> </li> </ul> <p>There are a number of ways you can contribute to the RAI project.</p>"},{"location":"faq/contributing/CONTRIBUTING/#discussions-and-support","title":"Discussions and support","text":"<p>Some of the easiest ways to contribute to RAI involve engaging in community discussions and support. This can be done by creating Issues and RFCs on github.</p>"},{"location":"faq/contributing/CONTRIBUTING/#contributing-code","title":"Contributing code","text":""},{"location":"faq/contributing/CONTRIBUTING/#setting-up-the-development-environment","title":"Setting up the development environment","text":"<p>Set up your development environment following the instructions.</p> <p>Additionally, setup the pre-commit:</p> <pre><code>sudo apt install shellcheck\npre-commit install\npre-commit run -a # Run the checks before committing\n</code></pre> <p>Optionally, install all RAI dependencies and run the tests:</p> <pre><code>poetry install --all-groups\ncolcon build --symlink-install\npytest tests/\n</code></pre>"},{"location":"faq/contributing/CONTRIBUTING/#starting-the-discussion","title":"Starting the discussion","text":"<p>Always try to engage in discussion first. Browse Issues and RFCs or start a discussion on ROS Embodied AI Community Group Discord to see if a feature you want to propose (or a similar one) has already been mentioned. If that is the case feel free to offer that you'll work on it, and propose what changes/additions you will make. One of the project maintainers will assign the issue to you, and you can start working on the code.</p>"},{"location":"faq/contributing/CONTRIBUTING/#submitting-code-changes","title":"Submitting code changes","text":"<p>To submit a change begin by forking this repository and making the changes on the fork. Once the changes are ready to be proposed create a pull request back to the repository. In order to maintain a linear and clear commit history please:</p> <ul> <li>make sure that all commits have meaningful messages</li> <li>if batches of \"cleanup\" or similar commits are present - squash them together</li> <li>rebase onto the main branch of repository before making the PR</li> </ul> <p>We follow the Conventional Commits specification for our commit messages. This means that each commit message should be structured as follows:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Common types include:</p> <ul> <li><code>feat</code>: A new feature</li> <li><code>fix</code>: A bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>style</code>: Changes that do not affect the meaning of the code</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature</li> <li><code>test</code>: Adding missing tests or correcting existing tests</li> <li><code>chore</code>: Changes to the build process or auxiliary tools</li> </ul> <p>Always make sure that both all tests are passing before making the PR. Once this is done open your PR describing what changes have been made and how to test if it's working. Request review from the maintainer who assigned you the issue.</p> <p>If the review requires modifications please make them on your forked repository and go through the above process. Once the PR is accepted it will be merged into the repository. Congratulations, and thank you for your contributions to the development of RAI.</p>"},{"location":"faq/contributing/CONTRIBUTING/#versioning-policy","title":"Versioning policy","text":"<p>RAI follows Semantic Versioning using the format MAJOR.MINOR.PATCH.</p> <ol> <li> <p>MAJOR version increments indicate incompatible API changes.    A MAJOR release may require users to update their code, configuration, or integration logic.    Breaking changes must be clearly documented in the release notes, including migration guidance when possible.</p> </li> <li> <p>MINOR version increments indicate backward compatible functionality additions.    A MINOR release may add new features, extensions, or optional capabilities without breaking existing behavior.    Existing APIs must continue to work as documented.</p> </li> <li> <p>PATCH version increments indicate backward compatible bug fixes.    A PATCH release must not introduce new features or behavior changes beyond fixing defects.    Performance improvements are acceptable if they do not change observable behavior.</p> </li> </ol> <p>For more information about semantic versioning, see https://semver.org/.</p>"},{"location":"faq/contributing/CONTRIBUTING/#pre-release-and-development-versions","title":"Pre release and development versions","text":"<p>During active development, pre release identifiers may be used to signal unstable or experimental states. These versions are not guaranteed to maintain backward compatibility and should not be relied upon in production systems. An example of a pre release version is <code>3.0.0a1</code>.</p>"},{"location":"faq/contributing/CONTRIBUTING/#versioning-scope","title":"Versioning scope","text":"<p>The version number applies to the public RAI API surface, including user facing Python APIs, ROS interfaces, and configuration schemas. Internal implementation details that are not part of the documented API may change without triggering a MAJOR version bump.</p>"},{"location":"faq/contributing/CONTRIBUTING/#release-requirements","title":"Release requirements","text":"<p>Before publishing a release:</p> <ol> <li>All tests must pass.</li> <li>Public API changes must be documented.</li> </ol> <p>This policy is intended to provide clear expectations for users and contributors about compatibility and upgrade impact.</p>"},{"location":"faq/contributing/SECURITY/","title":"Security Policy","text":""},{"location":"faq/contributing/SECURITY/#supported-versions","title":"Supported Versions","text":"<p>Only the newest release for Ubuntu 22.04, Ubuntu 24.04; ROS 2 Humble, and ROS 2 Jazzy are supported. The policy will evolve as the project matures.</p> Version Supported 0.9.x 1.0.x 1.1.x"},{"location":"faq/contributing/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Treat and report vulnerability as any other bug.</p>"},{"location":"intro/what_is_rai/","title":"Welcome to RAI","text":"<p>RAI is an open-source framework that makes it easy to add AI-powered features\u2014like natural language control and smart perception to your robots. It's designed to bridge the gap between cutting-edge AI capabilities and practical robotics applications, enabling developers to create more intelligent and accessible robotic systems.</p> <p>Whether you're building industrial automation solutions, educational robots, or research prototypes, RAI provides the tools and infrastructure needed to bring advanced AI capabilities to your robotics projects. With its modular architecture and flexible integration options, RAI adapts to your needs while maintaining high performance and reliability.</p>"},{"location":"intro/what_is_rai/#what-is-rai","title":"What is RAI?","text":"<ul> <li>It helps you control robots using natural language and advanced AI, making them more intuitive and accessible to use.</li> <li>Works with popular robotics systems (like ROS 2), but can also be used on its own, giving you flexibility in how you integrate it.</li> <li>Combines vision, speech, and sensor data to create comprehensive robotic understanding and control.</li> </ul>"},{"location":"intro/what_is_rai/#how-does-rai-work","title":"How does RAI work?","text":"<p>RAI connects human instructions to robot actions in four simple steps:</p> <ul> <li>Human gives a command: The user interacts with the robot using speech or text.</li> <li>RAI understands and reasons: RAI interprets the request, plans the response, and decides on the best action.</li> <li>RAI controls the robot: It sends commands to the robot through ROS 2 or the robot\u2019s SDK.</li> <li>Robot responds: The robot performs the action and can communicate back to the user, closing the loop.</li> </ul>"},{"location":"intro/what_is_rai/#who-is-rai-for","title":"Who is RAI for?","text":"<ul> <li>Robotics developers and researchers looking to enhance their systems with AI capabilities</li> <li>Anyone who wants to add AI and natural language to robots with existing robotics stack</li> <li>Teams building real-world robotic applications that require intuitive human-robot interaction</li> <li>Educational institutions teaching robotics, agentic systems and AI integration</li> <li>Startups and companies developing next-generation robotic solutions</li> <li>Hobbyists interested in exploring AI-powered robotics</li> </ul>"},{"location":"intro/what_is_rai/#what-can-i-do-with-rai","title":"What can I do with RAI?","text":"<ul> <li>Control robots with voice or text commands, making them more accessible and intuitive to use</li> <li>Integrate vision, speech, and sensor data for comprehensive environmental understanding</li> <li>Build and test complex robot behaviors through natural language instructions</li> <li>Use ready-made demos or create your own solutions tailored to specific use cases</li> <li>Develop multi-agent systems that can collaborate on complex tasks</li> <li>Create custom tools and behaviors that extend RAI's core functionality</li> <li>Implement real-time decision making based on environmental inputs</li> <li>Design interactive scenarios where robots can understand and respond to human intentions</li> </ul>"},{"location":"intro/what_is_rai/#quick-start","title":"Quick Start","text":"<p>Ready to try RAI?</p> <ul> <li>Quick Setup Guide</li> <li>Try a Demo</li> <li>Build Your Own Solution</li> </ul>"},{"location":"intro/what_is_rai/#try-our-demos","title":"Try Our Demos","text":"<p>See RAI in action:</p> <ul> <li>\ud83e\udd16 Manipulation Tasks - Watch RAI control a Franka Panda arm using natural language</li> <li>\ud83d\ude97 Autonomous Navigation - Explore RAI's capabilities with the ROSbot XL platform</li> <li>\ud83d\ude9c Agricultural Robotics - See how RAI handles complex decision-making in orchard environments</li> </ul>"},{"location":"intro/what_is_rai/#build-your-own-solution","title":"Build Your Own Solution","text":"<p>Follow our step-by-step walkthrough to:</p> <ul> <li>Deploy RAI on your robot</li> <li>Add custom tools and features</li> <li>Create advanced, multi-agents systems to tackle complex tasks</li> </ul>"},{"location":"intro/what_is_rai/#how-rai-works","title":"How RAI Works","text":"<ul> <li>Modular: Use only the parts you need</li> <li>Works with ROS 2 (Humble, Jazzy) but is not limited to it</li> <li>Handles communication, perception, and reasoning</li> </ul>"},{"location":"intro/what_is_rai/#community-support","title":"Community &amp; Support","text":"<ul> <li>Contribution Guide</li> <li>FAQ</li> <li>Join our Discord</li> <li>Embodied AI Community Group</li> </ul>"},{"location":"intro/what_is_rai/#learn-more","title":"Learn More","text":"<ul> <li>RAI at ROSCon 2024 (Talk)</li> <li>RAI Demos at ROSCon 2024</li> </ul>"},{"location":"setup/install/","title":"Quick setup guide","text":"<p>Before going further, make sure you have ROS 2 (jazzy or humble) installed and sourced on your system.</p> <p>Docker images</p> <p>RAI has experimental docker images. See the docker for instructions.</p> <p>There are two ways to start using RAI:</p> <ol> <li> <p>Installing RAI using pip (recommended for end users)</p> </li> <li> <p>Setting up a developer environment using poetry (recommended for developers)</p> </li> </ol>"},{"location":"setup/install/#installing-rai","title":"Installing RAI","text":"Virtual environment <p>We recommend installing RAI in a virtual environment (e.g., virtualenv, uv, or poetry) to keep your dependencies organized. Make sure to use the same version of python as the one used for ROS 2 (typically <code>python3.10</code> for Humble and <code>python3.12</code> for Jazzy).</p> <p>If you plan to use ROS 2 commands (<code>ros2 run</code> or <code>ros2 launch</code>), you'll need to add your virtual environment's Python packages to your <code>$PYTHONPATH</code>. This step is only necessary for ROS 2 integration - if you're just running RAI directly with Python, you can skip this step.</p> <p>For reference, here's how to set this up when installing RAI from source: setup_shell.sh</p> <ol> <li> <p>Install core functionality:</p> <pre><code>pip install rai-core\n</code></pre> </li> <li> <p>Initialize the global configuration file:</p> <pre><code>rai-config-init\n</code></pre> </li> <li> <p>Optionally install ROS 2 dependencies:</p> <pre><code>sudo apt install ros-${ROS_DISTRO}-rai-interfaces\n</code></pre> </li> </ol> <p>Package availability</p> <p><code>rai_perception</code> and <code>rai_nomad</code> are not yet available through pip. If your workflow relies on openset detection or NoMaD integration, please refer to the developer environment instructions setup.</p> <p><code>rai_interfaces</code> is available as <code>apt</code> package. However, due to package distribution delays, the latest version may not be immediately available. If you encounter missing imports, please build <code>rai_interfaces</code> from source.</p> RAI modules <p>RAI is a modular framework. You can install only the modules you need.</p> Module Description Documentation rai-core Core functionality link rai-whoami Embodiment module link rai-s2s Speech-to-Speech module link rai-sim Simulation module link rai-bench Benchmarking module link RAI outside of ROS 2 <p>RAI can be used outside of ROS 2. This means that no ROS 2 related features will be available.</p> <p>You can still use RAI's core agent framework, tool system, message passing, and integrations such as LangChain, even if ROS 2 is not installed or sourced on your machine. This is useful for:</p> <ul> <li>Developing and testing AI logic, tools, and workflows independently of any robotics middleware</li> <li>Running RAI agents in simulation or cloud environments where ROS 2 is not present</li> <li>Using RAI as a generic multimodal agent framework for non-robotic applications</li> </ul> <p>If you later decide to integrate with ROS 2, you can simply install and source ROS 2, and all ROS 2-specific RAI features (such as connectors, aggregators, and tools) will become available automatically.</p>"},{"location":"setup/install/#setting-up-developer-environment","title":"Setting up developer environment","text":""},{"location":"setup/install/#11-install-poetry","title":"1.1 Install poetry","text":"<p>RAI uses Poetry(2.1+) for python packaging and dependency management. Install poetry with the following line:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Alternatively, you can opt to do so by following the official docs.</p>"},{"location":"setup/install/#12-clone-the-repository","title":"1.2 Clone the repository:","text":"<pre><code>git clone https://github.com/RobotecAI/rai.git\ncd rai\n</code></pre>"},{"location":"setup/install/#13-download-rai_interfaces","title":"1.3 Download rai_interfaces","text":"<pre><code>vcs import &lt; ros_deps.repos\n</code></pre>"},{"location":"setup/install/#14-create-poetry-virtual-environment-and-install-dependencies","title":"1.4 Create poetry virtual environment and install dependencies:","text":"<pre><code>poetry install\nrosdep install --from-paths src --ignore-src -r -y\n</code></pre> <p>Additional dependencies</p> <p>RAI is modular. If you want to use features such as speech-to-speech, simulation and benchmarking suite, openset detection, or NoMaD integration, install additional dependencies:</p> <pre><code>poetry install --with perception,nomad,s2s,simbench # or `--all-groups` for full setup\n</code></pre> Group Name Description Dependencies s2s Speech-to-Speech functionality rai_asr, rai_tts simbench Simulation and benchmarking tools rai_sim, rai_bench perception Open-set detection capabilities groundingdino, groundedsam nomad Visual Navigation - NoMaD integration visualnav_transformer docs Documentation-related dependencies mkdocs, mkdocs-material, pymdown-extensions"},{"location":"setup/install/#15-configure-rai","title":"1.5 Configure RAI","text":"<p>Run the configuration tool to set up your LLM vendor and other settings:</p> <pre><code>poetry run streamlit run src/rai_core/rai/frontend/configurator.py\n</code></pre> <p>Web browser</p> <p>If the web browser does not open automatically, open the URL displayed in the terminal manually.</p>"},{"location":"setup/install/#2-build-the-project","title":"2. Build the project:","text":""},{"location":"setup/install/#21-build-rai-workspace","title":"2.1 Build RAI workspace","text":"<pre><code>colcon build --symlink-install\n</code></pre>"},{"location":"setup/install/#22-activate-the-virtual-environment","title":"2.2 Activate the virtual environment:","text":"<pre><code>source ./setup_shell.sh\n</code></pre>"},{"location":"setup/install/#3-setting-up-vendors","title":"3. Setting up vendors","text":"<p>RAI is vendor-agnostic. Use the configuration in <code>config.toml</code> to set up your vendor of choice for RAI modules. Vendor choices for RAI and our recommendations are summarized in Vendors Overview.</p> <p>Best-performing AI models</p> <p>We strongly recommend you to use of best-performing AI models to get the most out of RAI!</p> <p>Pick your local solution or service provider and follow one of these guides:</p> <ul> <li>Ollama</li> <li>OpenAI</li> <li>AWS Bedrock</li> </ul>"},{"location":"setup/setup_docker/","title":"Setup RAI with docker","text":"<p>Docker images are experimental</p> <p>Docker images are experimental. For tested setup, see the local setup.</p>"},{"location":"setup/setup_docker/#1-build-the-docker-image","title":"1. Build the docker image","text":"<p>Choose the docker image based on your preferred ROS 2 version. You may build the selected image with only the core dependencies or, alternatively, with all the additional modules. To build the docker image, you must clone the RAI repository:</p> <pre><code>git clone https://github.com/RobotecAI/rai.git\ncd rai\n</code></pre>"},{"location":"setup/setup_docker/#11-humble","title":"1.1. Humble","text":"<p>Core dependencies only:</p> <pre><code>docker build -t rai:humble --build-arg ROS_DISTRO=humble -f docker/Dockerfile .\n</code></pre> <p>All dependencies:</p> <pre><code>docker build -t rai:humble --build-arg ROS_DISTRO=humble --build-arg DEPENDENCIES=all_groups -f docker/Dockerfile .\n</code></pre>"},{"location":"setup/setup_docker/#12-jazzy","title":"1.2. Jazzy","text":"<p>Core dependencies only:</p> <pre><code>docker build -t rai:jazzy --build-arg ROS_DISTRO=jazzy -f docker/Dockerfile .\n</code></pre> <p>All dependencies:</p> <pre><code>docker build -t rai:jazzy --build-arg ROS_DISTRO=jazzy --build-arg DEPENDENCIES=all_groups -f docker/Dockerfile .\n</code></pre>"},{"location":"setup/setup_docker/#2-set-up-communications-between-docker-and-host-optional","title":"2. Set up communications between docker and host (Optional)","text":"<p>ROS 2 communication</p> <p>If you intend to run demos on the host machine, ensure the docker container can communicate with it. Test this by running the standard ROS 2 example with one node in docker and one on the host: link.</p> <p>ROS 2 distributions</p> <p>It is highly recommended that ROS 2 distribution on the host machine matches the ROS 2 distribution of the docker container. A distribution version mismatch may result in the demos not working correctly.</p> <p>To allow the container to communicate with the host machine, configure the host environment as presented below:</p> <ol> <li> <p>Source ROS 2 on the host machine:</p> <pre><code>source /opt/ros/jazzy/setup.bash # or humble\n</code></pre> </li> <li> <p>If not configured, set the <code>ROS_DOMAIN_ID</code> environment variable to a domain ID between 0 and 101, inclusive. Example:</p> <pre><code>export ROS_DOMAIN_ID=99\n</code></pre> </li> <li> <p>Install the eProsima Fast DDS middleware (should come preinstalled with ROS 2):</p> <pre><code>sudo apt install ros-\"${ROS_DISTRO}\"-fastrtps\n</code></pre> </li> <li> <p>Configure the DDS middleware using the <code>fastrtps_config.xml</code> file included in the RAI repository:</p> <pre><code>export FASTRTPS_DEFAULT_PROFILES_FILE=$(pwd)/docker/fastrtps_config.xml\n</code></pre> </li> <li> <p>Set the RMW to use eProsima Fast DDS:</p> <pre><code>export RMW_IMPLEMENTATION=rmw_fastrtps_cpp\n</code></pre> </li> </ol>"},{"location":"setup/setup_docker/#3-run-the-docker-container","title":"3. Run the docker container","text":""},{"location":"setup/setup_docker/#31-humble","title":"3.1. Humble","text":"<pre><code>docker run --net=host --ipc=host --pid=host -e ROS_DOMAIN_ID=$ROS_DOMAIN_ID -it rai:humble\n</code></pre>"},{"location":"setup/setup_docker/#32-jazzy","title":"3.2. Jazzy","text":"<pre><code>docker run --net=host --ipc=host --pid=host -e ROS_DOMAIN_ID=$ROS_DOMAIN_ID -it rai:jazzy\n</code></pre>"},{"location":"setup/setup_docker/#4-run-the-tests-to-confirm-the-setup","title":"4. Run the tests to confirm the setup","text":"<pre><code>cd /rai\nsource setup_shell.sh\npoetry run pytest tests/{agents,messages,tools,types}\n</code></pre>"},{"location":"setup/tracing/","title":"Tracing Configuration","text":"<p>RAI supports tracing capabilities to help monitor and analyze the performance of your LLM applications, at a minor performance cost. By default, tracing is off. This document outlines how to configure tracing for your RAI project.</p>"},{"location":"setup/tracing/#configuration","title":"Configuration","text":"<p>Tracing configuration is managed through the <code>config.toml</code> file. The relevant parameters for tracing are:</p>"},{"location":"setup/tracing/#project-name","title":"Project Name","text":"<p>The <code>project</code> field under the <code>[tracing]</code> section sets the name for your tracing project. This name will be used to identify your project in the tracing tools.</p> <p>Project name</p> <p>Project name is currently only used by LangSmith. Langfuse will upload traces to the default project.</p>"},{"location":"setup/tracing/#langfuse-open-source","title":"Langfuse (open-source)","text":"<p>Langfuse is an open-source observability &amp; analytics platform for LLM applications.</p> <p>To enable Langfuse tracing:</p> <ol> <li>Set <code>use_langfuse = true</code> in the <code>config.toml</code> file.</li> <li>Set the <code>LANGFUSE_PUBLIC_KEY</code> and <code>LANGFUSE_SECRET_KEY</code> environment variables with your Langfuse    credentials.</li> <li>Optionally, you can specify a custom Langfuse host by modifying the <code>host</code> field under    <code>[tracing.langfuse]</code>.</li> </ol>"},{"location":"setup/tracing/#langsmith-closed-source-paid-limited-free-tier","title":"LangSmith (closed-source, paid, limited free tier)","text":"<p>LangSmith is a platform for debugging, testing, and monitoring LangChain applications.</p> <p>To enable LangSmith tracing:</p> <ol> <li>Set <code>use_langsmith = true</code> in the <code>config.toml</code> file.</li> <li>Set the <code>LANGCHAIN_API_KEY</code> environment variable with your LangSmith API key.</li> <li> <p>Optionally, you can specify a custom LangSmith host by modifying the <code>host</code> field under    <code>[tracing.langsmith]</code>.</p> <p>For deployment details please refer to LangFuse documentation</p> </li> </ol>"},{"location":"setup/tracing/#usage","title":"Usage","text":"<p>To enable tracing in your RAI application, you need to import the <code>rai.get_tracing_callbacks</code> function and add it to the configuration when invoking your agent or model. Here's how to do it:</p> <ol> <li> <p>First, import the <code>get_tracing_callbacks()</code> function:</p> <pre><code>from rai import get_tracing_callbacks\n</code></pre> </li> <li> <p>Then, add it to the configuration when invoking your agent or model:</p> <pre><code>response = agent.invoke(\n    input_dict,\n    config={\"callbacks\": get_tracing_callbacks()}\n)\n</code></pre> </li> </ol> <p>By adding the get_tracing_callbacks() to the config parameter, you enable tracing for that specific invocation. The get_tracing_callbacks() function returns a list of callback handlers based on your configuration in config.toml.</p>"},{"location":"setup/tracing/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with tracing:</p> <ol> <li>Ensure all required environment variables are set correctly.</li> <li>Check whether tracing is on by checking whether <code>use_langsmith</code> or <code>use_langfuse</code> flag is set to    <code>true</code> in <code>config.toml</code>.</li> <li>Verify that you have the necessary permissions and valid API keys for the tracing services you're    using.</li> <li>Look for any error messages in your application logs related to tracing initialization.</li> </ol> <p>For more detailed information on using these tracing tools, refer to their respective documentation:</p> <ul> <li>LangSmith Documentation</li> <li>Langfuse Documentation</li> </ul>"},{"location":"setup/vendors/","title":"Vendor setup","text":"<p>RAI supports multiple vendors for AI models and tracing tools, both open-source and commercial APIs. To setu[ it is recommended to use the RAI Configurator.</p> <p>Alternatively vendors can be configured manually in <code>config.toml</code> file.</p>"},{"location":"setup/vendors/#vendors-overview","title":"Vendors Overview","text":"<p>The table summarizes vendor alternative for core AI service and optional RAI modules:</p> Module Open source Alternative Why to consider alternative? More information LLM service Ollama OpenAI, Bedrock Overall performance of the LLM models, supported modalities and features LangChain models Optional: Tracing tool Langfuse LangSmith Better integration with LangChain Comparison Optional: Text to speech KokoroTTS, OpenTTS ElevenLabs Arguably, significantly better voice synthesis <li> KokoroTTS </li><li> OpenTTS GitHub </li><li> RAI voice interface </li> Optional: Speech to text Whisper OpenAI Whisper (hosted) When suitable local GPU is not an option <li> Whisper GitHub </li><li> RAI voice interface </li> <p>Best-performing AI models</p> <p>Our recommendation, if your environment allows it, is to go with OpenAI GPT4o model, ElevenLabs for TTS, locally-hosted Whisper, and Langsmith.</p>"},{"location":"setup/vendors/#llm-model-configuration-in-rai","title":"LLM Model Configuration in RAI","text":"<p>In RAI you can configure 2 models: <code>simple model</code> and <code>complex model</code>:</p> <ul> <li><code>complex model</code> should be used for sophisticated tasks like multi-step reasoning.</li> <li><code>simple model</code> is more suitable for simpler tasks for example image description.</li> </ul> <pre><code>from rai import get_llm_model\n\ncomplex_llm = get_llm_model(model_type=\"complex\")\nsimple_llm = get_llm_model(model_type=\"simple\")\n</code></pre>"},{"location":"setup/vendors/#vendors-installation","title":"Vendors Installation","text":""},{"location":"setup/vendors/#ollama","title":"Ollama","text":"<p>Ollama can be used to host models locally.</p> <ol> <li>Install <code>Ollama</code> see: https://ollama.com/download</li> <li>Start Ollama server: <code>ollama serve</code></li> <li>Choose LLM model and endpoint type. Ollama server deliveres 2 endpoints:<ul> <li>Ollama endpoint: RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>ollama</code> vendor</li> <li>OpenAI endpoint: RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>openai</code> vendor -&gt; <code>Use OpenAI compatible API</code>   Both endpoints should work interchangeably and decision is only dedicated by user's convenience.</li> </ul> </li> </ol>"},{"location":"setup/vendors/#openai","title":"OpenAI","text":"<ol> <li>Setup your OpenAI account, generate    and set the API key:    <code>bash export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Use RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>ollama</code> vendor</li> </ol>"},{"location":"setup/vendors/#aws-bedrock","title":"AWS Bedrock","text":"<ol> <li> <p>Set AWS Access Keys keys to your AWS account.</p> <pre><code>export AWS_ACCESS_KEY_ID=\"...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport AWS_SESSION_TOKEN=\"...\"\n</code></pre> </li> <li> <p>Use RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>bedrock</code> vendor</p> </li> </ol>"},{"location":"setup/vendors/#complex-llm-model-configuration","title":"Complex LLM Model Configuration","text":"<p>For custom setups please use LangChain API.</p> <pre><code>from langchain_openai.chat_models import ChatOpenAI\nfrom langchain_aws.chat_models import ChatBedrock\nfrom langchain_community.chat_models import ChatOllama\n\nllm1 = ChatOpenAI(model=\"gpt-4o\")\nllm2 = ChatOllama(model='llava')\nllm = ChatBedrock(model=\"anthropic.claude-3-opus-20240229-v1:0\")\n</code></pre>"},{"location":"setup/vendors/#text-to-speech","title":"Text To Speech","text":"<p>For configuration use <code>Text To Speech</code> tab in RAI Configurator.</p> <p>Usage examples can be found in Voice Interface Tutorial</p>"},{"location":"setup/vendors/#speech-to-text","title":"Speech To Text","text":"<p>For configuration use <code>Speech Recognition</code> tab in RAI Configurator.</p> <p>Usage examples can be found in Voice Interface Tutorial</p>"},{"location":"simulation_and_benchmarking/overview/","title":"Simulation and Benchmarking Overview","text":"<p>RAI provides a comprehensive framework for simulation and benchmarking that consists of two main components:</p>"},{"location":"simulation_and_benchmarking/overview/#rai-sim","title":"RAI Sim","text":"<p>RAI Sim provides a simulator-agnostic interface that allows RAI to work with any simulation environment. It defines a standard interface (<code>SimulationBridge</code>) that abstracts the details of different simulators, enabling:</p> <ul> <li>Consistent behavior across different simulation environments</li> <li>Easy integration with new simulators</li> <li>Seamless switching between simulation backends</li> </ul> <p>The package also provides simulator bridges for concrete simulators, currently supporting only O3DE. For detailed information about the simulation interface, see RAI Sim Documentation.</p>"},{"location":"simulation_and_benchmarking/overview/#rai-bench","title":"RAI Bench","text":"<p>RAI Bench provides benchmarks with ready-to-use tasks and a framework to create your own tasks. It enables:</p> <ul> <li>Define and execute tasks</li> <li>Measure and evaluate performance</li> <li>Collect and analyze results</li> </ul> <p>For detailed information about the benchmarking framework, see RAI Bench Documentation.</p>"},{"location":"simulation_and_benchmarking/overview/#integration","title":"Integration","text":"<p>RAI Sim and RAI Bench work together to provide benchmarks which utilize simulations for evaluation:</p> <ol> <li>Simulation Interface: RAI Sim provides the foundation with its simulator-agnostic interface</li> <li>Task Definition: RAI Bench defines tasks that can be executed in any supported simulator</li> <li>Execution: Tasks are executed through the simulation interface</li> <li>Evaluation: Results are collected and analyzed using the benchmarking framework</li> </ol> <p>This architecture allows for:</p> <ul> <li>Flexible task definition independent of the simulator</li> <li>Consistent evaluation across different simulation environments</li> <li>Easy addition of new simulators and tasks</li> <li>Comprehensive performance analysis</li> </ul>"},{"location":"simulation_and_benchmarking/overview/#use-cases","title":"Use Cases","text":"<p>The combined framework supports various use cases:</p> <ol> <li>Task Evaluation: Testing and comparing different approaches to the same task</li> <li>Performance Analysis: Measuring and analyzing system performance</li> <li>Development Testing: Validating new features in simulation</li> <li>Research: Conducting experiments in controlled environments</li> </ol> <p>For specific implementation details and examples, refer to the respective documentation files.</p>"},{"location":"simulation_and_benchmarking/rai_bench/","title":"RAI Bench","text":"<p>RAI Bench is a comprehensive package that both provides benchmarks with ready-to-use tasks and offers a framework for creating new tasks. It's designed to evaluate the performance of AI agents in various environments.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#available-benchmarks","title":"Available Benchmarks","text":"<ul> <li>Manipulation O3DE Benchmark</li> <li>Tool Calling Agent Benchmark</li> <li>VLM Benchmark</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#manipulation-o3de-benchmark","title":"Manipulation O3DE Benchmark","text":"<p>Evaluates agent performance in robotic arm manipulation tasks within the O3DE simulation environment. The benchmark evaluates how well agents can process sensor data and use tools to manipulate objects in the environment.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#framework-components","title":"Framework Components","text":"<p>Manipulation O3DE Benchmark provides a framework for creating custom tasks and scenarios with these core components:</p> <p></p>"},{"location":"simulation_and_benchmarking/rai_bench/#task","title":"Task","text":"<p>The <code>Task</code> class is an abstract base class that defines the interface for tasks used in this benchmark. Each concrete Task must implement:</p> <ul> <li>prompts that will be passed to the agent</li> <li>validation of simulation configurations</li> <li>calculating results based on scene state</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#scenario","title":"Scenario","text":"<p>A <code>Scenario</code> represents a specific test case combining:</p> <ul> <li>A task to be executed</li> <li>A simulation configuration</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#manipulationo3debenchmark","title":"ManipulationO3DEBenchmark","text":"<p>The <code>ManipulationO3DEBenchmark</code> class manages the execution of scenarios and collects results. It provides:</p> <ul> <li>Scenario execution management</li> <li>Performance metrics tracking</li> <li>Logs and results</li> <li>Robotic stack needed, provided as <code>LaunchDescription</code></li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#available-tasks","title":"Available Tasks","text":"<p>The benchmark includes several predefined manipulation tasks:</p> <ol> <li> <p>MoveObjectsToLeftTask - Move specified objects to the left side of the table</p> </li> <li> <p>PlaceObjectAtCoordTask - Place specified objects at specific coordinates</p> </li> <li> <p>PlaceCubesTask - Place specified cubes adjacent to each other</p> </li> <li> <p>BuildCubeTowerTask - Stack specified cubes to form a tower</p> </li> <li> <p>GroupObjectsTask - Group specified objects of specified types together</p> </li> </ol> <p>Tasks are parametrizable so you can configure which objects should be manipulated and how much precision is needed to complete a task.</p> <p>Tasks are scored on a scale from 0.0 to 1.0, where:</p> <ul> <li>0.0 indicates no improvement or worse placement than the starting one</li> <li>1.0 indicates perfect completion</li> </ul> <p>The score is typically calculated as:</p> <pre><code>score = (correctly_placed_now - correctly_placed_initially) / initially_incorrect\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#available-scene-configs-and-scenarios","title":"Available Scene Configs and Scenarios","text":"<p>You can find predefined scene configs in <code>rai_bench/manipulation_o3de/predefined/configs/</code>.</p> <p>Predefined scenarios can be imported, for example, choosing tasks by difficulty:</p> <pre><code>from rai_bench.manipulation_o3de import get_scenarios\n\nget_scenarios(levels=[\"easy\", \"medium\"])\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#tool-calling-agent-benchmark","title":"Tool Calling Agent Benchmark","text":"<p>Evaluates agent performance independently from any simulation, based only on tool calls that the agent makes. To make it independent from simulations, this benchmark introduces tool mocks which can be adjusted for different tasks. This makes the benchmark more universal and a lot faster.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#framework-components_1","title":"Framework Components","text":""},{"location":"simulation_and_benchmarking/rai_bench/#subtask","title":"SubTask","text":"<p>The <code>SubTask</code> class is used to validate just one tool call. Following classes are available:</p> <ul> <li><code>CheckArgsToolCallSubTask</code> - verify if a certain tool was called with expected arguments</li> <li><code>CheckTopicFieldsToolCallSubTask</code> - verify if a message published to ROS2 topic was of proper type and included expected fields</li> <li><code>CheckServiceFieldsToolCallSubTask</code> - verify if a message published to ROS2 service was of proper type and included expected fields</li> <li><code>CheckActionFieldsToolCallSubTask</code> - verify if a message published to ROS2 action was of proper type and included expected fields</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#validator","title":"Validator","text":"<p>The <code>Validator</code> class can combine single or multiple subtasks to create a single validation step. Following validators are available:</p> <ul> <li>OrderedCallsValidator - requires a strict order of subtasks. The next subtask will be validated only when the previous one was completed. Validator passes when all subtasks pass.</li> <li>NotOrderedCallsValidator - doesn't enforce order of subtasks. Every subtask will be validated against every tool call. Validator passes when all subtasks pass.</li> <li>OneFromManyValidator - passes when any one of the given subtasks passes.</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#task_1","title":"Task","text":"<p>A Task represents a specific prompts and set of tools available. A list of validators is assigned to validate the performance.</p> Task class definition <p>As you can see, the framework is very flexible. Any SubTask can be combined into any Validator that can be later assigned to any Task.</p> <p>Every Task needs to define it's prompt and system prompt, what tools agent will have available, how many tool calls are required to complete it and how many optional tool calls are possible.</p> <p>Optional tool calls mean that a certain tool calls is not obligatory to pass the Task, but shoudn't be considered an error, example: <code>GetROS2RGBCameraTask</code> which has prompt: <code>Get RGB camera image.</code> requires making one tool call with <code>get_ros2_image</code> tool. But listing topics before doing it is a valid approach, so in this case opitonal tool calls is <code>1</code>.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task","title":"<code>rai_bench.tool_calling_agent.interfaces.Task</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>class Task(ABC):\n    complexity: Literal[\"easy\", \"medium\", \"hard\"]\n    type: str\n    recursion_limit: int = DEFAULT_RECURSION_LIMIT\n\n    def __init__(\n        self,\n        validators: List[Validator],\n        task_args: TaskArgs,\n        logger: loggers_type | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Abstract base class representing a complete task to be validated.\n\n        A Task consists of multiple Validators, where each Validator can be treated as a single\n        step that is scored atomically. Each Task has a consistent prompt and available tools,\n        with validation methods that can be parameterized.\n\n        Attributes\n        ----------\n        complexity : Literal[\"easy\", \"medium\", \"hard\"]\n            difficulty level of the task\n        type : str\n            type identifier for the task\n        recursion_limit : int, optional\n            maximum recursion depth allowed, by default DEFAULT_RECURSION_LIMIT\n\n        Parameters\n        ----------\n        validators : List[Validator]\n            List of validators that will be applied in sequence.\n        task_args : TaskArgs\n            Configuration parameters for the task specified by user\n        logger : logging.Logger\n            Logger for recording task validation results and errors.\n        \"\"\"\n        if logger:\n            self.logger = logger\n        else:\n            self.logger = logging.getLogger(__name__)\n        self.validators = validators\n        self.extra_tool_calls = task_args.extra_tool_calls\n        self.prompt_detail = task_args.prompt_detail\n        self.n_shots = task_args.examples_in_system_prompt\n\n    def set_logger(self, logger: loggers_type):\n        self.logger = logger\n        for validator in self.validators:\n            validator.logger = logger\n\n    def get_tool_calls_from_invoke(self, response: dict[str, Any]) -&gt; list[ToolCall]:\n        \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n        tool_calls: List[ToolCall] = []\n        for msg in response[\"messages\"]:\n            if isinstance(msg, AIMessage):\n                tool_calls.extend(msg.tool_calls)\n        return tool_calls\n\n    def get_tool_calls_from_messages(\n        self, messages: List[BaseMessage]\n    ) -&gt; list[ToolCall]:\n        \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n        tool_calls: List[ToolCall] = []\n        for msg in messages:\n            if isinstance(msg, AIMessage):\n                tool_calls.extend(msg.tool_calls)\n        return tool_calls\n\n    def dump_validators(self) -&gt; List[ValidatorResult]:\n        return [val.dump_results() for val in self.validators]\n\n    @property\n    @abstractmethod\n    def available_tools(self) -&gt; List[BaseTool]:\n        \"\"\"List of tool available for the agent\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def optional_tool_calls_number(self) -&gt; int:\n        \"\"\"Optional tool calls means calls that are not considered error.\n        For example listing topics at the beginning.\"\"\"\n        pass\n\n    @property\n    def max_tool_calls_number(self) -&gt; int:\n        \"\"\"maxiumum number of call to still pass task.\n        Includes extra tool calls params.\n        and optional tool calls number which depends on task.\n        \"\"\"\n        return (\n            self.required_calls\n            + self.optional_tool_calls_number\n            + self.extra_tool_calls\n        )\n\n    @property\n    def additional_calls(self) -&gt; int:\n        \"\"\"number of additional calls that can be done to still pass task.\n        Includes extra tool calls params.\n        and optional tool calls number which depends on task.\n        \"\"\"\n        return self.optional_tool_calls_number + self.extra_tool_calls\n\n    @property\n    def required_calls(self) -&gt; int:\n        \"\"\"Minimal number of calls required to complete task\"\"\"\n        total = 0\n        for val in self.validators:\n            total += len(val.subtasks)\n        return total\n\n    @abstractmethod\n    def get_system_prompt(self) -&gt; str:\n        \"\"\"Get the system prompt that will be passed to agent\n\n        Returns\n        -------\n        str\n            System prompt\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_base_prompt(self) -&gt; str:\n        \"\"\"\n        Get the base task instruciton,\n        it will be used to identify task in results processing\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_prompt(self) -&gt; str:\n        \"\"\"Get the task instruction - the prompt that will be passed to agent.\n\n        Returns\n        -------\n        str\n            Prompt\n        \"\"\"\n        pass\n\n    def validate(self, tool_calls: List[ToolCall]):\n        \"\"\"Validate a list of tool calls against all validators in sequence\"\"\"\n        self.logger.debug(\n            f\"required_calls: {self.required_calls}, extra_calls {self.extra_tool_calls}\"\n        )\n        remaining_tool_calls = tool_calls[: self.max_tool_calls_number].copy()\n        self.logger.debug(f\"Tool calls to validate: {remaining_tool_calls}\")\n\n        done_properly = 0\n        for validator in self.validators:\n            if_success, remaining_tool_calls = validator.validate(\n                tool_calls=remaining_tool_calls\n            )\n\n            if if_success:\n                done_properly += 1\n\n        return done_properly / len(self.validators)\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.additional_calls","title":"<code>additional_calls</code>  <code>property</code>","text":"<p>number of additional calls that can be done to still pass task. Includes extra tool calls params. and optional tool calls number which depends on task.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.available_tools","title":"<code>available_tools</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>List of tool available for the agent</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.max_tool_calls_number","title":"<code>max_tool_calls_number</code>  <code>property</code>","text":"<p>maxiumum number of call to still pass task. Includes extra tool calls params. and optional tool calls number which depends on task.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.optional_tool_calls_number","title":"<code>optional_tool_calls_number</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Optional tool calls means calls that are not considered error. For example listing topics at the beginning.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.required_calls","title":"<code>required_calls</code>  <code>property</code>","text":"<p>Minimal number of calls required to complete task</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.__init__","title":"<code>__init__(validators, task_args, logger=None)</code>","text":"<p>Abstract base class representing a complete task to be validated.</p> <p>A Task consists of multiple Validators, where each Validator can be treated as a single step that is scored atomically. Each Task has a consistent prompt and available tools, with validation methods that can be parameterized.</p> <p>Attributes:</p> Name Type Description <code>complexity</code> <code>Literal['easy', 'medium', 'hard']</code> <p>difficulty level of the task</p> <code>type</code> <code>str</code> <p>type identifier for the task</p> <code>recursion_limit</code> <code>(int, optional)</code> <p>maximum recursion depth allowed, by default DEFAULT_RECURSION_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>validators</code> <code>List[Validator]</code> <p>List of validators that will be applied in sequence.</p> required <code>task_args</code> <code>TaskArgs</code> <p>Configuration parameters for the task specified by user</p> required <code>logger</code> <code>Logger</code> <p>Logger for recording task validation results and errors.</p> <code>None</code> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def __init__(\n    self,\n    validators: List[Validator],\n    task_args: TaskArgs,\n    logger: loggers_type | None = None,\n) -&gt; None:\n    \"\"\"\n    Abstract base class representing a complete task to be validated.\n\n    A Task consists of multiple Validators, where each Validator can be treated as a single\n    step that is scored atomically. Each Task has a consistent prompt and available tools,\n    with validation methods that can be parameterized.\n\n    Attributes\n    ----------\n    complexity : Literal[\"easy\", \"medium\", \"hard\"]\n        difficulty level of the task\n    type : str\n        type identifier for the task\n    recursion_limit : int, optional\n        maximum recursion depth allowed, by default DEFAULT_RECURSION_LIMIT\n\n    Parameters\n    ----------\n    validators : List[Validator]\n        List of validators that will be applied in sequence.\n    task_args : TaskArgs\n        Configuration parameters for the task specified by user\n    logger : logging.Logger\n        Logger for recording task validation results and errors.\n    \"\"\"\n    if logger:\n        self.logger = logger\n    else:\n        self.logger = logging.getLogger(__name__)\n    self.validators = validators\n    self.extra_tool_calls = task_args.extra_tool_calls\n    self.prompt_detail = task_args.prompt_detail\n    self.n_shots = task_args.examples_in_system_prompt\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_base_prompt","title":"<code>get_base_prompt()</code>  <code>abstractmethod</code>","text":"<p>Get the base task instruciton, it will be used to identify task in results processing</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>@abstractmethod\ndef get_base_prompt(self) -&gt; str:\n    \"\"\"\n    Get the base task instruciton,\n    it will be used to identify task in results processing\n    \"\"\"\n    pass\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_prompt","title":"<code>get_prompt()</code>  <code>abstractmethod</code>","text":"<p>Get the task instruction - the prompt that will be passed to agent.</p> <p>Returns:</p> Type Description <code>str</code> <p>Prompt</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>@abstractmethod\ndef get_prompt(self) -&gt; str:\n    \"\"\"Get the task instruction - the prompt that will be passed to agent.\n\n    Returns\n    -------\n    str\n        Prompt\n    \"\"\"\n    pass\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_system_prompt","title":"<code>get_system_prompt()</code>  <code>abstractmethod</code>","text":"<p>Get the system prompt that will be passed to agent</p> <p>Returns:</p> Type Description <code>str</code> <p>System prompt</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>@abstractmethod\ndef get_system_prompt(self) -&gt; str:\n    \"\"\"Get the system prompt that will be passed to agent\n\n    Returns\n    -------\n    str\n        System prompt\n    \"\"\"\n    pass\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_tool_calls_from_invoke","title":"<code>get_tool_calls_from_invoke(response)</code>","text":"<p>Extracts all tool calls from the response, flattened across all AI messages.</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def get_tool_calls_from_invoke(self, response: dict[str, Any]) -&gt; list[ToolCall]:\n    \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n    tool_calls: List[ToolCall] = []\n    for msg in response[\"messages\"]:\n        if isinstance(msg, AIMessage):\n            tool_calls.extend(msg.tool_calls)\n    return tool_calls\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_tool_calls_from_messages","title":"<code>get_tool_calls_from_messages(messages)</code>","text":"<p>Extracts all tool calls from the response, flattened across all AI messages.</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def get_tool_calls_from_messages(\n    self, messages: List[BaseMessage]\n) -&gt; list[ToolCall]:\n    \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n    tool_calls: List[ToolCall] = []\n    for msg in messages:\n        if isinstance(msg, AIMessage):\n            tool_calls.extend(msg.tool_calls)\n    return tool_calls\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.validate","title":"<code>validate(tool_calls)</code>","text":"<p>Validate a list of tool calls against all validators in sequence</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def validate(self, tool_calls: List[ToolCall]):\n    \"\"\"Validate a list of tool calls against all validators in sequence\"\"\"\n    self.logger.debug(\n        f\"required_calls: {self.required_calls}, extra_calls {self.extra_tool_calls}\"\n    )\n    remaining_tool_calls = tool_calls[: self.max_tool_calls_number].copy()\n    self.logger.debug(f\"Tool calls to validate: {remaining_tool_calls}\")\n\n    done_properly = 0\n    for validator in self.validators:\n        if_success, remaining_tool_calls = validator.validate(\n            tool_calls=remaining_tool_calls\n        )\n\n        if if_success:\n            done_properly += 1\n\n    return done_properly / len(self.validators)\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#toolcallingagentbenchmark","title":"ToolCallingAgentBenchmark","text":"<p>The ToolCallingAgentBenchmark class manages the execution of tasks and collects results.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#available-tasks_1","title":"Available Tasks","text":"<p>There are predefined Tasks available which are grouped by categories:</p> <ul> <li>Basic - require retrieving info from certain topics</li> <li>Manipulation</li> <li>Custom Interfaces - requires using messages with custom interfaces</li> </ul> <p>Every Task has assigned the <code>complexity</code> which reflects the difficulty.</p> <p>When creating a Task, you can define few params:</p> <pre><code>class TaskArgs(BaseModel):\n    \"\"\"Holds the configurations specified by user\"\"\"\n\n    extra_tool_calls: int = 0\n    prompt_detail: Literal[\"brief\", \"descriptive\"] = \"brief\"\n    examples_in_system_prompt: Literal[0, 2, 5] = 0\n</code></pre> <ul> <li> <p>examples_in_system_prompt - How many examples there are in system prompts, example:</p> <ul> <li><code>0</code>: <code>You are a ROS 2 expert that want to solve tasks. You have access to various tools that allow you to query the ROS 2 system. Be proactive and use the tools to answer questions.</code></li> <li><code>2</code>: <code>You are a ROS 2 expert that want to solve tasks. You have access to various tools that allow you to query the ROS 2 system. Be proactive and use the tools to answer questions. Example of tool calls: get_ros2_message_interface, args: {'msg_type': 'geometry_msgs/msg/Twist'} publish_ros2_message, args: {'topic': '/cmd_vel', 'message_type': 'geometry_msgs/msg/Twist', 'message': {linear: {x: 0.5, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.0}}}</code></li> </ul> </li> <li> <p>prompt_detail - How descriptive should the Task prompt be, example:</p> <ul> <li><code>brief</code>: \"Get all camera images\"</li> <li> <p><code>descriptive</code>: \"Get all camera images from all available camera sources in the system.     This includes both RGB color images and depth images.     You can discover what camera topics are available and capture images from each.\"</p> <p>Descriptive prompts provides guidance and tips.</p> </li> </ul> </li> <li> <p>extra_tool_calls - How many extra tool calls an agent can make and still pass the Task, example:</p> <ul> <li><code>GetROS2RGBCameraTask</code> has 1 required tool call and 1 optional. When <code>extra_tool_calls</code> set to 5, agent can correct himself couple times and still pass even with 7 tool calls. There can be 2 types of invalid tool calls, first when the tool is used incorrectly and agent receives an error - this allows him to correct himself easier. Second type is when tool is called properly but it is not the tool that should be called or it is called with wrong params. In this case agent won't get any error so it will be harder for him to correct, but BOTH of these cases are counted as <code>extra tool call</code>.</li> </ul> </li> </ul> <p>If you want to know details about every task, visit <code>rai_bench/tool_calling_agent/tasks</code></p>"},{"location":"simulation_and_benchmarking/rai_bench/#vlm-benchmark","title":"VLM Benchmark","text":"<p>The VLM Benchmark is a benchmark for VLM models. It includes a set of tasks containing questions related to images and evaluates the performance of the agent that returns the answer in the structured format.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#running","title":"Running","text":"<p>To run the benchmark:</p> <pre><code>cd rai\nsource setup_shell.sh\npython src/rai_bench/rai_bench/examples/vlm_benchmark.py --model-name gemma3:4b --vendor ollama\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_sim/","title":"RAI Sim","text":"<p>RAI Sim is a package that provides an interface for connecting with various simulation environments. It is designed to be simulator-agnostic, allowing RAI to work with any simulation environment that implements the required interface.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#core-components","title":"Core Components","text":""},{"location":"simulation_and_benchmarking/rai_sim/#simulationbridge","title":"SimulationBridge","text":"<p>The <code>SimulationBridge</code> is an abstract base class that defines the interface for communicating with different simulation environments. It provides the following key functionalities:</p> <ul> <li>Scene setup and management</li> <li>Entity spawning and despawning</li> <li>Object pose retrieval</li> <li>Scene state monitoring</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#sceneconfig","title":"SceneConfig","text":"<p>The <code>SceneConfig</code> is a configuration class that specifies the entities to be spawned in the simulation.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#simulationconfig","title":"SimulationConfig","text":"<p>The <code>SimulationConfig</code> is an abstract configuration class. Each simulation bridge can extend this with additional parameters specific to its implementation.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#scenestate","title":"SceneState","text":"<p>The <code>SceneState</code> class maintains information about the current state of the simulation scene, including:</p> <ul> <li>List of currently spawned entities</li> <li>Current poses of all entities</li> <li>Entity tracking and management</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#implementation-details","title":"Implementation Details","text":""},{"location":"simulation_and_benchmarking/rai_sim/#entity-management","title":"Entity Management","text":"<p>The package provides two main entity classes:</p> <ul> <li><code>Entity</code>: Represents an entity that can be spawned in the simulation</li> <li><code>SpawnedEntity</code>: Represents an entity that has been successfully spawned</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#tools","title":"Tools","text":"<p>RAI Sim includes utility tools for working with simulations:</p> <ul> <li><code>GetObjectPositionsGroundTruthTool</code>: Retrieves accurate positional data for objects in the simulation</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#usage","title":"Usage","text":"<p>To use RAI Sim with a specific simulation environment:</p> <ol> <li>Create a custom <code>SimulationBridge</code> implementation for your simulator</li> <li>Extend <code>SimulationConfig</code> with simulator-specific parameters</li> <li>Implement the required abstract methods:<ul> <li><code>init_simulation</code></li> <li><code>setup_scene</code></li> <li><code>_spawn_entity</code></li> <li><code>_despawn_entity</code></li> <li><code>get_object_pose</code></li> <li><code>get_scene_state</code></li> </ul> </li> </ol>"},{"location":"simulation_and_benchmarking/rai_sim/#configuration","title":"Configuration","text":"<p>Simulation configurations are typically loaded from YAML files with the following structure:</p> <pre><code>frame_id: &lt;reference_frame&gt;\nentities:\n    - name: &lt;unique_entity_name&gt;\n      prefab_name: &lt;resource_name&gt;\n      pose:\n          translation:\n              x: &lt;x_coordinate&gt;\n              y: &lt;y_coordinate&gt;\n              z: &lt;z_coordinate&gt;\n          rotation:\n              x: &lt;x_rotation&gt;\n              y: &lt;y_rotation&gt;\n              z: &lt;z_rotation&gt;\n              w: &lt;w_rotation&gt;\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_sim/#error-handling","title":"Error Handling","text":"<p>The package includes comprehensive error handling for:</p> <ul> <li>Duplicate entity names</li> <li>Failed entity spawning/despawning</li> <li>Invalid configurations</li> <li>Simulation process management</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#integration-with-rai-bench","title":"Integration with RAI Bench","text":"<p>RAI Sim serves as the foundation for RAI Bench by providing:</p> <ul> <li>A consistent interface for all simulation environments</li> <li>Entity management and tracking</li> <li>Scene state monitoring</li> <li>Configuration management</li> </ul> <p>This allows RAI Bench to focus on task definition and evaluation while remaining simulator-agnostic.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#launchmanager","title":"LaunchManager","text":"<p>RAI Sim also provides a ROS2LaunchManager class that manages the start and shutdown of ROS 2<code>LaunchDescription</code></p> ROS2LaunchManager class definition"},{"location":"simulation_and_benchmarking/rai_sim/#rai_sim.launch_manager.ROS2LaunchManager","title":"<code>rai_sim.launch_manager.ROS2LaunchManager</code>","text":"Source code in <code>rai_sim/launch_manager.py</code> <pre><code>class ROS2LaunchManager:\n    def __init__(self) -&gt; None:\n        self._stop_event: Optional[Event] = None\n        self._process: Optional[multiprocessing.Process] = None\n\n    def start(self, launch_description: LaunchDescription) -&gt; None:\n        self._stop_event = multiprocessing.Event()\n        self._process = multiprocessing.Process(\n            target=self._run_process,\n            args=(self._stop_event, launch_description),\n            daemon=True,\n        )\n        self._process.start()\n\n    def shutdown(self) -&gt; None:\n        if self._stop_event:\n            self._stop_event.set()\n        if self._process:\n            self._process.join()\n\n    def _run_process(\n        self, stop_event: Event, launch_description: LaunchDescription\n    ) -&gt; None:\n        loop = asyncio.get_event_loop()\n        asyncio.set_event_loop(loop)\n        launch_service = LaunchService()\n        launch_service.include_launch_description(launch_description)\n        # launch description launched\n        launch_task = loop.create_task(launch_service.run_async())\n        # when stop event set\n        loop.run_until_complete(loop.run_in_executor(None, stop_event.wait))\n        if not launch_task.done():\n            # XXX (jmatejcz) the shutdown function sends shutdown signal to all\n            # nodes launch with launch description which should do the trick\n            # but some nodes are stubborn and there is a possibility\n            # that they don't close. If this will happen sending PKILL for all\n            # ros nodes will be needed\n            shutdown_task = loop.create_task(\n                launch_service.shutdown(),\n            )\n            # shutdown task should complete when all nodes are closed\n            # but wait also for launch task to close just to be sure\n            loop.run_until_complete(asyncio.gather(shutdown_task, launch_task))\n</code></pre>"},{"location":"simulation_and_benchmarking/simulators/","title":"Simulators","text":"<p>TBD</p>"},{"location":"speech_to_speech/overview/","title":"Speech To Speech","text":""},{"location":"speech_to_speech/overview/#introduction","title":"Introduction","text":"<p><code>rai s2s</code> provides tools and components for voice interaction with the system. This package contains plug-and-play Agents which can be easily integrated with Agents provided by <code>rai core</code>, as well as custom ones. It also provides integration with host sound system, which can be used for low level sound manipulation.</p>"},{"location":"speech_to_speech/overview/#core-components","title":"Core Components","text":"Component Description Agents Agents in <code>rai s2s</code> provide functionality for voice interaction with the rest of the system. Models <code>rai s2s</code> provides a models which can be optionally installed and utilized by the Agents. Connector The <code>sounddevice</code> connector allows for interfacing directly with sound devices for asynchronous sound IO."},{"location":"speech_to_speech/overview/#best-practices","title":"Best Practices","text":"<p>When utilizing S2S features:</p> <ol> <li>Deployment of <code>SpeechToSpeechAgent</code> is meant for local setup, while the <code>SpeechRecognition</code> and <code>TextToSpeech</code> Agents are meant to be ran on separate hosts.</li> <li>Note that <code>sounddevice</code> python API has notable issues in multi-threaded environment - this can lead to issues when developing Agents using the <code>SoundDeviceConnector</code></li> </ol>"},{"location":"speech_to_speech/sounddevice/","title":"Sound Device Connector","text":"<p>The <code>SoundDeviceConnector</code> provides a Human-Robot Interface (HRI) for audio streaming, playback, and recording using sound devices. It is designed for seamless integration with RAI agents and tools requiring audio input/output, and conforms to the generic <code>HRIConnector</code> interface.</p> Connector Description Example Usage <code>SoundDeviceConnector</code> Audio streaming, playback, and recording via sounddevice. Implements HRIConnector for audio. <code>connector = SoundDeviceConnector(...)</code>"},{"location":"speech_to_speech/sounddevice/#key-features","title":"Key Features","text":"<ul> <li>Audio playback (write) and recording (read) with flexible device configuration</li> <li>Asynchronous (streaming) and synchronous (service call) audio operations</li> <li>Thread-safe device management and clean shutdown</li> <li>Unified message type (<code>SoundDeviceMessage</code>) for audio and control</li> <li>Full support for the HRIConnector interface</li> </ul>"},{"location":"speech_to_speech/sounddevice/#initialization","title":"Initialization","text":"<p>To use the connector, specify the target (output) and source (input) devices with their configurations:</p> <pre><code>from rai_s2s.sound_device import SoundDeviceConfig, SoundDeviceConnector\n\n# Example device configurations\noutput_config = SoundDeviceConfig(device_name=\"Speaker\", channels=1)\ninput_config = SoundDeviceConfig(device_name=\"Microphone\", channels=1)\n\nconnector = SoundDeviceConnector(\n    targets=[(\"speaker\", output_config)],\n    sources=[(\"mic\", input_config)],\n)\n</code></pre> <p>Tip</p> <p>If you're experiencing audio issues and device_name is set to 'default', try specifying the exact device name instead, as this often resolves the problem.</p>"},{"location":"speech_to_speech/sounddevice/#message-type-sounddevicemessage","title":"Message Type: <code>SoundDeviceMessage</code>","text":"<pre><code>from rai_s2s.sound_device import SoundDeviceMessage\n\nmsg = SoundDeviceMessage(\n    audios=[audio_data],   # List of audio data (bytes or numpy arrays)\n    read=False,            # Set True for recording\n    stop=False,            # Set True to stop playback/recording\n    duration=2.0           # Recording duration (seconds), if applicable\n)\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#example-usage","title":"Example Usage","text":""},{"location":"speech_to_speech/sounddevice/#audio-playback-synchronous","title":"Audio Playback (Synchronous)","text":"<pre><code># Play audio synchronously\nmsg = SoundDeviceMessage(audios=[audio_data])\nconnector.send_message(msg, target=\"speaker\")\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#audio-recording-synchronous","title":"Audio Recording (Synchronous)","text":"<pre><code># Record audio synchronously (blocking)\nmsg = SoundDeviceMessage(read=True)\nrecorded_msg = connector.service_call(msg, target=\"mic\", duration=2.0)\nrecorded_audio = recorded_msg.audios[0]\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#audio-streaming-asynchronous","title":"Audio Streaming (Asynchronous)","text":"<pre><code># Start asynchronous audio recording\nmsg = SoundDeviceMessage(read=True)\ndef on_feedback(audio_chunk):\n    print(\"Received chunk\", audio_chunk)\ndef on_done(final_audio):\n    print(\"Recording finished\")\naction_handle = connector.start_action(\n    action_data=msg,\n    target=\"mic\",\n    on_feedback=on_feedback,\n    on_done=on_done\n)\n\n# Stop the stream when done\nconnector.terminate_action(action_handle)\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#device-management","title":"Device Management","text":"<ul> <li>Configure devices at initialization or using <code>configure_device(target, config)</code>.</li> <li>Retrieve audio parameters using <code>get_audio_params(target)</code>.</li> <li>All devices are managed in a thread-safe way and are properly closed on <code>shutdown()</code>.</li> </ul>"},{"location":"speech_to_speech/sounddevice/#error-handling","title":"Error Handling","text":"<ul> <li>All methods raise <code>SoundDeviceError</code> on invalid operations (e.g., unsupported message types, missing audio data).</li> <li>Use <code>send_message</code> with <code>stop=True</code> to stop playback or recording.</li> <li><code>receive_message</code> is not supported (use actions or service calls for recording).</li> </ul>"},{"location":"speech_to_speech/sounddevice/#see-also","title":"See Also","text":"<ul> <li>Connectors Overview</li> <li>Agents</li> </ul>"},{"location":"speech_to_speech/agents/asr/","title":"SpeechRecognitionAgent","text":""},{"location":"speech_to_speech/agents/asr/#overview","title":"Overview","text":"<p>The <code>SpeechRecognitionAgent</code> in the RAI framework is a specialized agent that performs voice activity detection (VAD), audio recording, and transcription. It integrates tightly with audio input sources and ROS2 messaging, allowing it to serve as a real-time voice interface for robotic systems.</p> <p>This agent manages multiple pipelines for detecting when to start and stop recording, performs transcription using configurable models, and broadcasts messages to relevant ROS2 topics.</p>"},{"location":"speech_to_speech/agents/asr/#class-definition","title":"Class Definition","text":"SpeechRecognitionAgent class definition"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent","title":"<code>rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>Agent responsible for voice recognition, transcription, and processing voice activity.</p> <p>Parameters:</p> Name Type Description Default <code>microphone_config</code> <code>SoundDeviceConfig</code> <p>Configuration for the microphone device used for audio input.</p> required <code>ros2_name</code> <code>str</code> <p>Name of the ROS2 node.</p> required <code>transcription_model</code> <code>BaseTranscriptionModel</code> <p>Model used for transcribing audio input to text.</p> required <code>vad</code> <code>BaseVoiceDetectionModel</code> <p>Voice activity detection model used to determine when speech is present.</p> required <code>grace_period</code> <code>float</code> <p>Time in seconds to wait before stopping recording after speech ends, by default 1.0.</p> <code>1.0</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance for logging messages, by default None.</p> <code>None</code> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>class SpeechRecognitionAgent(BaseAgent):\n    \"\"\"\n    Agent responsible for voice recognition, transcription, and processing voice activity.\n\n    Parameters\n    ----------\n    microphone_config : SoundDeviceConfig\n        Configuration for the microphone device used for audio input.\n    ros2_name : str\n        Name of the ROS2 node.\n    transcription_model : BaseTranscriptionModel\n        Model used for transcribing audio input to text.\n    vad : BaseVoiceDetectionModel\n        Voice activity detection model used to determine when speech is present.\n    grace_period : float, optional\n        Time in seconds to wait before stopping recording after speech ends, by default 1.0.\n    logger : Optional[logging.Logger], optional\n        Logger instance for logging messages, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        microphone_config: SoundDeviceConfig,\n        ros2_name: str,\n        transcription_model: BaseTranscriptionModel,\n        vad: BaseVoiceDetectionModel,\n        grace_period: float = 1.0,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if logger is None:\n            self.logger = logging.getLogger(__name__)\n        else:\n            self.logger = logger\n        self.microphone = SoundDeviceConnector(\n            targets=[], sources=[(\"microphone\", microphone_config)]\n        )\n        self.ros2_hri_connector = ROS2HRIConnector(ros2_name)\n        self.ros2_connector = ROS2Connector(ros2_name + \"ari\")\n        super().__init__()\n        self.should_record_pipeline: List[BaseVoiceDetectionModel] = []\n        self.should_stop_pipeline: List[BaseVoiceDetectionModel] = []\n\n        self.transcription_model = transcription_model\n        self.transcription_lock = Lock()\n\n        self.vad: BaseVoiceDetectionModel = vad\n\n        self.grace_period = grace_period\n        self.grace_period_start = 0\n\n        self.recording_started = False\n        self.ran_setup = False\n\n        self.sample_buffer = []\n        self.sample_buffer_lock = Lock()\n        self.active_thread = \"\"\n        self.transcription_threads: dict[str, ThreadData] = {}\n        self.transcription_buffers: dict[str, list[NDArray]] = {}\n        self.is_playing = True\n\n    @classmethod\n    def from_config(cls, cfg_path: Optional[str] = None) -&gt; Self:\n        cfg = load_config(cfg_path)\n        microphone_configuration = SoundDeviceConfig(\n            stream=True,\n            channels=1,\n            device_name=cfg.microphone.device_name,\n            block_size=1280,\n            consumer_sampling_rate=16000,\n            dtype=\"int16\",\n            device_number=None,\n            is_input=True,\n            is_output=False,\n        )\n        match cfg.transcribe.model_type:\n            case \"LocalWhisper (Free)\":\n                from rai_s2s.asr.models import LocalWhisper\n\n                model = LocalWhisper(\n                    cfg.transcribe.model_name, 16000, language=cfg.transcribe.language\n                )\n            case \"FasterWhisper (Free)\":\n                from rai_s2s.asr.models import FasterWhisper\n\n                model = FasterWhisper(\n                    cfg.transcribe.model_name, 16000, language=cfg.transcribe.language\n                )\n            case \"OpenAI (Cloud)\":\n                from rai_s2s.asr.models import OpenAIWhisper\n\n                model = OpenAIWhisper(\n                    cfg.transcribe.model_name, 16000, language=cfg.transcribe.language\n                )\n            case _:\n                raise ValueError(f\"Unknown model name f{cfg.transcribe.model_name}\")\n\n        match cfg.voice_activity_detection.model_name:\n            case \"SileroVAD\":\n                from rai_s2s.asr.models import SileroVAD\n\n                vad = SileroVAD(16000, cfg.voice_activity_detection.threshold)\n\n        agent = cls(microphone_configuration, \"rai_auto_asr_agent\", model, vad)\n        if cfg.wakeword.is_used:\n            match cfg.wakeword.model_type:\n                case \"OpenWakeWord\":\n                    from rai_s2s.asr.models import OpenWakeWord\n\n                    agent.add_detection_model(\n                        OpenWakeWord(cfg.wakeword.model_name, cfg.wakeword.threshold)\n                    )\n        return agent\n\n    def __call__(self):\n        self.run()\n\n    def add_detection_model(\n        self, model: BaseVoiceDetectionModel, pipeline: str = \"record\"\n    ):\n        \"\"\"\n        Add a voice detection model to the specified processing pipeline.\n\n        Parameters\n        ----------\n        model : BaseVoiceDetectionModel\n            The voice detection model to be added.\n        pipeline : str, optional\n            The pipeline where the model should be added, either 'record' or 'stop'.\n            Default is 'record'.\n\n        Raises\n        ------\n        ValueError\n            If the specified pipeline is not 'record' or 'stop'.\n        \"\"\"\n\n        if pipeline == \"record\":\n            self.should_record_pipeline.append(model)\n        elif pipeline == \"stop\":\n            self.should_stop_pipeline.append(model)\n        else:\n            raise ValueError(\"Pipeline should be either 'record' or 'stop'\")\n\n    def run(self):\n        \"\"\"\n        Start the voice recognition agent, initializing the microphone and handling incoming audio samples.\n        \"\"\"\n        self.running = True\n        msg = SoundDeviceMessage(read=True)\n        self.listener_handle = self.microphone.start_action(\n            action_data=msg,\n            target=\"microphone\",\n            on_feedback=self._on_new_sample,\n            on_done=lambda: None,\n        )\n        self.logger.info(\"Started Voice Agent\")\n\n    def stop(self):\n        \"\"\"\n        Clean exit the voice recognition agent, ensuring all transcription threads finish before termination.\n        \"\"\"\n        self.logger.info(\"Stopping Voice Agent\")\n        self.running = False\n        self.microphone.terminate_action(self.listener_handle)\n        self.ros2_hri_connector.shutdown()\n        self.ros2_connector.shutdown()\n        while not all(\n            [thread[\"joined\"] for thread in self.transcription_threads.values()]\n        ):\n            for thread_id in self.transcription_threads:\n                if self.transcription_threads[thread_id][\"event\"].is_set():\n                    self.transcription_threads[thread_id][\"thread\"].join()\n                    self.transcription_threads[thread_id][\"joined\"] = True\n                else:\n                    self.logger.info(\n                        f\"Waiting for transcription of {thread_id} to finish...\"\n                    )\n        self.logger.info(\"Voice agent stopped\")\n\n    def _on_new_sample(self, indata: np.ndarray, status_flags: dict[str, Any]):\n        sample_time = time.time()\n        with self.sample_buffer_lock:\n            self.sample_buffer.append(indata)\n            if not self.recording_started and len(self.sample_buffer) &gt; 5:\n                self.sample_buffer = self.sample_buffer[-5:]\n\n        # attempt to join finished threads:\n        for thread_id in self.transcription_threads:\n            if self.transcription_threads[thread_id][\"event\"].is_set():\n                self.transcription_threads[thread_id][\"thread\"].join()\n                self.transcription_threads[thread_id][\"joined\"] = True\n\n        voice_detected, output_parameters = self.vad(indata, {})\n        self.logger.debug(f\"Voice detected: {voice_detected}: {output_parameters}\")\n        should_record = False\n        if voice_detected and not self.recording_started:\n            should_record = self._should_record(indata, output_parameters)\n\n        if should_record:\n            self.logger.info(\"starting recording...\")\n            self.recording_started = True\n            thread_id = str(uuid4())[0:8]\n            transcription_thread = Thread(\n                target=self._transcription_thread,\n                args=[thread_id],\n            )\n            transcription_finished = Event()\n            self.active_thread = thread_id\n            self.transcription_threads[thread_id] = {\n                \"thread\": transcription_thread,\n                \"event\": transcription_finished,\n                \"transcription\": \"\",\n                \"joined\": False,\n            }\n\n        if voice_detected:\n            self.logger.debug(\"Voice detected... resetting grace period\")\n            self.grace_period_start = sample_time\n            self._send_ros2_message(\"pause\", \"/voice_commands\")\n            self.is_playing = False\n        if (\n            self.recording_started\n            and sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self.logger.info(\n                \"Grace period ended... stopping recording, starting transcription\"\n            )\n            self.recording_started = False\n            self.grace_period_start = 0\n            with self.sample_buffer_lock:\n                self.transcription_buffers[self.active_thread] = self.sample_buffer\n                self.sample_buffer = []\n            self.transcription_threads[self.active_thread][\"thread\"].start()\n            self.active_thread = \"\"\n            self._send_ros2_message(\"stop\", \"/voice_commands\")\n            self.is_playing = False\n        elif not self.is_playing and (\n            sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self._send_ros2_message(\"play\", \"/voice_commands\")\n            self.is_playing = True\n\n    def _should_record(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; bool:\n        if len(self.should_record_pipeline) == 0:\n            return True\n        for model in self.should_record_pipeline:\n            detected, output = model(audio_data, input_parameters)\n            self.logger.debug(f\"detected {detected}, output {output}\")\n            if detected:\n                model.reset()\n                return True\n        return False\n\n    def _transcription_thread(self, identifier: str):\n        self.logger.info(f\"transcription thread {identifier} started\")\n        audio_data = np.concatenate(self.transcription_buffers[identifier])\n\n        # NOTE: this is only necessary for the local model, but it seems to cause no relevant performance drops in case of cloud models\n        with self.transcription_lock:\n            transcription = self.transcription_model.transcribe(audio_data)\n        self._send_ros2_message(transcription, \"/from_human\")\n        self.transcription_threads[identifier][\"transcription\"] = transcription\n        self.transcription_threads[identifier][\"event\"].set()\n\n    def _send_ros2_message(self, data: str, topic: str):\n        self.logger.debug(f\"Sending message to {topic}: {data}\")\n        if topic == \"/voice_commands\":\n            msg = ROS2Message(payload={\"data\": data})\n            try:\n                self.ros2_connector.send_message(\n                    msg, topic, msg_type=\"std_msgs/msg/String\"\n                )\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {topic}: {e}\")\n        else:\n            msg = ROS2HRIMessage(\n                text=data,\n                message_author=\"human\",\n                communication_id=ROS2HRIMessage.generate_conversation_id(),\n            )\n            self.ros2_hri_connector.send_message(msg, topic)\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent.add_detection_model","title":"<code>add_detection_model(model, pipeline='record')</code>","text":"<p>Add a voice detection model to the specified processing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseVoiceDetectionModel</code> <p>The voice detection model to be added.</p> required <code>pipeline</code> <code>str</code> <p>The pipeline where the model should be added, either 'record' or 'stop'. Default is 'record'.</p> <code>'record'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified pipeline is not 'record' or 'stop'.</p> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>def add_detection_model(\n    self, model: BaseVoiceDetectionModel, pipeline: str = \"record\"\n):\n    \"\"\"\n    Add a voice detection model to the specified processing pipeline.\n\n    Parameters\n    ----------\n    model : BaseVoiceDetectionModel\n        The voice detection model to be added.\n    pipeline : str, optional\n        The pipeline where the model should be added, either 'record' or 'stop'.\n        Default is 'record'.\n\n    Raises\n    ------\n    ValueError\n        If the specified pipeline is not 'record' or 'stop'.\n    \"\"\"\n\n    if pipeline == \"record\":\n        self.should_record_pipeline.append(model)\n    elif pipeline == \"stop\":\n        self.should_stop_pipeline.append(model)\n    else:\n        raise ValueError(\"Pipeline should be either 'record' or 'stop'\")\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent.run","title":"<code>run()</code>","text":"<p>Start the voice recognition agent, initializing the microphone and handling incoming audio samples.</p> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start the voice recognition agent, initializing the microphone and handling incoming audio samples.\n    \"\"\"\n    self.running = True\n    msg = SoundDeviceMessage(read=True)\n    self.listener_handle = self.microphone.start_action(\n        action_data=msg,\n        target=\"microphone\",\n        on_feedback=self._on_new_sample,\n        on_done=lambda: None,\n    )\n    self.logger.info(\"Started Voice Agent\")\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent.stop","title":"<code>stop()</code>","text":"<p>Clean exit the voice recognition agent, ensuring all transcription threads finish before termination.</p> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Clean exit the voice recognition agent, ensuring all transcription threads finish before termination.\n    \"\"\"\n    self.logger.info(\"Stopping Voice Agent\")\n    self.running = False\n    self.microphone.terminate_action(self.listener_handle)\n    self.ros2_hri_connector.shutdown()\n    self.ros2_connector.shutdown()\n    while not all(\n        [thread[\"joined\"] for thread in self.transcription_threads.values()]\n    ):\n        for thread_id in self.transcription_threads:\n            if self.transcription_threads[thread_id][\"event\"].is_set():\n                self.transcription_threads[thread_id][\"thread\"].join()\n                self.transcription_threads[thread_id][\"joined\"] = True\n            else:\n                self.logger.info(\n                    f\"Waiting for transcription of {thread_id} to finish...\"\n                )\n    self.logger.info(\"Voice agent stopped\")\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#purpose","title":"Purpose","text":"<p>The <code>SpeechRecognitionAgent</code> class enables real-time voice processing with the following responsibilities:</p> <ul> <li>Detecting speech through VAD</li> <li>Managing recording state and grace periods</li> <li>Buffering and threading transcription processes</li> <li>Publishing transcriptions and control messages to ROS2 topics</li> <li>Supporting multiple VAD and transcription model types</li> </ul>"},{"location":"speech_to_speech/agents/asr/#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Description <code>microphone_config</code> <code>SoundDeviceConfig</code> Configuration for the microphone input. <code>ros2_name</code> <code>str</code> Name of the ROS2 node. <code>transcription_model</code> <code>BaseTranscriptionModel</code> Model instance for transcribing speech. <code>vad</code> <code>BaseVoiceDetectionModel</code> Model for detecting voice activity. <code>grace_period</code> <code>float</code> Time (in seconds) to continue buffering after speech ends. Defaults to <code>1.0</code>. <code>logger</code> <code>Optional[logging.Logger]</code> Logger instance. If <code>None</code>, defaults to module logger."},{"location":"speech_to_speech/agents/asr/#key-methods","title":"Key Methods","text":""},{"location":"speech_to_speech/agents/asr/#from_config","title":"<code>from_config()</code>","text":"<p>Creates a <code>SpeechRecognitionAgent</code> instance from a YAML config file. Dynamically loads the required transcription and VAD models.</p>"},{"location":"speech_to_speech/agents/asr/#run","title":"<code>run()</code>","text":"<p>Starts the microphone stream and handles incoming audio samples.</p>"},{"location":"speech_to_speech/agents/asr/#stop","title":"<code>stop()</code>","text":"<p>Stops the agent gracefully, joins all running transcription threads, and shuts down ROS2 connectors.</p>"},{"location":"speech_to_speech/agents/asr/#add_detection_modelmodel-pipelinerecord","title":"<code>add_detection_model(model, pipeline=\"record\")</code>","text":"<p>Adds a custom VAD model to a processing pipeline.</p> <ul> <li><code>pipeline</code> can be either <code>'record'</code> or <code>'stop'</code></li> </ul> <p><code>'stop'</code> pipeline</p> <p>The <code>'stop'</code> pipeline is present for forward compatibility. It currently doesn't affect Agent's functioning.</p>"},{"location":"speech_to_speech/agents/asr/#best-practices","title":"Best Practices","text":"<ol> <li>Graceful Shutdown: Always call <code>stop()</code> to ensure transcription threads complete.</li> <li>Model Compatibility: Ensure all transcription and VAD models are compatible with the sample rate (typically 16 kHz).</li> <li>Thread Safety: Use provided locks for shared state, especially around the transcription model.</li> <li>Logging: Utilize <code>self.logger</code> for debug and info logs to aid in tracing activity.</li> <li>Config-driven Design: Use <code>from_config()</code> to ensure modular and portable deployment.</li> </ol>"},{"location":"speech_to_speech/agents/asr/#architecture","title":"Architecture","text":"<p>The <code>SpeechRecognitionAgent</code> typically interacts with the following components:</p> <ul> <li>SoundDeviceConnector: Interfaces with microphone audio input.</li> <li>BaseVoiceDetectionModel: Determines whether speech is present.</li> <li>BaseTranscriptionModel: Converts speech audio into text.</li> <li>ROS2Connector / ROS2HRIConnector: Publishes transcription and control messages to ROS2 topics.</li> <li>Config Loader: Dynamically creates agent from structured config files.</li> </ul>"},{"location":"speech_to_speech/agents/asr/#see-also","title":"See Also","text":"<ul> <li>BaseAgent: Abstract agent class providing lifecycle and logging support.</li> <li>ROS2 Connectors: Communication layer for ROS2 topics.</li> <li>Models: For available voice based models and instructions for creating new ones.</li> <li>TextToSpeech: For TextToSpeechAgent meant for distributed deployment.</li> </ul>"},{"location":"speech_to_speech/agents/overview/","title":"S2S Agents","text":""},{"location":"speech_to_speech/agents/overview/#overview","title":"Overview","text":"<p>Agents in RAI are modular components that encapsulate specific functionalities and behaviors. They follow a consistent interface defined by the <code>BaseAgent</code> class and can be combined to create complex robotic systems. The Speech to Speech Agents are used for voice-based interaction, and communicate with other agents.</p>"},{"location":"speech_to_speech/agents/overview/#speechtospeechagent","title":"SpeechToSpeechAgent","text":"<p><code>SpeechToSpeechAgent</code> is the abstract base class for locally deployable S2S Agents. It provides functionality to manage sound device integration, as well as defines the communication schema for integration with the rest of the system.</p>"},{"location":"speech_to_speech/agents/overview/#class-definition","title":"Class Definition","text":"SpeechToSpeechAgent class definition"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent","title":"<code>rai_s2s.s2s.agents.SpeechToSpeechAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>class SpeechToSpeechAgent(BaseAgent):\n    def __init__(\n        self,\n        from_human_topic: str,\n        to_human_topic: str,\n        *,\n        microphone_config: SoundDeviceConfig,\n        speaker_config: SoundDeviceConfig,\n        transcription_model: BaseTranscriptionModel,\n        vad: BaseVoiceDetectionModel,\n        tts: TTSModel,\n        grace_period: float = 1.0,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        super().__init__()\n        if logger is not None:\n            self.logger = logger\n        self.sound_connector = SoundDeviceConnector(\n            targets=[(\"speaker\", speaker_config)],\n            sources=[(\"microphone\", microphone_config)],\n        )\n\n        self.from_human_topic = from_human_topic\n        self.to_human_topic = to_human_topic\n\n        sample_rate, _, out_channels = self.sound_connector.get_audio_params(\"speaker\")\n        tts.sample_rate = sample_rate\n        tts.channels = out_channels\n\n        self.playback_data = PlayData()\n\n        self.should_record_pipeline: List[BaseVoiceDetectionModel] = []\n        self.should_stop_pipeline: List[BaseVoiceDetectionModel] = []\n\n        self.current_transcription_id = str(uuid4())[0:8]\n        self.current_speech_id = None\n        self.text_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n\n        self.terminate_agent = Event()\n\n        self.audio_generating_thread = Thread(target=self._audio_gen_thread)\n        self.audio_generating_thread.start()\n        self.audio_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n        self.remembered_speech_ids: list[str] = []\n\n        self.tts_model = tts\n\n        self.transcription_model = transcription_model\n\n        self.vad: BaseVoiceDetectionModel = vad\n        self.grace_period = grace_period\n        self.grace_period_start = 0\n\n        self.sample_buffer = []\n        self.sample_buffer_lock = Lock()\n        self.transcription_lock = Lock()\n        self.active_thread = \"\"\n        self.transcription_threads: dict[str, ThreadData] = {}\n        self.transcription_buffers: dict[str, list[NDArray]] = {}\n        self.is_playing = False\n\n        self.recording_started = False\n        # self.ran_setup = False\n\n        self.hri_connector: HRIConnector = self._setup_hri_connector()\n\n        self.microphone_samples: Optional[np.ndarray] = None\n        self.save_flag = False\n\n    @abstractmethod\n    def _setup_hri_connector(self) -&gt; HRIConnector: ...\n\n    def _audio_gen_thread(self):\n        while not self.terminate_agent.wait(timeout=0.01):\n            if self.current_transcription_id in self.text_queues:\n                try:\n                    data = self.text_queues[self.current_transcription_id].get(\n                        block=False\n                    )\n                except Empty:\n                    continue\n                audio = self.tts_model.get_speech(data)\n                try:\n                    self.audio_queues[self.current_transcription_id].put(audio)\n                except KeyError as e:\n                    self.logger.error(\n                        f\"Could not find queue for {self.current_transcription_id}: queuse: {self.audio_queues.keys()}\"\n                    )\n                    raise e\n\n    def run(self):\n        \"\"\"\n        Start the text-to-speech agent, initializing playback and launching the transcription thread.\n        \"\"\"\n        self.running = True\n        self.logger.info(\"Starting SpeechToSpeechAgent...\")\n\n        msg = SoundDeviceMessage(read=False)\n        self.player_handle = self.sound_connector.start_action(\n            action_data=msg,\n            target=\"speaker\",\n            on_feedback=self._speaker_callback,\n            on_done=lambda: None,\n        )\n        msg = SoundDeviceMessage(read=True)\n        self.listener_handle = self.sound_connector.start_action(\n            action_data=msg,\n            target=\"microphone\",\n            on_feedback=self._on_microphone_sample,\n            on_done=lambda: None,\n        )\n        self.logger.info(\"SpeechToSpeechAgent Started!\")\n\n    def _speaker_callback(self, outdata, frames, time, status_dict):\n        set_flags = [flag for flag, status in status_dict.items() if status]\n\n        if set_flags:\n            self.logger.warning(\"Flags set:\" + \", \".join(set_flags))\n        if self.playback_data.playing:\n            if self.playback_data.current_segment is None:\n                try:\n                    self.playback_data.current_segment = self.audio_queues[\n                        self.current_transcription_id\n                    ].get(block=False)\n                    self.playback_data.data = np.array(\n                        self.playback_data.current_segment.get_array_of_samples()  # type: ignore\n                    ).reshape(-1, self.playback_data.channels)\n                except Empty:\n                    pass\n                except KeyError:\n                    pass\n            if self.playback_data.data is not None:\n                current_frame = self.playback_data.current_frame\n                chunksize = min(len(self.playback_data.data) - current_frame, frames)\n                outdata[:chunksize] = self.playback_data.data[\n                    current_frame : current_frame + chunksize\n                ]\n                if chunksize &lt; frames:\n                    outdata[chunksize:] = 0\n                    self.playback_data.current_frame = 0\n                    self.playback_data.current_segment = None\n                    self.playback_data.data = None\n                else:\n                    self.playback_data.current_frame += chunksize\n\n    def _on_microphone_sample(self, indata: np.ndarray, status_flags: dict[str, Any]):\n        sample_time = time.time()\n        with self.sample_buffer_lock:\n            self.sample_buffer.append(indata)\n            if not self.recording_started and len(self.sample_buffer) &gt; 5:\n                self.sample_buffer = self.sample_buffer[-5:]\n\n        # attempt to join finished threads:\n        for thread_id in self.transcription_threads:\n            if self.transcription_threads[thread_id][\"event\"].is_set():\n                self.transcription_threads[thread_id][\"thread\"].join()\n                self.transcription_threads[thread_id][\"joined\"] = True\n\n        voice_detected, output_parameters = self.vad(indata, {})\n        self.logger.debug(f\"Voice detected: {voice_detected}: {output_parameters}\")\n        should_record = False\n        if voice_detected and not self.recording_started:\n            should_record = self._should_record(indata, output_parameters)\n\n        if should_record:\n            self.logger.info(\"starting recording...\")\n            self.recording_started = True\n            thread_id = str(uuid4())[0:8]\n            transcription_thread = Thread(\n                target=self._transcription_thread,\n                args=[thread_id],\n            )\n            transcription_finished = Event()\n            self.active_thread = thread_id\n            self.transcription_threads[thread_id] = {\n                \"thread\": transcription_thread,\n                \"event\": transcription_finished,\n                \"transcription\": \"\",\n                \"joined\": False,\n            }\n\n        if voice_detected:\n            self.logger.debug(\"Voice detected... resetting grace period\")\n            self.grace_period_start = sample_time\n            self.set_playback_state(\"pause\")\n            self.is_playing = False\n        if (\n            self.recording_started\n            and sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self.logger.info(\n                \"Grace period ended... stopping recording, starting transcription\"\n            )\n            self.recording_started = False\n            self.grace_period_start = 0\n            with self.sample_buffer_lock:\n                self.transcription_buffers[self.active_thread] = self.sample_buffer\n                self.sample_buffer = []\n            self.transcription_threads[self.active_thread][\"thread\"].start()\n            self.active_thread = \"\"\n            self.set_playback_state(\"stop\")\n            self.is_playing = False\n        elif not self.is_playing and (\n            sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self.set_playback_state(\"play\")\n            self.is_playing = True\n\n    def _should_record(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; bool:\n        if len(self.should_record_pipeline) == 0:\n            return True\n        for model in self.should_record_pipeline:\n            detected, output = model(audio_data, input_parameters)\n            self.logger.debug(f\"detected {detected}, output {output}\")\n            if detected:\n                model.reset()\n                return True\n        return False\n\n    def _transcription_thread(self, identifier: str):\n        self.logger.info(f\"transcription thread {identifier} started\")\n        audio_data = np.concatenate(self.transcription_buffers[identifier])\n        with (\n            self.transcription_lock\n        ):  # this is only necessary for the local model... TODO: fix this somehow\n            transcription = self.transcription_model.transcribe(audio_data)\n        self._send_from_human_message(transcription)\n        self.transcription_threads[identifier][\"transcription\"] = transcription\n        self.transcription_threads[identifier][\"event\"].set()\n\n    @abstractmethod\n    def _send_from_human_message(self, data: str): ...\n\n    def _on_to_human_message(self, message: HRIMessage):\n        self.logger.info(f\"Receieved message from human: {message.text}\")\n        self.logger.warning(\n            f\"Starting playback, current id: {self.current_transcription_id}\"\n        )\n        if (\n            self.current_speech_id is None\n            and message.communication_id is not None\n            and message.communication_id not in self.remembered_speech_ids\n        ):\n            self.current_speech_id = message.communication_id\n            self.remembered_speech_ids.append(self.current_speech_id)\n            if len(self.remembered_speech_ids) &gt; 64:\n                self.remembered_speech_ids.pop(0)\n        if self.current_speech_id == message.communication_id:\n            self.text_queues[self.current_transcription_id].put(message.text)\n        self.playback_data.playing = True\n\n    def add_detection_model(self, model: BaseVoiceDetectionModel):\n        \"\"\"\n        Add a voice detection model to check before recording starts.\n\n        Parameters\n        ----------\n        model : BaseVoiceDetectionModel\n            The voice detection model to be added.\n        \"\"\"\n\n        self.should_record_pipeline.append(model)\n\n    def set_playback_state(self, state: Literal[\"play\", \"pause\", \"stop\"]):\n        \"\"\"\n        Set the playback state of the system.\n\n        Parameters\n        ----------\n        state : {\"play\", \"pause\", \"stop\"}\n            The desired playback state:\n            - \"play\": Start or resume playback.\n            - \"pause\": Pause the current playback.\n            - \"stop\": Stop playback and reset playback-related data and queues.\n\n        Notes\n        -----\n        - When state is \"stop\", this method:\n          - Resets the `current_speech_id`.\n          - Generates a new `current_transcription_id`.\n          - Initializes new audio and text queues.\n          - Clears previous playback data.\n        - Logs actions and transitions for debugging and monitoring purposes.\n        \"\"\"\n        if state == \"play\":\n            self.playback_data.playing = True\n        elif state == \"pause\":\n            self.playback_data.playing = False\n        elif state == \"stop\":\n            self.current_speech_id = None\n            self.playback_data.playing = False\n            previous_id = self.current_transcription_id\n            self.logger.warning(f\"Stopping playback, previous id: {previous_id}\")\n            self.current_transcription_id = str(uuid4())[0:8]\n            self.audio_queues[self.current_transcription_id] = Queue()\n            self.text_queues[self.current_transcription_id] = Queue()\n            try:\n                del self.audio_queues[previous_id]\n                del self.text_queues[previous_id]\n            except KeyError:\n                pass\n            self.playback_data.data = None\n            self.playback_data.current_frame = 0\n            self.playback_data.current_segment = None\n\n        self.logger.debug(f\"Current status is: {self.playback_data.playing}\")\n\n    def stop(self):\n        \"\"\"\n        Clean exit the speech-to-speech agent, terminating playback and joining the transcription thread.\n        \"\"\"\n        self.sound_connector.shutdown()\n\n        self.logger.info(\"Stopping TextToSpeechAgent\")\n        self.terminate_agent.set()\n        if self.audio_generating_thread is not None:\n            self.audio_generating_thread.join()\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.add_detection_model","title":"<code>add_detection_model(model)</code>","text":"<p>Add a voice detection model to check before recording starts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseVoiceDetectionModel</code> <p>The voice detection model to be added.</p> required Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def add_detection_model(self, model: BaseVoiceDetectionModel):\n    \"\"\"\n    Add a voice detection model to check before recording starts.\n\n    Parameters\n    ----------\n    model : BaseVoiceDetectionModel\n        The voice detection model to be added.\n    \"\"\"\n\n    self.should_record_pipeline.append(model)\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.run","title":"<code>run()</code>","text":"<p>Start the text-to-speech agent, initializing playback and launching the transcription thread.</p> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start the text-to-speech agent, initializing playback and launching the transcription thread.\n    \"\"\"\n    self.running = True\n    self.logger.info(\"Starting SpeechToSpeechAgent...\")\n\n    msg = SoundDeviceMessage(read=False)\n    self.player_handle = self.sound_connector.start_action(\n        action_data=msg,\n        target=\"speaker\",\n        on_feedback=self._speaker_callback,\n        on_done=lambda: None,\n    )\n    msg = SoundDeviceMessage(read=True)\n    self.listener_handle = self.sound_connector.start_action(\n        action_data=msg,\n        target=\"microphone\",\n        on_feedback=self._on_microphone_sample,\n        on_done=lambda: None,\n    )\n    self.logger.info(\"SpeechToSpeechAgent Started!\")\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.set_playback_state","title":"<code>set_playback_state(state)</code>","text":"<p>Set the playback state of the system.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>(play, pause, stop)</code> <p>The desired playback state: - \"play\": Start or resume playback. - \"pause\": Pause the current playback. - \"stop\": Stop playback and reset playback-related data and queues.</p> <code>\"play\"</code> Notes <ul> <li>When state is \"stop\", this method:</li> <li>Resets the <code>current_speech_id</code>.</li> <li>Generates a new <code>current_transcription_id</code>.</li> <li>Initializes new audio and text queues.</li> <li>Clears previous playback data.</li> <li>Logs actions and transitions for debugging and monitoring purposes.</li> </ul> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def set_playback_state(self, state: Literal[\"play\", \"pause\", \"stop\"]):\n    \"\"\"\n    Set the playback state of the system.\n\n    Parameters\n    ----------\n    state : {\"play\", \"pause\", \"stop\"}\n        The desired playback state:\n        - \"play\": Start or resume playback.\n        - \"pause\": Pause the current playback.\n        - \"stop\": Stop playback and reset playback-related data and queues.\n\n    Notes\n    -----\n    - When state is \"stop\", this method:\n      - Resets the `current_speech_id`.\n      - Generates a new `current_transcription_id`.\n      - Initializes new audio and text queues.\n      - Clears previous playback data.\n    - Logs actions and transitions for debugging and monitoring purposes.\n    \"\"\"\n    if state == \"play\":\n        self.playback_data.playing = True\n    elif state == \"pause\":\n        self.playback_data.playing = False\n    elif state == \"stop\":\n        self.current_speech_id = None\n        self.playback_data.playing = False\n        previous_id = self.current_transcription_id\n        self.logger.warning(f\"Stopping playback, previous id: {previous_id}\")\n        self.current_transcription_id = str(uuid4())[0:8]\n        self.audio_queues[self.current_transcription_id] = Queue()\n        self.text_queues[self.current_transcription_id] = Queue()\n        try:\n            del self.audio_queues[previous_id]\n            del self.text_queues[previous_id]\n        except KeyError:\n            pass\n        self.playback_data.data = None\n        self.playback_data.current_frame = 0\n        self.playback_data.current_segment = None\n\n    self.logger.debug(f\"Current status is: {self.playback_data.playing}\")\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.stop","title":"<code>stop()</code>","text":"<p>Clean exit the speech-to-speech agent, terminating playback and joining the transcription thread.</p> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Clean exit the speech-to-speech agent, terminating playback and joining the transcription thread.\n    \"\"\"\n    self.sound_connector.shutdown()\n\n    self.logger.info(\"Stopping TextToSpeechAgent\")\n    self.terminate_agent.set()\n    if self.audio_generating_thread is not None:\n        self.audio_generating_thread.join()\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#communication","title":"Communication","text":"<p>The Agent communicates through two communication channels provided during initialization - <code>from_human</code> and <code>to_human</code>. On the <code>from_human</code> channel text transcribed from human voice is published. On the <code>to_human</code> channel receives text to be played to the human through text-to-speech.</p>"},{"location":"speech_to_speech/agents/overview/#voice-interaction","title":"Voice interaction","text":"<p>The voice interaction is performed through two audio streams, with two devices. These devices can be different, but don't have to - and in case of most local deployments they will be the same. The list of available sounddevices for configuration can be obtained by running <code>python -c \"import sounddevice as sd; print(sd.query_devices())\"</code>. The configuration requires the user to specify the name of the sound device to be used for interfacing. This is the entire string from the index until the comma before the hostapi (typically <code>ALSA</code> on Ubuntu).</p> <p>The voice interaction works as follows: - The user speaks, which leads to the <code>VoiceActivityDetection</code> model activation. - [Optional] the recording pipeline (containing other models like OpenWakeWord) runs checks. - The recording starts. - The recording continues until the user stops talking (based on silence grace period). - The recording is transcribed and sent to the system. - The Agent receives text data to be played to the user. - The playback begins. - The playback can be interrupted by user speaking: - if there is additional recording pipeline the playback will pause while the user speaks (and continue, if the pipeline returns false). - otherwise the new recording will be send to the system, and transcription will stop the playback.</p>"},{"location":"speech_to_speech/agents/overview/#implementations","title":"Implementations","text":"<p>ROS based implementation is available in <code>ROS2S2SAgent</code>.</p> ROS2S2SAgent class definition"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.ros2s2s_agent.ROS2S2SAgent","title":"<code>rai_s2s.s2s.agents.ros2s2s_agent.ROS2S2SAgent</code>","text":"<p>               Bases: <code>SpeechToSpeechAgent</code></p> Source code in <code>rai_s2s/s2s/agents/ros2s2s_agent.py</code> <pre><code>class ROS2S2SAgent(SpeechToSpeechAgent):\n    def __init__(\n        self,\n        from_human_topic: str,\n        to_human_topic: str,\n        *,\n        microphone_config: SoundDeviceConfig,\n        speaker_config: SoundDeviceConfig,\n        transcription_model: BaseTranscriptionModel,\n        vad: BaseVoiceDetectionModel,\n        tts: TTSModel,\n        grace_period: float = 1,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        super().__init__(\n            from_human_topic,\n            to_human_topic,\n            microphone_config=microphone_config,\n            speaker_config=speaker_config,\n            transcription_model=transcription_model,\n            vad=vad,\n            tts=tts,\n            grace_period=grace_period,\n            logger=logger,\n            **kwargs,\n        )\n\n    def _setup_hri_connector(self):\n        hri_connector = ROS2HRIConnector()\n        hri_connector.register_callback(self.to_human_topic, self._on_to_human_message)\n        return hri_connector\n\n    def _send_from_human_message(self, data: str):\n        print(f\"Sending message to {self.from_human_topic}\")\n        self.hri_connector.send_message(\n            ROS2HRIMessage(text=data), self.from_human_topic\n        )\n\n    def _send_to_human_message(self, data: str):\n        print(f\"Sending message to {self.to_human_topic}\")\n        self.hri_connector.send_message(ROS2HRIMessage(text=data), self.to_human_topic)\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#see-also","title":"See Also","text":"<ul> <li>Models: For available voice based models and instructions for creating new ones.</li> <li>AutomaticSpeechRecognition: For AutomaticSpeechRecognitionAgent meant for distributed deployment.</li> <li>TextToSpeech: For TextToSpeechAgent meant for distributed deployment.</li> </ul>"},{"location":"speech_to_speech/agents/tts/","title":"TextToSpeechAgent","text":""},{"location":"speech_to_speech/agents/tts/#overview","title":"Overview","text":"<p>The <code>TextToSpeechAgent</code> in the RAI framework is a modular agent responsible for converting incoming text into audio using a text-to-speech (TTS) model and playing it through a configured audio output device. It supports real-time playback control through ROS2 messages and handles asynchronous speech processing using threads and queues.</p>"},{"location":"speech_to_speech/agents/tts/#class-definition","title":"Class Definition","text":"TextToSpeechAgent class definition"},{"location":"speech_to_speech/agents/tts/#rai_s2s.tts.agents.TextToSpeechAgent","title":"<code>rai_s2s.tts.agents.TextToSpeechAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>Agent responsible for converting text to speech and handling audio playback.</p> <p>Parameters:</p> Name Type Description Default <code>speaker_config</code> <code>SoundDeviceConfig</code> <p>Configuration for the sound device used for playback.</p> required <code>ros2_name</code> <code>str</code> <p>Name of the ROS2 node.</p> required <code>tts</code> <code>TTSModel</code> <p>Text-to-speech model used for generating audio.</p> required <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance for logging messages, by default None.</p> <code>None</code> <code>max_speech_history</code> <code>int</code> <p>Maximum amount of speech ids to remember, by default 64</p> <code>64</code> Source code in <code>rai_s2s/tts/agents/tts_agent.py</code> <pre><code>class TextToSpeechAgent(BaseAgent):\n    \"\"\"\n    Agent responsible for converting text to speech and handling audio playback.\n\n    Parameters\n    ----------\n    speaker_config : SoundDeviceConfig\n        Configuration for the sound device used for playback.\n    ros2_name : str\n        Name of the ROS2 node.\n    tts : TTSModel\n        Text-to-speech model used for generating audio.\n    logger : Optional[logging.Logger], optional\n        Logger instance for logging messages, by default None.\n    max_speech_history : int, optional\n        Maximum amount of speech ids to remember, by default 64\n    \"\"\"\n\n    def __init__(\n        self,\n        speaker_config: SoundDeviceConfig,\n        ros2_name: str,\n        tts: TTSModel,\n        logger: Optional[logging.Logger] = None,\n        max_speech_history=64,\n    ):\n        if logger is None:\n            self.logger = logging.getLogger(__name__)\n        else:\n            self.logger = logger\n\n        self.speaker = SoundDeviceConnector(\n            targets=[(\"speaker\", speaker_config)], sources=[]\n        )\n        sample_rate, _, out_channels = self.speaker.get_audio_params(\"speaker\")\n        tts.sample_rate = sample_rate\n        tts.channels = out_channels\n\n        self.node_base_name = ros2_name\n        self.model = tts\n        self.ros2_connector = self._setup_ros2_connector()\n        super().__init__()\n\n        self.current_transcription_id = str(uuid4())[0:8]\n        self.current_speech_id = None\n        self.text_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n        self.audio_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n        self.remembered_speech_ids: list[str] = []\n\n        self.tog_play_event = Event()\n        self.stop_event = Event()\n        self.current_audio = None\n\n        self.terminate_agent = Event()\n        self.transcription_thread = None\n        self.running = False\n\n        self.playback_data = PlayData()\n\n    @classmethod\n    def from_config(cls, cfg_path: Optional[str] = None) -&gt; Self:\n        cfg = load_config(cfg_path)\n        config = SoundDeviceConfig(\n            stream=True,\n            is_output=True,\n            device_name=cfg.speaker.device_name,\n        )\n        match cfg.text_to_speech.model_type:\n            case \"ElevenLabs\":\n                from rai_s2s.tts.models import ElevenLabsTTS\n\n                if cfg.text_to_speech.voice != \"\":\n                    model = ElevenLabsTTS(voice=cfg.text_to_speech.voice)\n                else:\n                    raise ValueError(\"ElevenLabs [tts] vendor required voice to be set\")\n            case \"OpenTTS\":\n                from rai_s2s.tts.models import OpenTTS\n\n                if cfg.text_to_speech.voice != \"\":\n                    model = OpenTTS(voice=cfg.text_to_speech.voice)\n                else:\n                    model = OpenTTS()\n            case \"KokoroTTS\":\n                from rai_s2s.tts.models import KokoroTTS\n\n                if cfg.text_to_speech.voice != \"\":\n                    model = KokoroTTS(voice=cfg.text_to_speech.voice)\n                else:\n                    model = KokoroTTS()\n            case _:\n                raise ValueError(f\"Unknown model_type: {cfg.text_to_speech.model_type}\")\n        return cls(config, \"rai_auto_tts\", model)\n\n    def __call__(self):\n        self.run()\n\n    def run(self):\n        \"\"\"\n        Start the text-to-speech agent, initializing playback and launching the transcription thread.\n        \"\"\"\n        self.running = True\n        self.logger.info(\"TextToSpeechAgent started\")\n        self.transcription_thread = Thread(target=self._transcription_thread)\n        self.transcription_thread.start()\n\n        msg = SoundDeviceMessage(read=False)\n        self.speaker.start_action(\n            msg,\n            \"speaker\",\n            on_feedback=self._speaker_callback,\n            on_done=lambda: None,\n        )\n\n    def _speaker_callback(self, outdata, frames, time, status_dict):\n        set_flags = [flag for flag, status in status_dict.items() if status]\n\n        if set_flags:\n            self.logger.warning(\"Flags set:\" + \", \".join(set_flags))\n        if self.playback_data.playing:\n            if self.playback_data.current_segment is None:\n                try:\n                    self.playback_data.current_segment = self.audio_queues[\n                        self.current_transcription_id\n                    ].get(block=False)\n                    self.playback_data.data = np.array(\n                        self.playback_data.current_segment.get_array_of_samples()  # type: ignore\n                    ).reshape(-1, self.playback_data.channels)\n                except Empty:\n                    pass\n                except KeyError:\n                    pass\n            if self.playback_data.data is not None:\n                current_frame = self.playback_data.current_frame\n                chunksize = min(len(self.playback_data.data) - current_frame, frames)\n                outdata[:chunksize] = self.playback_data.data[\n                    current_frame : current_frame + chunksize\n                ]\n                if chunksize &lt; frames:\n                    outdata[chunksize:] = 0\n                    self.playback_data.current_frame = 0\n                    self.playback_data.current_segment = None\n                    self.playback_data.data = None\n                else:\n                    self.playback_data.current_frame += chunksize\n\n        if not self.playback_data.playing:\n            outdata[:] = np.zeros(outdata.size).reshape(outdata.shape)\n\n    def stop(self):\n        \"\"\"\n        Clean exit the text-to-speech agent, terminating playback and joining the transcription thread.\n        \"\"\"\n        self.logger.info(\"Stopping TextToSpeechAgent\")\n        self.terminate_agent.set()\n        if self.transcription_thread is not None:\n            self.transcription_thread.join()\n\n    def _transcription_thread(self):\n        while not self.terminate_agent.wait(timeout=0.01):\n            if self.current_transcription_id in self.text_queues:\n                try:\n                    data = self.text_queues[self.current_transcription_id].get(\n                        block=False\n                    )\n                except Empty:\n                    continue\n                audio = self.model.get_speech(data)\n                try:\n                    self.audio_queues[self.current_transcription_id].put(audio)\n                except KeyError as e:\n                    self.logger.error(\n                        f\"Could not find queue for {self.current_transcription_id}: queuse: {self.audio_queues.keys()}\"\n                    )\n                    raise e\n\n    def _setup_ros2_connector(self):\n        self.hri_ros2_connector = ROS2HRIConnector(\n            self.node_base_name  # , \"single_threaded\"\n        )\n        self.hri_ros2_connector.register_callback(\n            \"/to_human\", self._on_to_human_message\n        )\n        self.ros2_connector = ROS2Connector(\n            self.node_base_name  # , False, \"single_threaded\"\n        )\n        self.ros2_connector.register_callback(\n            \"/voice_commands\", self._on_command_message, msg_type=\"std_msgs/msg/String\"\n        )\n\n    def _on_to_human_message(self, msg: ROS2HRIMessage):\n        self.logger.debug(f\"Receieved message from human: {msg.text}\")\n        self.logger.warning(\n            f\"Starting playback, current id: {self.current_transcription_id}\"\n        )\n        if (\n            self.current_speech_id is None\n            and msg.communication_id is not None\n            and msg.communication_id not in self.remembered_speech_ids\n        ):\n            self.current_speech_id = msg.communication_id\n            self.remembered_speech_ids.append(self.current_speech_id)\n            if len(self.remembered_speech_ids) &gt; 64:\n                self.remembered_speech_ids.pop(0)\n        if self.current_speech_id == msg.communication_id:\n            self.text_queues[self.current_transcription_id].put(msg.text)\n        self.playback_data.playing = True\n\n    def _on_command_message(self, message: ROS2Message):\n        self.logger.info(f\"Receieved status message: {message}\")\n        if message.payload.data == \"tog_play\":\n            self.playback_data.playing = not self.playback_data.playing\n        elif message.payload.data == \"play\":\n            self.playback_data.playing = True\n        elif message.payload.data == \"pause\":\n            self.playback_data.playing = False\n        elif message.payload.data == \"stop\":\n            self.current_speech_id = None\n            self.playback_data.playing = False\n            previous_id = self.current_transcription_id\n            self.logger.warning(f\"Stopping playback, previous id: {previous_id}\")\n            self.current_transcription_id = str(uuid4())[0:8]\n            self.audio_queues[self.current_transcription_id] = Queue()\n            self.text_queues[self.current_transcription_id] = Queue()\n            try:\n                del self.audio_queues[previous_id]\n                del self.text_queues[previous_id]\n            except KeyError:\n                pass\n            self.playback_data.data = None\n            self.playback_data.current_frame = 0\n            self.playback_data.current_segment = None\n\n        self.logger.debug(f\"Current status is: {self.playback_data.playing}\")\n</code></pre>"},{"location":"speech_to_speech/agents/tts/#rai_s2s.tts.agents.TextToSpeechAgent.run","title":"<code>run()</code>","text":"<p>Start the text-to-speech agent, initializing playback and launching the transcription thread.</p> Source code in <code>rai_s2s/tts/agents/tts_agent.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start the text-to-speech agent, initializing playback and launching the transcription thread.\n    \"\"\"\n    self.running = True\n    self.logger.info(\"TextToSpeechAgent started\")\n    self.transcription_thread = Thread(target=self._transcription_thread)\n    self.transcription_thread.start()\n\n    msg = SoundDeviceMessage(read=False)\n    self.speaker.start_action(\n        msg,\n        \"speaker\",\n        on_feedback=self._speaker_callback,\n        on_done=lambda: None,\n    )\n</code></pre>"},{"location":"speech_to_speech/agents/tts/#rai_s2s.tts.agents.TextToSpeechAgent.stop","title":"<code>stop()</code>","text":"<p>Clean exit the text-to-speech agent, terminating playback and joining the transcription thread.</p> Source code in <code>rai_s2s/tts/agents/tts_agent.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Clean exit the text-to-speech agent, terminating playback and joining the transcription thread.\n    \"\"\"\n    self.logger.info(\"Stopping TextToSpeechAgent\")\n    self.terminate_agent.set()\n    if self.transcription_thread is not None:\n        self.transcription_thread.join()\n</code></pre>"},{"location":"speech_to_speech/agents/tts/#purpose","title":"Purpose","text":"<p>The <code>TextToSpeechAgent</code> enables:</p> <ul> <li>Real-time conversion of text to speech</li> <li>Playback control (play/pause/stop) via ROS2 messages</li> <li>Dynamic loading of TTS models from configuration</li> <li>Robust audio handling using queues and event-driven logic</li> <li>Integration with human-robot interaction topics (HRI)</li> </ul>"},{"location":"speech_to_speech/agents/tts/#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Description <code>speaker_config</code> <code>SoundDeviceConfig</code> Configuration for the audio output (speaker). <code>ros2_name</code> <code>str</code> Name of the ROS2 node. <code>tts</code> <code>TTSModel</code> Text-to-speech model instance. <code>logger</code> <code>Optional[logging.Logger]</code> Logger instance, or default logger if <code>None</code>. <code>max_speech_history</code> <code>int</code> Number of speech message IDs to remember (default: 64)."},{"location":"speech_to_speech/agents/tts/#key-methods","title":"Key Methods","text":""},{"location":"speech_to_speech/agents/tts/#from_configcfg_path-optionalstr","title":"<code>from_config(cfg_path: Optional[str])</code>","text":"<p>Instantiates the agent from a configuration file, dynamically selecting the TTS model and setting up audio output.</p>"},{"location":"speech_to_speech/agents/tts/#run","title":"<code>run()</code>","text":"<p>Initializes the agent:</p> <ul> <li>Starts a thread to handle queued text-to-speech conversion</li> <li>Launches speaker playback via <code>SoundDeviceConnector</code></li> </ul>"},{"location":"speech_to_speech/agents/tts/#stop","title":"<code>stop()</code>","text":"<p>Gracefully stops the agent by setting the termination flag and joining the transcription thread.</p>"},{"location":"speech_to_speech/agents/tts/#communication","title":"Communication","text":"<p>The Agent uses the <code>ROS2HRIConnector</code> for connection through 2 ROS2 topics:</p> <ul> <li><code>/to_human</code>: Incoming text messages to convert. Uses <code>rai_interfaces/msg/HRIMessage</code>.</li> <li><code>/voice_commands</code>: Playback control with ROS2 <code>std_msgs/msg/String</code>. Valid values: <code>\"play\"</code>, <code>\"pause\"</code>, <code>\"stop\"</code></li> </ul>"},{"location":"speech_to_speech/agents/tts/#best-practices","title":"Best Practices","text":"<ol> <li>Queue Management: Properly track transcription IDs to avoid queue collisions or memory leaks.</li> <li>Playback Sync: Ensure audio queues are flushed on <code>stop</code> to avoid replaying outdated speech.</li> <li>Graceful Shutdown: Always call <code>stop()</code> to terminate threads cleanly.</li> <li>Model Configuration: Ensure model-specific settings (e.g., voice selection for ElevenLabs) are defined in config files.</li> </ol>"},{"location":"speech_to_speech/agents/tts/#architecture","title":"Architecture","text":"<p>The <code>TextToSpeechAgent</code> interacts with the following core components:</p> <ul> <li>TTSModel: Converts text into audio (e.g., ElevenLabsTTS, OpenTTS)</li> <li>SoundDeviceConnector: Sends synthesized audio to output hardware</li> <li>ROS2HRIConnector: Handles incoming HRI and command messages</li> <li>Queues and Threads: Enable asynchronous and buffered audio processing</li> </ul>"},{"location":"speech_to_speech/agents/tts/#see-also","title":"See Also","text":"<ul> <li>BaseAgent: Abstract base for all agents in RAI</li> <li>SoundDeviceConnector: For details on speaker configuration and streaming</li> <li>Text-to-Speech Models: Supported TTS engines and usage</li> <li>ROS2 HRI Messaging: Interfacing with <code>/to_human</code> and <code>/voice_commands</code></li> </ul>"},{"location":"speech_to_speech/models/overview/","title":"Models","text":""},{"location":"speech_to_speech/models/overview/#overview","title":"Overview","text":"<p>This package provides three primary types of models:</p> <ul> <li>Voice Activity Detection (VAD)</li> <li>Wake Word Detection</li> <li>Transcription</li> </ul> <p>These models are designed with simple and consistent interfaces to allow chaining and integration into audio processing pipelines.</p>"},{"location":"speech_to_speech/models/overview/#model-interfaces","title":"Model Interfaces","text":""},{"location":"speech_to_speech/models/overview/#vad-and-wake-word-detection-api","title":"VAD and Wake Word Detection API","text":"<p>All VAD and Wake Word detection models implement a common <code>detect</code> interface:</p> <pre><code>    def detect(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; Tuple[bool, dict[str, Any]]:\n</code></pre> <p>This design supports chaining multiple models together by passing the output dictionary (<code>input_parameters</code>) from one model into the next.</p>"},{"location":"speech_to_speech/models/overview/#transcription-api","title":"Transcription API","text":"<p>Transcription models implement the <code>transcribe</code> method:</p> <pre><code>    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n</code></pre> <p>This method takes raw audio data encoded as 2-byte integers and returns the corresponding text transcription.</p>"},{"location":"speech_to_speech/models/overview/#included-models","title":"Included Models","text":""},{"location":"speech_to_speech/models/overview/#silerovad","title":"SileroVAD","text":"<ul> <li>Open source model: GitHub</li> <li>No additional setup required</li> <li>Returns a confidence value indicating the presence of speech in the audio</li> </ul> SileroVAD"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.silero_vad.SileroVAD","title":"<code>rai_s2s.asr.models.silero_vad.SileroVAD</code>","text":"<p>               Bases: <code>BaseVoiceDetectionModel</code></p> <p>Voice Activity Detection (VAD) model using SileroVAD.</p> <p>This class loads the SileroVAD model from Torch Hub and detects speech presence in an audio signal. It supports two sampling rates: 8000 Hz and 16000 Hz.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_rate</code> <code>Literal[8000, 16000]</code> <p>The sampling rate of the input audio. Must be either 8000 or 16000. Default is 16000.</p> <code>16000</code> <code>threshold</code> <code>float</code> <p>Confidence threshold for voice detection. If the VAD confidence exceeds this threshold, the method returns <code>True</code> (indicating voice presence). Default is 0.5.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the VAD model, set to <code>\"silero_vad\"</code>.</p> <code>model</code> <code>Module</code> <p>The loaded SileroVAD model.</p> <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the input audio (either 8000 or 16000).</p> <code>window_size</code> <code>int</code> <p>The size of the processing window, determined by the sampling rate. - 512 samples for 16000 Hz - 256 samples for 8000 Hz</p> <code>threshold</code> <code>float</code> <p>Confidence threshold for determining voice activity.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported sampling rate is provided.</p> Source code in <code>rai_s2s/asr/models/silero_vad.py</code> <pre><code>class SileroVAD(BaseVoiceDetectionModel):\n    \"\"\"\n    Voice Activity Detection (VAD) model using SileroVAD.\n\n    This class loads the SileroVAD model from Torch Hub and detects speech presence in an audio signal.\n    It supports two sampling rates: 8000 Hz and 16000 Hz.\n\n    Parameters\n    ----------\n    sampling_rate : Literal[8000, 16000], optional\n        The sampling rate of the input audio. Must be either 8000 or 16000. Default is 16000.\n    threshold : float, optional\n        Confidence threshold for voice detection. If the VAD confidence exceeds this threshold,\n        the method returns `True` (indicating voice presence). Default is 0.5.\n\n    Attributes\n    ----------\n    model_name : str\n        Name of the VAD model, set to `\"silero_vad\"`.\n    model : torch.nn.Module\n        The loaded SileroVAD model.\n    sampling_rate : int\n        The sampling rate of the input audio (either 8000 or 16000).\n    window_size : int\n        The size of the processing window, determined by the sampling rate.\n        - 512 samples for 16000 Hz\n        - 256 samples for 8000 Hz\n    threshold : float\n        Confidence threshold for determining voice activity.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported sampling rate is provided.\n    \"\"\"\n\n    def __init__(self, sampling_rate: Literal[8000, 16000] = 16000, threshold=0.5):\n        super(SileroVAD, self).__init__()\n        self.model_name = \"silero_vad\"\n        self.model, _ = torch.hub.load(\n            repo_or_dir=\"snakers4/silero-vad\",\n            model=self.model_name,\n        )  # type: ignore\n        # NOTE: See silero vad implementation: https://github.com/snakers4/silero-vad/blob/9060f664f20eabb66328e4002a41479ff288f14c/src/silero_vad/utils_vad.py#L61\n        if sampling_rate == 16000:\n            self.sampling_rate = 16000\n            self.window_size = 512\n        elif sampling_rate == 8000:\n            self.sampling_rate = 8000\n            self.window_size = 256\n        else:\n            raise ValueError(\n                \"Only 8000 and 16000 sampling rates are supported\"\n            )  # TODO: consider if this should be a ValueError or something else\n        self.threshold = threshold\n\n    def _int2float(self, sound: NDArray[np.int16]):\n        converted_sound = sound.astype(\"float32\")\n        converted_sound *= 1 / 32768\n        converted_sound = converted_sound.squeeze()\n        return converted_sound\n\n    def detect(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; Tuple[bool, dict[str, Any]]:\n        \"\"\"\n        Detects voice activity in the given audio data.\n\n        This method processes a window of the most recent audio samples, computes a confidence score\n        using the SileroVAD model, and determines if the confidence exceeds the specified threshold.\n\n        Parameters\n        ----------\n        audio_data : NDArray\n            A NumPy array containing audio input data.\n        input_parameters : dict of str to Any\n            Additional parameters for detection.\n\n        Returns\n        -------\n        Tuple[bool, dict]\n            - A boolean indicating whether voice activity was detected (`True` if detected, `False` otherwise).\n            - A dictionary containing the computed VAD confidence score.\n        \"\"\"\n        vad_confidence = self.model(\n            torch.tensor(self._int2float(audio_data[-self.window_size :])),\n            self.sampling_rate,\n        ).item()\n        ret = input_parameters.copy()\n        ret.update({self.model_name: {\"vad_confidence\": vad_confidence}})\n\n        return vad_confidence &gt; self.threshold, ret\n\n    def reset(self):\n        \"\"\"\n        Resets the voice activity detection model.\n        \"\"\"\n        self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.silero_vad.SileroVAD.detect","title":"<code>detect(audio_data, input_parameters)</code>","text":"<p>Detects voice activity in the given audio data.</p> <p>This method processes a window of the most recent audio samples, computes a confidence score using the SileroVAD model, and determines if the confidence exceeds the specified threshold.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>NDArray</code> <p>A NumPy array containing audio input data.</p> required <code>input_parameters</code> <code>dict of str to Any</code> <p>Additional parameters for detection.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, dict]</code> <ul> <li>A boolean indicating whether voice activity was detected (<code>True</code> if detected, <code>False</code> otherwise).</li> <li>A dictionary containing the computed VAD confidence score.</li> </ul> Source code in <code>rai_s2s/asr/models/silero_vad.py</code> <pre><code>def detect(\n    self, audio_data: NDArray, input_parameters: dict[str, Any]\n) -&gt; Tuple[bool, dict[str, Any]]:\n    \"\"\"\n    Detects voice activity in the given audio data.\n\n    This method processes a window of the most recent audio samples, computes a confidence score\n    using the SileroVAD model, and determines if the confidence exceeds the specified threshold.\n\n    Parameters\n    ----------\n    audio_data : NDArray\n        A NumPy array containing audio input data.\n    input_parameters : dict of str to Any\n        Additional parameters for detection.\n\n    Returns\n    -------\n    Tuple[bool, dict]\n        - A boolean indicating whether voice activity was detected (`True` if detected, `False` otherwise).\n        - A dictionary containing the computed VAD confidence score.\n    \"\"\"\n    vad_confidence = self.model(\n        torch.tensor(self._int2float(audio_data[-self.window_size :])),\n        self.sampling_rate,\n    ).item()\n    ret = input_parameters.copy()\n    ret.update({self.model_name: {\"vad_confidence\": vad_confidence}})\n\n    return vad_confidence &gt; self.threshold, ret\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.silero_vad.SileroVAD.reset","title":"<code>reset()</code>","text":"<p>Resets the voice activity detection model.</p> Source code in <code>rai_s2s/asr/models/silero_vad.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the voice activity detection model.\n    \"\"\"\n    self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#openwakeword","title":"OpenWakeWord","text":"<ul> <li>Open source project: GitHub</li> <li>Supports predefined and custom wake words</li> <li>Returns <code>True</code> when the specified wake word is detected in the audio</li> </ul> OpenWakeWord"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord","title":"<code>rai_s2s.asr.models.open_wake_word.OpenWakeWord</code>","text":"<p>               Bases: <code>BaseVoiceDetectionModel</code></p> <p>A wake word detection model using the Open Wake Word framework.</p> <p>This class loads a specified wake word model and detects whether a wake word is present in the provided audio input.</p> <p>Parameters:</p> Name Type Description Default <code>wake_word_model_path</code> <code>str</code> <p>Path to the wake word model file or name of a standard one.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for wake word detection. If a prediction surpasses this value, the model will trigger a wake word detection. Default is 0.1.</p> <code>0.1</code> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model, set to <code>\"open_wake_word\"</code>.</p> <code>model</code> <code>Model</code> <p>The Open Wake Word model instance used for inference.</p> <code>threshold</code> <code>float</code> <p>The confidence threshold for determining wake word detection.</p> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>class OpenWakeWord(BaseVoiceDetectionModel):\n    \"\"\"\n    A wake word detection model using the Open Wake Word framework.\n\n    This class loads a specified wake word model and detects whether a wake word is present\n    in the provided audio input.\n\n    Parameters\n    ----------\n    wake_word_model_path : str\n        Path to the wake word model file or name of a standard one.\n    threshold : float, optional\n        The confidence threshold for wake word detection. If a prediction surpasses this\n        value, the model will trigger a wake word detection. Default is 0.1.\n\n    Attributes\n    ----------\n    model_name : str\n        The name of the model, set to `\"open_wake_word\"`.\n    model : OWWModel\n        The Open Wake Word model instance used for inference.\n    threshold : float\n        The confidence threshold for determining wake word detection.\n    \"\"\"\n\n    def __init__(self, wake_word_model_path: str, threshold: float = 0.1):\n        \"\"\"\n        Initializes the OpenWakeWord detection model.\n\n        Parameters\n        ----------\n        wake_word_model_path : str\n            Path to the wake word model file.\n        threshold : float, optional\n            Confidence threshold for wake word detection. Default is 0.1.\n        \"\"\"\n        super(OpenWakeWord, self).__init__()\n        self.model_name = \"open_wake_word\"\n        download_models()\n        self.model = OWWModel(\n            wakeword_models=[\n                wake_word_model_path,\n            ],\n            inference_framework=\"onnx\",\n        )\n        self.threshold = threshold\n\n    def detect(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; Tuple[bool, dict[str, Any]]:\n        \"\"\"\n        Detects whether a wake word is present in the given audio data.\n\n        This method runs inference on the provided audio data and determines whether\n        the detected confidence surpasses the threshold. If so, it resets the model\n        and returns `True`, indicating a wake word detection.\n\n        Parameters\n        ----------\n        audio_data : NDArray\n            A NumPy array representing the input audio data.\n        input_parameters : dict of str to Any\n            Additional input parameters to be included in the output.\n\n        Returns\n        -------\n        Tuple[bool, dict]\n            A tuple where the first value is a boolean indicating whether the wake word\n            was detected (`True` if detected, `False` otherwise). The second value is\n            a dictionary containing predictions and confidence values for them.\n\n        Raises\n        ------\n        Exception\n            If the predictions returned by the model are not in the expected dictionary format.\n        \"\"\"\n        predictions = self.model.predict(audio_data)\n        ret = input_parameters.copy()\n        ret.update({self.model_name: {\"predictions\": predictions}})\n        if not isinstance(predictions, dict):\n            raise Exception(\n                f\"Unexpected format from model predict {type(predictions)}:{predictions}\"\n            )\n        for _, value in predictions.items():  # type ignore\n            if value &gt; self.threshold:\n                self.model.reset()\n                return True, ret\n        return False, ret\n\n    def reset(self):\n        \"\"\"\n        Resets the wake word detection model.\n        \"\"\"\n        self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord.__init__","title":"<code>__init__(wake_word_model_path, threshold=0.1)</code>","text":"<p>Initializes the OpenWakeWord detection model.</p> <p>Parameters:</p> Name Type Description Default <code>wake_word_model_path</code> <code>str</code> <p>Path to the wake word model file.</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold for wake word detection. Default is 0.1.</p> <code>0.1</code> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>def __init__(self, wake_word_model_path: str, threshold: float = 0.1):\n    \"\"\"\n    Initializes the OpenWakeWord detection model.\n\n    Parameters\n    ----------\n    wake_word_model_path : str\n        Path to the wake word model file.\n    threshold : float, optional\n        Confidence threshold for wake word detection. Default is 0.1.\n    \"\"\"\n    super(OpenWakeWord, self).__init__()\n    self.model_name = \"open_wake_word\"\n    download_models()\n    self.model = OWWModel(\n        wakeword_models=[\n            wake_word_model_path,\n        ],\n        inference_framework=\"onnx\",\n    )\n    self.threshold = threshold\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord.detect","title":"<code>detect(audio_data, input_parameters)</code>","text":"<p>Detects whether a wake word is present in the given audio data.</p> <p>This method runs inference on the provided audio data and determines whether the detected confidence surpasses the threshold. If so, it resets the model and returns <code>True</code>, indicating a wake word detection.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>NDArray</code> <p>A NumPy array representing the input audio data.</p> required <code>input_parameters</code> <code>dict of str to Any</code> <p>Additional input parameters to be included in the output.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, dict]</code> <p>A tuple where the first value is a boolean indicating whether the wake word was detected (<code>True</code> if detected, <code>False</code> otherwise). The second value is a dictionary containing predictions and confidence values for them.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the predictions returned by the model are not in the expected dictionary format.</p> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>def detect(\n    self, audio_data: NDArray, input_parameters: dict[str, Any]\n) -&gt; Tuple[bool, dict[str, Any]]:\n    \"\"\"\n    Detects whether a wake word is present in the given audio data.\n\n    This method runs inference on the provided audio data and determines whether\n    the detected confidence surpasses the threshold. If so, it resets the model\n    and returns `True`, indicating a wake word detection.\n\n    Parameters\n    ----------\n    audio_data : NDArray\n        A NumPy array representing the input audio data.\n    input_parameters : dict of str to Any\n        Additional input parameters to be included in the output.\n\n    Returns\n    -------\n    Tuple[bool, dict]\n        A tuple where the first value is a boolean indicating whether the wake word\n        was detected (`True` if detected, `False` otherwise). The second value is\n        a dictionary containing predictions and confidence values for them.\n\n    Raises\n    ------\n    Exception\n        If the predictions returned by the model are not in the expected dictionary format.\n    \"\"\"\n    predictions = self.model.predict(audio_data)\n    ret = input_parameters.copy()\n    ret.update({self.model_name: {\"predictions\": predictions}})\n    if not isinstance(predictions, dict):\n        raise Exception(\n            f\"Unexpected format from model predict {type(predictions)}:{predictions}\"\n        )\n    for _, value in predictions.items():  # type ignore\n        if value &gt; self.threshold:\n            self.model.reset()\n            return True, ret\n    return False, ret\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord.reset","title":"<code>reset()</code>","text":"<p>Resets the wake word detection model.</p> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the wake word detection model.\n    \"\"\"\n    self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#openaiwhisper","title":"OpenAIWhisper","text":"<ul> <li>Cloud-based transcription model: Documentation</li> <li>Requires setting the <code>OPEN_API_KEY</code> environment variable</li> <li>Offers language and model customization via the API</li> </ul> OpenAIWhisper"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_ai_whisper.OpenAIWhisper","title":"<code>rai_s2s.asr.models.open_ai_whisper.OpenAIWhisper</code>","text":"<p>               Bases: <code>BaseTranscriptionModel</code></p> Source code in <code>rai_s2s/asr/models/open_ai_whisper.py</code> <pre><code>class OpenAIWhisper(BaseTranscriptionModel):\n    def __init__(\n        self, model_name: str, sample_rate: int, language: str = \"en\", **kwargs\n    ):\n        super().__init__(model_name, sample_rate, language)\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n        self.api_key = api_key\n        self.openai_client = OpenAI()\n        self.model = partial(\n            self.openai_client.audio.transcriptions.create,\n            model=self.model_name,\n            **kwargs,\n        )\n        self.logger = logging.getLogger(__name__)\n        self.samples = []\n\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        normalized_data = data.astype(np.float32) / 32768.0\n        with io.BytesIO() as temp_wav_buffer:\n            wavfile.write(temp_wav_buffer, self.sample_rate, normalized_data)\n            temp_wav_buffer.seek(0)\n            temp_wav_buffer.name = \"temp.wav\"\n            response = self.model(file=temp_wav_buffer, language=self.language)\n        transcription = response.text\n        self.logger.info(\"transcription: %s\", transcription)\n        return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#localwhisper","title":"LocalWhisper","text":"<ul> <li>Local deployment of OpenAI Whisper: GitHub</li> <li>Supports GPU acceleration</li> <li>Same configuration interface as OpenAIWhisper</li> </ul> LocalWhisper"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.LocalWhisper","title":"<code>rai_s2s.asr.models.local_whisper.LocalWhisper</code>","text":"<p>               Bases: <code>BaseTranscriptionModel</code></p> <p>A transcription model using OpenAI's Whisper, running locally.</p> <p>This class loads a Whisper model and performs speech-to-text transcription on audio data. It supports GPU acceleration if available.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Whisper model to load.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the input audio, in Hz.</p> required <code>language</code> <code>str</code> <p>The language of the transcription output. Default is \"en\" (English).</p> <code>'en'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for loading the Whisper model.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>whisper</code> <code>Whisper</code> <p>The loaded Whisper model for transcription.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for logging transcription results.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>class LocalWhisper(BaseTranscriptionModel):\n    \"\"\"\n    A transcription model using OpenAI's Whisper, running locally.\n\n    This class loads a Whisper model and performs speech-to-text transcription\n    on audio data. It supports GPU acceleration if available.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the Whisper model to load.\n    sample_rate : int\n        The sample rate of the input audio, in Hz.\n    language : str, optional\n        The language of the transcription output. Default is \"en\" (English).\n    **kwargs : dict, optional\n        Additional keyword arguments for loading the Whisper model.\n\n    Attributes\n    ----------\n    whisper : whisper.Whisper\n        The loaded Whisper model for transcription.\n    logger : logging.Logger\n        Logger instance for logging transcription results.\n    \"\"\"\n\n    def __init__(\n        self, model_name: str, sample_rate: int, language: str = \"en\", **kwargs\n    ):\n        super().__init__(model_name, sample_rate, language)\n        self.decode_options = {\n            \"language\": language,  # Set language to English\n            \"task\": \"transcribe\",  # Set task to transcribe (not translate)\n            \"fp16\": False,  # Use FP32 instead of FP16 for better precision\n            \"without_timestamps\": True,  # Don't include timestamps in output\n            \"suppress_tokens\": [-1],  # Default tokens to suppress\n            \"suppress_blank\": True,  # Suppress blank outputs\n            \"beam_size\": 5,  # Beam size for beam search\n        }\n        if torch.cuda.is_available():\n            self.whisper = whisper.load_model(self.model_name, device=\"cuda\", **kwargs)\n        else:\n            self.whisper = whisper.load_model(self.model_name, **kwargs)\n\n        self.logger = logging.getLogger(__name__)\n\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        \"\"\"\n        Transcribes speech from the given audio data using Whisper.\n\n        This method normalizes the input audio, processes it using the Whisper model,\n        and returns the transcribed text.\n\n        Parameters\n        ----------\n        data : NDArray[np.int16]\n            A NumPy array containing the raw audio waveform data.\n\n        Returns\n        -------\n        str\n            The transcribed text from the audio input.\n        \"\"\"\n        normalized_data = data.astype(np.float32) / 32768.0\n\n        result = whisper.transcribe(\n            self.whisper, normalized_data, **self.decode_options\n        )\n        transcription = result[\"text\"]\n        self.logger.info(\"transcription: %s\", transcription)\n        transcription = cast(str, transcription)\n        self.latest_transcription = transcription\n        return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.LocalWhisper.transcribe","title":"<code>transcribe(data)</code>","text":"<p>Transcribes speech from the given audio data using Whisper.</p> <p>This method normalizes the input audio, processes it using the Whisper model, and returns the transcribed text.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[int16]</code> <p>A NumPy array containing the raw audio waveform data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transcribed text from the audio input.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n    \"\"\"\n    Transcribes speech from the given audio data using Whisper.\n\n    This method normalizes the input audio, processes it using the Whisper model,\n    and returns the transcribed text.\n\n    Parameters\n    ----------\n    data : NDArray[np.int16]\n        A NumPy array containing the raw audio waveform data.\n\n    Returns\n    -------\n    str\n        The transcribed text from the audio input.\n    \"\"\"\n    normalized_data = data.astype(np.float32) / 32768.0\n\n    result = whisper.transcribe(\n        self.whisper, normalized_data, **self.decode_options\n    )\n    transcription = result[\"text\"]\n    self.logger.info(\"transcription: %s\", transcription)\n    transcription = cast(str, transcription)\n    self.latest_transcription = transcription\n    return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#fasterwhisper","title":"FasterWhisper","text":"<ul> <li>Optimized Whisper variant: GitHub</li> <li>Designed for high speed and low memory usage</li> <li>Follows the same API as Whisper models</li> </ul> FasterWhisper"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.FasterWhisper","title":"<code>rai_s2s.asr.models.local_whisper.FasterWhisper</code>","text":"<p>               Bases: <code>BaseTranscriptionModel</code></p> <p>A transcription model using Faster Whisper for efficient speech-to-text conversion.</p> <p>This class loads a Faster Whisper model, optimized for speed and efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Faster Whisper model to load.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the input audio, in Hz.</p> required <code>language</code> <code>str</code> <p>The language of the transcription output. Default is \"en\" (English).</p> <code>'en'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for loading the Faster Whisper model.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>WhisperModel</code> <p>The loaded Faster Whisper model instance.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for logging transcription results.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>class FasterWhisper(BaseTranscriptionModel):\n    \"\"\"\n    A transcription model using Faster Whisper for efficient speech-to-text conversion.\n\n    This class loads a Faster Whisper model, optimized for speed and efficiency.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the Faster Whisper model to load.\n    sample_rate : int\n        The sample rate of the input audio, in Hz.\n    language : str, optional\n        The language of the transcription output. Default is \"en\" (English).\n    **kwargs : dict, optional\n        Additional keyword arguments for loading the Faster Whisper model.\n\n    Attributes\n    ----------\n    model : WhisperModel\n        The loaded Faster Whisper model instance.\n    logger : logging.Logger\n        Logger instance for logging transcription results.\n    \"\"\"\n\n    def __init__(\n        self, model_name: str, sample_rate: int, language: str = \"en\", **kwargs\n    ):\n        super().__init__(model_name, sample_rate, language)\n        self.model = WhisperModel(model_name, **kwargs)\n        self.logger = logging.getLogger(__name__)\n\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        \"\"\"\n        Transcribes speech from the given audio data using Faster Whisper.\n\n        This method normalizes the input audio, processes it using the Faster Whisper model,\n        and returns the transcribed text.\n\n        Parameters\n        ----------\n        data : NDArray[np.int16]\n            A NumPy array containing the raw audio waveform data.\n\n        Returns\n        -------\n        str\n            The transcribed text from the audio input.\n        \"\"\"\n        normalized_data = data.astype(np.float32) / 32768.0\n        segments, _ = self.model.transcribe(normalized_data)\n        transcription = \" \".join(segment.text for segment in segments)\n        self.logger.info(\"transcription: %s\", transcription)\n        return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.FasterWhisper.transcribe","title":"<code>transcribe(data)</code>","text":"<p>Transcribes speech from the given audio data using Faster Whisper.</p> <p>This method normalizes the input audio, processes it using the Faster Whisper model, and returns the transcribed text.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[int16]</code> <p>A NumPy array containing the raw audio waveform data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transcribed text from the audio input.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n    \"\"\"\n    Transcribes speech from the given audio data using Faster Whisper.\n\n    This method normalizes the input audio, processes it using the Faster Whisper model,\n    and returns the transcribed text.\n\n    Parameters\n    ----------\n    data : NDArray[np.int16]\n        A NumPy array containing the raw audio waveform data.\n\n    Returns\n    -------\n    str\n        The transcribed text from the audio input.\n    \"\"\"\n    normalized_data = data.astype(np.float32) / 32768.0\n    segments, _ = self.model.transcribe(normalized_data)\n    transcription = \" \".join(segment.text for segment in segments)\n    self.logger.info(\"transcription: %s\", transcription)\n    return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#elevenlabs","title":"ElevenLabs","text":"<ul> <li>Cloud-based TTS model: Website</li> <li>Requires the environment variable <code>ELEVENLABS_API_KEY</code> with a valid key</li> </ul> ElevenLabs"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS","title":"<code>rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS</code>","text":"<p>               Bases: <code>TTSModel</code></p> <p>A text-to-speech (TTS) model interface for ElevenLabs.</p> <p>Parameters:</p> Name Type Description Default <code>voice</code> <code>str</code> <p>The voice model to use.</p> required <code>base_url</code> <code>str</code> <p>The API endpoint for the ElevenLabs API, by default None.</p> <code>None</code> Source code in <code>rai_s2s/tts/models/elevenlabs_tts.py</code> <pre><code>class ElevenLabsTTS(TTSModel):\n    \"\"\"\n    A text-to-speech (TTS) model interface for ElevenLabs.\n\n    Parameters\n    ----------\n    voice : str, optional\n        The voice model to use.\n    base_url : str, optional\n        The API endpoint for the ElevenLabs API, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        voice: str,\n        base_url: str | None = None,\n    ):\n        api_key = os.getenv(key=\"ELEVENLABS_API_KEY\")\n        if api_key is None:\n            raise TTSModelError(\"ELEVENLABS_API_KEY environment variable is not set.\")\n\n        self.client = ElevenLabs(base_url=base_url, api_key=api_key)\n        self.voice_settings = VoiceSettings(\n            stability=0.7,\n            similarity_boost=0.5,\n        )\n\n        voices = self.client.voices.get_all().voices\n        voice_id = next((v.voice_id for v in voices if v.name == voice), None)\n        if voice_id is None:\n            raise TTSModelError(f\"Voice {voice} not found\")\n        self.voice = Voice(voice_id=voice_id, settings=self.voice_settings)\n\n    def get_speech(self, text: str) -&gt; AudioSegment:\n        \"\"\"\n        Converts text into speech using the ElevenLabs API.\n\n        Parameters\n        ----------\n        text : str\n            The input text to be converted into speech.\n\n        Returns\n        -------\n        AudioSegment\n            The generated speech as an `AudioSegment` object.\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the ElevenLabs API is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n        try:\n            response = self.client.generate(\n                text=text,\n                voice=self.voice,\n                optimize_streaming_latency=4,\n            )\n            audio_data = b\"\".join(response)\n        except Exception as e:\n            raise TTSModelError(f\"Error occurred while fetching audio: {e}\") from e\n\n        # Load audio into memory (ElevenLabs returns MP3)\n        audio_segment = AudioSegment.from_mp3(BytesIO(audio_data))\n        return audio_segment\n\n    def get_tts_params(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Returns TTS sampling rate and channels.\n\n        The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n        Returns\n        -------\n        Tuple[int, int]\n            sample rate, channels\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the ElevenLabs API is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n        data = self.get_speech(\"A\")\n        return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS.get_speech","title":"<code>get_speech(text)</code>","text":"<p>Converts text into speech using the ElevenLabs API.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be converted into speech.</p> required <p>Returns:</p> Type Description <code>AudioSegment</code> <p>The generated speech as an <code>AudioSegment</code> object.</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the ElevenLabs API is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/elevenlabs_tts.py</code> <pre><code>def get_speech(self, text: str) -&gt; AudioSegment:\n    \"\"\"\n    Converts text into speech using the ElevenLabs API.\n\n    Parameters\n    ----------\n    text : str\n        The input text to be converted into speech.\n\n    Returns\n    -------\n    AudioSegment\n        The generated speech as an `AudioSegment` object.\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the ElevenLabs API is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n    try:\n        response = self.client.generate(\n            text=text,\n            voice=self.voice,\n            optimize_streaming_latency=4,\n        )\n        audio_data = b\"\".join(response)\n    except Exception as e:\n        raise TTSModelError(f\"Error occurred while fetching audio: {e}\") from e\n\n    # Load audio into memory (ElevenLabs returns MP3)\n    audio_segment = AudioSegment.from_mp3(BytesIO(audio_data))\n    return audio_segment\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS.get_tts_params","title":"<code>get_tts_params()</code>","text":"<p>Returns TTS sampling rate and channels.</p> <p>The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>sample rate, channels</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the ElevenLabs API is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/elevenlabs_tts.py</code> <pre><code>def get_tts_params(self) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns TTS sampling rate and channels.\n\n    The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n    Returns\n    -------\n    Tuple[int, int]\n        sample rate, channels\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the ElevenLabs API is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n    data = self.get_speech(\"A\")\n    return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#opentts","title":"OpenTTS","text":"<ul> <li>Open source TTS solution: GitHub</li> <li>Easy setup via Docker:</li> </ul> <pre><code> docker run -it -p 5500:5500 synesthesiam/opentts:en --no-espeak\n</code></pre> <ul> <li>Provides a TTS server running on port 5500</li> <li>Supports multiple voices and configurations</li> </ul> OpenTTS"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.open_tts.OpenTTS","title":"<code>rai_s2s.tts.models.open_tts.OpenTTS</code>","text":"<p>               Bases: <code>TTSModel</code></p> <p>A text-to-speech (TTS) model interface for OpenTTS.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint for the OpenTTS server, by default \"http://localhost:5500/api/tts\".</p> <code>'http://localhost:5500/api/tts'</code> <code>voice</code> <code>str</code> <p>The voice model to use, by default \"larynx:blizzard_lessac-glow_tts\".</p> <code>'larynx:blizzard_lessac-glow_tts'</code> Source code in <code>rai_s2s/tts/models/open_tts.py</code> <pre><code>class OpenTTS(TTSModel):\n    \"\"\"\n    A text-to-speech (TTS) model interface for OpenTTS.\n\n    Parameters\n    ----------\n    url : str, optional\n        The API endpoint for the OpenTTS server, by default \"http://localhost:5500/api/tts\".\n    voice : str, optional\n        The voice model to use, by default \"larynx:blizzard_lessac-glow_tts\".\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str = \"http://localhost:5500/api/tts\",\n        voice: str = \"larynx:blizzard_lessac-glow_tts\",\n    ):\n        self.url = url\n        self.voice = voice\n\n    def get_speech(self, text: str) -&gt; AudioSegment:\n        \"\"\"\n        Converts text into speech using the OpenTTS API.\n\n        Parameters\n        ----------\n        text : str\n            The input text to be converted into speech.\n\n        Returns\n        -------\n        AudioSegment\n            The generated speech as an `AudioSegment` object.\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the OpenTTS server is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n        params = {\n            \"voice\": self.voice,\n            \"text\": text,\n        }\n        try:\n            response = requests.get(self.url, params=params)\n        except requests.exceptions.RequestException as e:\n            raise TTSModelError(\n                f\"Error occurred while fetching audio: {e}, check if OpenTTS server is running correctly.\"\n            ) from e\n\n        content_type = response.headers.get(\"Content-Type\", \"\")\n\n        if \"audio\" not in content_type:\n            raise TTSModelError(\"Response does not contain audio data\")\n\n        # Load audio into memory\n        audio_bytes = BytesIO(response.content)\n        sample_rate, data = read(audio_bytes)\n        if data.dtype == np.int32:\n            data = (data / 2**16).astype(np.int16)  # Scale down from int32\n        elif data.dtype == np.uint8:\n            data = (data - 128).astype(np.int16) * 256  # Convert uint8 to int16\n        elif data.dtype == np.float32:\n            data = (\n                (data * 32768).clip(-32768, 32767).astype(np.int16)\n            )  # Convert float32 to int16\n\n        audio = AudioSegment(\n            data.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1\n        )\n        if self.sample_rate == -1:\n            return audio\n        else:\n            return self._resample(audio)\n\n    def get_tts_params(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Returns TTS samling rate and channels.\n\n        The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n        Returns\n        -------\n        Tuple[int, int]\n            sample rate, channels\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the OpenTTS server is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n\n        data = self.get_speech(\"A\")\n        return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.open_tts.OpenTTS.get_speech","title":"<code>get_speech(text)</code>","text":"<p>Converts text into speech using the OpenTTS API.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be converted into speech.</p> required <p>Returns:</p> Type Description <code>AudioSegment</code> <p>The generated speech as an <code>AudioSegment</code> object.</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the OpenTTS server is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/open_tts.py</code> <pre><code>def get_speech(self, text: str) -&gt; AudioSegment:\n    \"\"\"\n    Converts text into speech using the OpenTTS API.\n\n    Parameters\n    ----------\n    text : str\n        The input text to be converted into speech.\n\n    Returns\n    -------\n    AudioSegment\n        The generated speech as an `AudioSegment` object.\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the OpenTTS server is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n    params = {\n        \"voice\": self.voice,\n        \"text\": text,\n    }\n    try:\n        response = requests.get(self.url, params=params)\n    except requests.exceptions.RequestException as e:\n        raise TTSModelError(\n            f\"Error occurred while fetching audio: {e}, check if OpenTTS server is running correctly.\"\n        ) from e\n\n    content_type = response.headers.get(\"Content-Type\", \"\")\n\n    if \"audio\" not in content_type:\n        raise TTSModelError(\"Response does not contain audio data\")\n\n    # Load audio into memory\n    audio_bytes = BytesIO(response.content)\n    sample_rate, data = read(audio_bytes)\n    if data.dtype == np.int32:\n        data = (data / 2**16).astype(np.int16)  # Scale down from int32\n    elif data.dtype == np.uint8:\n        data = (data - 128).astype(np.int16) * 256  # Convert uint8 to int16\n    elif data.dtype == np.float32:\n        data = (\n            (data * 32768).clip(-32768, 32767).astype(np.int16)\n        )  # Convert float32 to int16\n\n    audio = AudioSegment(\n        data.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1\n    )\n    if self.sample_rate == -1:\n        return audio\n    else:\n        return self._resample(audio)\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.open_tts.OpenTTS.get_tts_params","title":"<code>get_tts_params()</code>","text":"<p>Returns TTS samling rate and channels.</p> <p>The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>sample rate, channels</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the OpenTTS server is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/open_tts.py</code> <pre><code>def get_tts_params(self) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns TTS samling rate and channels.\n\n    The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n    Returns\n    -------\n    Tuple[int, int]\n        sample rate, channels\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the OpenTTS server is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n\n    data = self.get_speech(\"A\")\n    return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#custom-models","title":"Custom Models","text":""},{"location":"speech_to_speech/models/overview/#voice-detection-models","title":"Voice Detection Models","text":"<p>To implement a custom VAD or Wake Word model, inherit from <code>rai_asr.base.BaseVoiceDetectionModel</code> and implement the following methods:</p> <pre><code>class MyDetectionModel(BaseVoiceDetectionModel):\n    def detect(self, audio_data: NDArray, input_parameters: dict[str, Any]) -&gt; Tuple[bool, dict[str, Any]]:\n        ...\n\n    def reset(self):\n        ...\n</code></pre>"},{"location":"speech_to_speech/models/overview/#transcription-models","title":"Transcription Models","text":"<p>To implement a custom transcription model, inherit from <code>rai_asr.base.BaseTranscriptionModel</code> and implement:</p> <pre><code>class MyTranscriptionModel(BaseTranscriptionModel):\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        ...\n</code></pre>"},{"location":"speech_to_speech/models/overview/#tts-models","title":"TTS Models","text":"<p>To create a custom TTS model, inherit from <code>rai_tts.models.base.TTSModel</code> and implement the required interface:</p> <pre><code>class MyTTSModel(TTSModel):\n    def get_speech(self, text: str) -&gt; AudioSegment:\n        ...\n        return AudioSegment()\n\n    def get_tts_params(self) -&gt; Tuple[int, int]:\n        ...\n        return sample_rate, channels\n</code></pre>"},{"location":"tutorials/benchmarking/","title":"Benchmarking","text":"<p>Note</p> <p>If you aren't familiar with our benchmark package, please read RAI Bench first.</p> <p>Currently, we offer 2 predefined benchmarks:</p> <ul> <li>Manipulation_O3DE</li> <li>Tool_Calling_Agent</li> </ul> <p>If you want to test multiple models across different benchmark configurations, go to Testing Models.</p> <p>If your goal is creating custom tasks and scenarios, visit Creating Custom Tasks.</p>"},{"location":"tutorials/benchmarking/#manipulation-o3de","title":"Manipulation O3DE","text":"<ul> <li>Follow the main setup Basic Setup and setup from Manipulation demo Setup</li> <li>To see available options run:     <pre><code>python src/rai_bench/rai_bench/examples/manipulation_o3de.py --help\n</code></pre></li> <li> <p>Example usage:</p> <pre><code>python src/rai_bench/rai_bench/examples/manipulation_o3de.py --model-name qwen2.5:7b --vendor ollama --levels trivial\n</code></pre> <p>Note</p> <p>When using Ollama, be sure to pull the model first.</p> <p>Warning</p> <p>Running all scenarios will take a while. If you want to just try it out, we recommend choosing just one level of difficulty.</p> </li> </ul>"},{"location":"tutorials/benchmarking/#tool-calling-agent","title":"Tool Calling Agent","text":"<ul> <li>This benchmark does not require any additional setup besides the main one Basic Setup</li> <li>To see available options run:     <pre><code>python src/rai_bench/rai_bench/examples/tool_calling_agent.py --help\n</code></pre></li> <li>Example usage:</li> </ul> <pre><code>python src/rai_bench/rai_bench/examples/tool_calling_agent.py --model-name qwen2.5:7b --vendor ollama --extra-tool-calls 5 --task-types basic  --n-shots 5 --prompt-detail descriptive --complexities easy\n</code></pre>"},{"location":"tutorials/benchmarking/#testing-models","title":"Testing Models","text":"<p>The best way of benchmarking your models is using the <code>src/rai_bench/rai_bench/examples/benchmarking_models.py</code></p> <p>Feel free to modify the benchmark configs to suit your needs, you can choose every possible set of params and the benchmark will be run tasks with every combination:</p> <pre><code>if __name__ == \"__main__\":\n    # Define models you want to benchmark\n    model_names = [\"qwen3:4b\", \"llama3.2:3b\"]\n    vendors = [\"ollama\", \"ollama\"]\n\n    # Define benchmarks that will be used\n    mani_conf = ManipulationO3DEBenchmarkConfig(\n        o3de_config_path=\"src/rai_bench/rai_bench/manipulation_o3de/predefined/configs/o3de_config.yaml\",\n        levels=[  # define what difficulty of tasks to include in benchmark\n            \"trivial\",\n            \"easy\",\n        ],\n        repeats=1,  # how many times to repeat\n    )\n    tool_conf = ToolCallingAgentBenchmarkConfig(\n        extra_tool_calls=[0, 5],  # how many extra tool calls allowed to still pass\n        task_types=[  # what types of tasks to include\n            \"basic\",\n            \"custom_interfaces\",\n        ],\n        N_shots=[0, 2],  # examples in system prompt\n        prompt_detail=[\"brief\", \"descriptive\"],  # how descriptive should task prompt be\n        repeats=1,\n    )\n\n    out_dir = \"src/rai_bench/rai_bench/experiments\"\n    test_models(\n        model_names=model_names,\n        vendors=vendors,\n        benchmark_configs=[mani_conf, tool_conf],\n        out_dir=out_dir,\n        # if you want to pass any additinal args to model\n        additional_model_args=[\n            {\"reasoning\": False},\n            {},\n        ],\n    )\n</code></pre> <p>Based on the example above the <code>Tool Calling</code> benchmark will run basic and custom_interfaces tasks with every configuration of [extra_tool_calls x N_shots x prompt_detail] provided which will result in almost 500 tasks. Manipulation benchmark will run all specified task level once as there is no additional params. Reapeat is set to 1 in both configs so there will be no additional runs.</p> <p>Note</p> <p>When using ollama vendor make sure to pull used models first</p>"},{"location":"tutorials/benchmarking/#viewing-results","title":"Viewing Results","text":"<p>From every benchmark run, there will be results saved in the provided output directory:</p> <ul> <li>Logs - in <code>benchmark.log</code> file</li> <li>results_summary.csv - for overall metrics</li> <li>results.csv - for detailed results of every task/scenario</li> </ul> <p>When using <code>test_models</code>, the output directories will be saved as <code>&lt;run_datetime&gt;/&lt;benchmark_name&gt;/&lt;model&gt;/&lt;repeat&gt;/...</code> and this format can be visualized with our Streamlit script:</p> <pre><code>streamlit run src/rai_bench/rai_bench/examples/visualise_streamlit.py\n</code></pre>"},{"location":"tutorials/benchmarking/#creating-custom-tasks","title":"Creating Custom Tasks","text":""},{"location":"tutorials/benchmarking/#manipulation-o3de-scenarios","title":"Manipulation O3DE Scenarios","text":"<p>To create your own Scenarios, you will need a Scene Config and Task - check out example <code>src/rai_bench/rai_bench/examples/custom_scenario.py</code>. You can combine already existing Scene and existing Task to create a new Scenario like:</p> <pre><code>import logging\nfrom pathlib import Path\nfrom typing import List, Sequence, Tuple, Union\n\nfrom rclpy.impl.rcutils_logger import RcutilsLogger\n\nfrom rai_bench.manipulation_o3de.benchmark import Scenario\nfrom rai_bench.manipulation_o3de.interfaces import (\n    ManipulationTask,\n)\nfrom rai_bench.manipulation_o3de.tasks import PlaceObjectAtCoordTask\nfrom rai_sim.simulation_bridge import Entity, SceneConfig\n\nloggers_type = Union[RcutilsLogger, logging.Logger]\n\n### Define your scene setup ####################3\npath_to_your_config = (\n    \"src/rai_bench/rai_bench/manipulation_o3de/predefined/configs/1a.yaml\"\n)\nscene_config = SceneConfig.load_base_config(Path(path_to_your_config))\n\n# configure existing Task with different params\ntarget_coords = (0.1, 0.1)\ndisp = 0.1\ntask = PlaceObjectAtCoordTask(\n    obj_type=\"apple\",\n    target_position=target_coords,\n    allowable_displacement=disp,\n)\n\nScenario(task=task, scene_config=scene_config, scene_config_path=path_to_your_config)\n</code></pre> <p>But you can also create them from scratch. Creating a Scene Config is very easy, just declare entities in a YAML file like:</p> <pre><code>entities:\n  - name: apple1\n    prefab_name: apple # make sure that this prefab exists in simulation\n      pose:\n          translation:\n              x: 0.0\n              y: 0.5\n              z: 0.05\n          rotation:\n              x: 0.0\n              y: 0.0\n              z: 0.0\n              w: 1.0\n</code></pre> <p>Creating your own Task will require slightly more effort. Let's start with something simple - a Task that will require throwing given objects off the table:</p> <pre><code>class ThrowObjectsOffTableTask(ManipulationTask):\n    def __init__(self, obj_types: List[str], logger: loggers_type | None = None):\n        super().__init__(logger=logger)\n        # obj_types is a list of objects that are subject of the task\n        # In this case, it will mean which objects should be thrown off the table\n        # can be any objects\n        self.obj_types = obj_types\n\n    @property\n    def task_prompt(self) -&gt; str:\n        # define prompt\n        obj_names = \", \".join(obj + \"s\" for obj in self.obj_types).replace(\"_\", \" \")\n        # 0.0 z is the level of table, so any coord below that means it is off the table\n        return f\"Manipulate objects, so that all of the {obj_names} are dropped outside of the table (for example y&lt;-0.75).\"\n\n    def check_if_required_objects_present(self, simulation_config: SceneConfig) -&gt; bool:\n        # Validate if any required objects are present in sim config\n        # if there is not a single object of provided type, there is no point in running\n        # this task of given scene config\n        count = sum(\n            1 for ent in simulation_config.entities if ent.prefab_name in self.obj_types\n        )\n        return count &gt; 1\n\n    def calculate_correct(self, entities: Sequence[Entity]) -&gt; Tuple[int, int]:\n        selected_type_objects = self.filter_entities_by_object_type(\n            entities=entities, object_types=self.obj_types\n        )\n\n        # check how many objects are below table, that will be our metric\n        correct = sum(\n            1 for ent in selected_type_objects if ent.pose.pose.position.z &lt; 0.0\n        )\n\n        incorrect: int = len(selected_type_objects) - correct\n        return correct, incorrect\n\n\n# configure existing Task with different params\ntarget_coords = (0.1, 0.1)\ndisp = 0.1\ntask = ThrowObjectsOffTableTask(\n    obj_types=[\"apple\"],\n)\n\nsuper_scenario = Scenario(\n    task=task, scene_config=scene_config, scene_config_path=path_to_your_config\n)\n</code></pre> <p>As <code>obj_types</code> is parameterizable, it enables various variants of this Task. In combination with a lot of simulation configs available, it means that a single Task can provide dozens of scenarios.</p> <p>Then yo test it simply run:</p> <pre><code>##### Now you can run it in benchmark ##################\nif __name__ == \"__main__\":\n    from pathlib import Path\n\n    from rai_bench import (\n        define_benchmark_logger,\n    )\n    from rai_bench.manipulation_o3de import run_benchmark\n    from rai_bench.utils import get_llm_for_benchmark\n\n    experiment_dir = Path(out_dir=\"src/rai_bench/experiments/custom_task/\")\n\n    experiment_dir.mkdir(parents=True, exist_ok=True)\n    bench_logger = define_benchmark_logger(out_dir=experiment_dir)\n\n    llm = get_llm_for_benchmark(\n        model_name=\"gpt-4o\",\n        vendor=\"openai\",\n    )\n\n    run_benchmark(\n        llm=llm,\n        out_dir=experiment_dir,\n        # use your scenario\n        scenarios=[super_scenario],\n        bench_logger=bench_logger,\n    )\n</code></pre> <p>Congratulations, you just created and launched your first Scenario from scratch!</p>"},{"location":"tutorials/benchmarking/#tool-calling-tasks","title":"Tool Calling Tasks","text":"<p>To create a Tool Calling Task, you will need to define Subtasks, Validators, and Task itself. Check the example <code>src/rai_bench/rai_bench/examples/custom_task.py</code>. Let's create a basic task that requires using a tool to receive a message from a specific topic.</p> <pre><code>from typing import List\n\nfrom langchain_core.tools import BaseTool\n\nfrom rai_bench.tool_calling_agent.interfaces import Task, TaskArgs\nfrom rai_bench.tool_calling_agent.mocked_tools import (\n    MockGetROS2TopicsNamesAndTypesTool,\n    MockReceiveROS2MessageTool,\n)\nfrom rai_bench.tool_calling_agent.subtasks import (\n    CheckArgsToolCallSubTask,\n)\nfrom rai_bench.tool_calling_agent.validators import (\n    OrderedCallsValidator,\n)\n\n\n# This Task will check if robot can receive msessage from specified topic\nclass GetROS2RobotPositionTask(Task):\n    complexity = \"easy\"\n    type = \"custom\"\n\n    @property\n    def available_tools(self) -&gt; List[BaseTool]:\n        # define topics that will be seen by agent\n        TOPICS = [\n            \"/robot_position\",\n            \"/attached_collision_object\",\n            \"/clock\",\n            \"/collision_object\",\n        ]\n\n        TOPICS_STRING = [\n            \"topic: /attached_collision_object\\ntype: moveit_msgs/msg/AttachedCollisionObject\\n\",\n            \"topic: /clock\\ntype: rosgraph_msgs/msg/Clock\\n\",\n            \"topic: /collision_object\\ntype: moveit_msgs/msg/CollisionObject\\n\",\n            \"topic: /robot_position\\n type: sensor_msgs/msg/RobotPosition\",\n        ]\n        # define which tools will be available for agent\n        return [\n            MockGetROS2TopicsNamesAndTypesTool(\n                mock_topics_names_and_types=TOPICS_STRING\n            ),\n            MockReceiveROS2MessageTool(available_topics=TOPICS),\n        ]\n\n    def get_system_prompt(self) -&gt; str:\n        return \"You are a ROS 2 expert that want to solve tasks. You have access to various tools that allow you to query the ROS 2 system.\"\n\n    def get_base_prompt(self) -&gt; str:\n        return \"Get the position of the robot.\"\n\n    def get_prompt(self) -&gt; str:\n        # Create versions for different levels\n        if self.prompt_detail == \"brief\":\n            return self.get_base_prompt()\n        else:\n            return (\n                f\"{self.get_base_prompt()} \"\n                \"You can discover what topics are currently active.\"\n            )\n\n    @property\n    def optional_tool_calls_number(self) -&gt; int:\n        # Listing topics before getting any message is fine\n        return 1\n\n\n# define subtask\nreceive_robot_pos_subtask = CheckArgsToolCallSubTask(\n    expected_tool_name=\"receive_ros2_message\",\n    expected_args={\"topic\": \"/robot_position\"},\n    expected_optional_args={\n        \"timeout_sec\": int  # if there is not exact value expected, you can pass type\n    },\n)\n# use OrderedCallValidator as there is only 1 subtask to check\ntopics_ord_val = OrderedCallsValidator(subtasks=[receive_robot_pos_subtask])\n\n\n# optionally pass number of extra tool calls\nargs = TaskArgs(extra_tool_calls=0)\nsuper_task = GetROS2RobotPositionTask(validators=[topics_ord_val], task_args=args)\n</code></pre> <p>Then run it with:</p> <pre><code>##### Now you can run it in benchmark ##################\nif __name__ == \"__main__\":\n    from pathlib import Path\n\n    from rai_bench import (\n        define_benchmark_logger,\n    )\n    from rai_bench.tool_calling_agent import (\n        run_benchmark,\n    )\n    from rai_bench.utils import get_llm_for_benchmark\n\n    experiment_dir = Path(\"src/rai_bench/rai_bench/experiments/custom_task\")\n    experiment_dir.mkdir(parents=True, exist_ok=True)\n    bench_logger = define_benchmark_logger(out_dir=experiment_dir)\n\n    super_task.set_logger(bench_logger)\n\n    llm = get_llm_for_benchmark(\n        model_name=\"gpt-4o\",\n        vendor=\"openai\",\n    )\n\n    run_benchmark(\n        llm=llm,\n        out_dir=experiment_dir,\n        tasks=[super_task],\n        bench_logger=bench_logger,\n    )\n</code></pre>"},{"location":"tutorials/create_robots_whoami/","title":"Robot's identity within RAI","text":"<p>RAI Agent needs to understand what kind of robot it is running on. This includes its looks, purpose, ethical code, equipment, capabilities and documentation. To configure RAI for your robot, provide contents for your robot's so called <code>whoami</code> package.</p>"},{"location":"tutorials/create_robots_whoami/#configuration-example-franka-emika-panda-arm","title":"Configuration example - Franka Emika Panda arm","text":"<ol> <li> <p>Setup the repository using quick setup guide</p> </li> <li> <p>Fill in the <code>panda/</code> folder with data:</p> <p>2.1. Save this image into <code>panda/images</code></p> <p>2.2. Save this document in <code>panda/documentation</code></p> <p>2.3. Save this urdf in <code>panda/urdf</code></p> </li> <li> <p>Build the embodiment info using <code>build_whoami.py</code>:</p> <pre><code>python src/rai_whoami/rai_whoami/build_whoami.py panda/ --build-vector-db\n</code></pre> <p>Vector database</p> <p>Building the vector database with cloud vendors might lead to costs. Consider using the local <code>ollama</code> provider for this task. The embedding model can be configured in <code>config.toml</code> (<code>ollama</code> works locally, see docs/setup/vendors.md).</p> </li> <li> <p>Examine the generated files</p> </li> </ol> <p>After running the build command, inspect the generated files in the <code>panda/generated</code> directory. The folder should contain a info.json file containing:</p> <ul> <li><code>rules</code>: List of rules</li> <li><code>capabilities</code>: List of capabilities</li> <li><code>behaviors</code>: List of behaviors</li> <li><code>description</code>: Description of the robot</li> <li><code>images</code>: Base64 encoded images</li> </ul>"},{"location":"tutorials/create_robots_whoami/#testing-ros-2","title":"Testing (ROS 2)","text":"<p>You can test the generated package by using the RAI Whoami services:</p> <ol> <li>Using the RAI Whoami services:</li> </ol> <p>Run the RAI Whoami services:</p> <pre><code>python src/rai_whoami/rai_whoami/scripts/ros2_embodiment_service.py panda/ &amp;\npython src/rai_whoami/rai_whoami/scripts/ros2_vector_store_retrieval_service.py panda/\n</code></pre> <p>With the services running, you can query the robot's identity and vector database:</p> <pre><code># Get robot's identity\nros2 service call /rai_whoami_embodiment_info_service rai_interfaces/srv/EmbodimentInfo\n\n# Query the vector database\nros2 service call /rai_whoami_documentation_service rai_interfaces/srv/VectorStoreRetrieval \"query: 'maximum load'\"\n</code></pre> <p>If your service calls succeed and you can access the embodiment info and vector database, your robot's whoami package has been properly initialized.</p> <ol> <li>Alternatively, you can use the RAI Whoami tools directly in your Python code:</li> </ol> <pre><code>from rai_whoami import EmbodimentInfo\nfrom rai_whoami.tools import QueryDatabaseTool\n\n# Load embodiment info\ninfo = EmbodimentInfo.from_directory(\"panda/generated\")\n\n# Create a system prompt for your LLM\nsystem_prompt = info.to_langchain()\n\n# Use the vector database tool\nquery_tool = QueryDatabaseTool(root_dir=\"panda/generated\")\nquery_tool._run(query=\"maximum load\")\n</code></pre>"},{"location":"tutorials/overview/","title":"RAI Tutorials Overview","text":"<p>This directory contains a collection of tutorials that guide you through various aspects of the RAI (Robot AI) framework. Each tutorial focuses on different components and use cases of the system.</p>"},{"location":"tutorials/overview/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/overview/#1-walkthrough","title":"1. Walkthrough","text":"<p>A step-by-step guide to creating and deploying a custom RAI agent on a ROS 2 enabled robot. This tutorial covers:</p> <ul> <li>Creating a custom RAI Agent from scratch</li> <li>Implementing platform-specific tools for robot control</li> <li>Building an optimized system prompt using rai whoami</li> <li>Deploying and interacting with the agent</li> </ul>"},{"location":"tutorials/overview/#2-create-robots-whoami","title":"2. Create Robot's Whoami","text":"<p>Learn how to configure RAI to understand your robot's identity, including its appearance, purpose, ethical code, equipment, capabilities, and documentation. This tutorial covers:</p> <ul> <li>Setting up the robot's <code>whoami</code> package</li> <li>Building embodiment information</li> <li>Testing the configuration with ROS 2 services</li> <li>Using the whoami tools in Python code</li> </ul>"},{"location":"tutorials/overview/#3-tools","title":"3. Tools","text":"<p>A comprehensive guide to tool development and usage in RAI. This tutorial covers:</p> <ul> <li>Understanding the fundamental concepts of tools in LangChain</li> <li>Creating custom tools using both <code>BaseTool</code> class and <code>@tool</code> decorator</li> <li>Implementing single-modal and multimodal tools</li> <li>Developing ROS 2 specific tools</li> <li>Tool initialization and configuration</li> <li>Using tools in both distributed and local setups</li> </ul>"},{"location":"tutorials/overview/#4-voice-interface","title":"4. Voice Interface","text":"<p>Learn how to implement human-robot interaction through voice commands. This tutorial covers:</p> <ul> <li>Setting up Automatic Speech Recognition (ASR) agent</li> <li>Configuring Text-to-Speech (TTS) agent</li> <li>Running a complete speech-to-speech communication example</li> </ul>"},{"location":"tutorials/overview/#5-benchmarking","title":"5. Benchmarking","text":"<p>Guide to running benchmarks, testing your models, creating new tasks and visualising results. This tutorial covers:</p> <ul> <li>Running benchmarks with predefined tasks.</li> <li>Interpreting and visualizing the results.</li> <li>Creating your own tasks and scenarios!</li> </ul>"},{"location":"tutorials/overview/#getting-started","title":"Getting Started","text":"<p>To get started with RAI, we recommend following these tutorials in the following order:</p> <ol> <li>Begin with the Walkthrough to understand the basic concepts and create your first agent</li> <li>Learn about Tools to understand how to extend your agent's capabilities</li> <li>Configure your robot's identity using Create Robot's Whoami</li> <li>Add voice interaction capabilities using the Voice Interface tutorial</li> <li>Test your models with Benchmarks</li> </ol> <p>Each tutorial includes practical examples and code snippets to help you implement the concepts in your own projects.</p>"},{"location":"tutorials/tools/","title":"Tool use and development","text":"<p>Tools are a fundamental concept in LangChain that allow AI models to interact with external systems and perform specific operations. Think of tools as callable functions that bridge the gap between natural language understanding and system execution.</p> <p>RAI offers a comprehensive set of pre-built tools, including both general-purpose and ROS 2-specific tools here. However, in some cases, you may need to develop custom tools tailored to specific robots or applications. This guide demonstrates how to create custom tools in RAI using the LangChain framework.</p>"},{"location":"tutorials/tools/#how-llms-understand-tools","title":"How LLMs Understand Tools","text":"<p>When an LLM is given access to tools, it needs to understand:</p> <ol> <li>What tools are available</li> <li>What each tool does</li> <li>What inputs each tool accepts</li> </ol> <p>This understanding is achieved through the tool's metadata fields, which are automatically processed by LangChain and made available to the LLM. The LLM uses this information to:</p> <ul> <li>Determine which tool to use for a given task</li> <li>Format the correct input arguments</li> <li>Interpret the tool's output</li> </ul> <p>The key fields that enable this understanding are:</p> <ul> <li><code>name</code>: A unique identifier for the tool, does not have to be equal to the function/class name</li> <li><code>description</code>: A natural language explanation of what the tool does</li> <li><code>args_schema</code>: A structured definition of the tool's input parameters</li> </ul> <p>RAI supports two primary approaches for implementing tools, each with distinct advantages:</p>"},{"location":"tutorials/tools/#basetool-class","title":"<code>BaseTool</code> Class","text":"<ul> <li>Offers full control over tool behavior and lifecycle</li> <li>Allows configuration parameters</li> <li>Supports stateful operations (e.g., maintaining ROS 2 connector instances)</li> </ul>"},{"location":"tutorials/tools/#tool-decorator","title":"<code>@tool</code> Decorator","text":"<ul> <li>Provides a lightweight, functional approach</li> <li>Ideal for stateless operations</li> <li>Minimizes boilerplate code</li> <li>Suited for simple, single-purpose tools</li> </ul> <p>Use the <code>BaseTool</code> class when state management, or extensive configuration is required. Choose the <code>@tool</code> decorator for simple, stateless functionality where conciseness is preferred.</p>"},{"location":"tutorials/tools/#creating-a-custom-tool","title":"Creating a Custom Tool","text":"<p>LangChain tools typically return either a string or a tuple containing a string and an artifact.</p> <p>RAI extends LangChain's tool capabilities by supporting multimodal tools\u2014tools that return not only text but also other content types, such as images, audio, or structured data. This is achieved using a special object called <code>MultimodalArtifact</code> along with a custom <code>ToolRunner</code> class.</p>"},{"location":"tutorials/tools/#single-modal-tool-text-output","title":"Single-Modal Tool (Text Output)","text":"<p>Here's an example of a single-modal tool implemented using class inheritance:</p> <p>Class-based tools</p> <p>Class-based tools provide more control and flexibility compared to function-based tools:</p> <ul> <li>Allow passing additional parameters (e.g., connectors, configuration)</li> <li>Support stateful operations</li> </ul> <pre><code>from langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\nfrom typing import Type\n\n\nclass GrabObjectToolInput(BaseModel):\n    \"\"\"Input schema for the GrabObjectTool.\n\n    This schema defines the expected input parameters for the tool.\n    The Field class provides additional metadata that helps the LLM understand\n    the parameter's purpose and constraints.\n    Fields must include a type annotation.\n    \"\"\"\n    object_name: str = Field(description=\"The name of the object to grab\")\n\n\nclass GrabObjectTool(BaseTool):\n    \"\"\"Tool for grabbing objects using a robot.\n\n    The following fields are crucial for LLM understanding:\n\n    name: A unique identifier that the LLM uses to reference this tool\n    description: A natural language explanation that helps the LLM understand when to use this tool\n    args_schema: Links to the input schema, helping the LLM understand required parameters\n    \"\"\"\n    name: str = \"grab_object\"\n    description: str = \"Grabs a specified object using the robot's manipulator\"\n    args_schema: Type[GrabObjectToolInput] = GrabObjectToolInput\n\n    robot_arm: RobotArm # custom parameter for dependency injection\n\n    def _run(self, object_name: str) -&gt; str:\n        \"\"\"Execute the object grabbing operation.\n\n        The LLM will receive this output and can use it to:\n        1. Determine if the operation was successful\n        2. Extract relevant information\n        3. Decide on next steps\n        \"\"\"\n        try:\n            status = self.robot_arm.grab_object(object_name)\n            return f\"Successfully grabbed object: {object_name}, status: {status}\"\n        except Exception as e:\n            return f\"Failed to grab object: {object_name}, error: {str(e)}\"\n</code></pre> <p>Alternatively, using the <code>@tool</code> decorator for simpler, stateless operations:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef grab_object(object_name: str) -&gt; str:  # function name is equal to name in class API\n    \"\"\"Grabs a specified object using the robot's manipulator.\"\"\"  # equal to description in class API\n    try:\n        status = robot_arm.grab_object(object_name)\n        return f\"Successfully grabbed object: {object_name}, status: {status}\"\n    except Exception as e:\n        return f\"Failed to grab object: {object_name}, error: {str(e)}\"\n</code></pre>"},{"location":"tutorials/tools/#multimodal-tool-text-image-output","title":"Multimodal Tool (Text + Image Output)","text":"<p>RAI supports multimodal tools through the <code>rai.agents.langchain.core.ToolRunner</code> class. These tools must use this runner either directly or via agents such as create_react_runnable to handle multimedia output correctly.</p> <pre><code>from langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\nfrom typing import Type, Tuple\nfrom rai.messages import MultimodalArtifact\n\n\nclass Get360ImageToolInput(BaseModel):\n    \"\"\"Input schema for the Get360ImageTool.\"\"\"\n    topic: str = Field(description=\"The topic name for the 360 image\")\n\n\nclass Get360ImageTool(BaseTool):\n    \"\"\"Tool for retrieving 360-degree images.\"\"\"\n    name: str = \"get_360_image\"\n    description: str = \"Retrieves a 360-degree image from the specified topic\"\n    args_schema: Type[Get360ImageToolInput] = Get360ImageToolInput\n    response_format: str = \"content_and_artifact\"\n\n    def _run(self, topic: str) -&gt; Tuple[str, MultimodalArtifact]:\n        try:\n            image = robot.get_360_image(topic)\n            return \"Successfully retrieved 360 image\", MultimodalArtifact(images=[image])\n        except Exception as e:\n            return f\"Failed to retrieve image: {str(e)}\", MultimodalArtifact(images=[])\n</code></pre>"},{"location":"tutorials/tools/#ros-2-tools","title":"ROS 2 Tools","text":"<p>RAI includes a base class for ROS 2 tools, supporting configuration of readable, writable, and forbidden topics/actions/services, as well as ROS 2 connector.</p> <pre><code>from rai.tools.ros2.base import BaseROS2Tool\nfrom pydantic import BaseModel, Field\nfrom typing import Type, cast\nfrom sensor_msgs.msg import PointCloud2\n\n\nclass GetROS2LidarDataToolInput(BaseModel):\n    \"\"\"Input schema for the GetROS2LidarDataTool.\"\"\"\n    topic: str = Field(description=\"The topic name for the LiDAR data\")\n\n\nclass GetROS2LidarDataTool(BaseROS2Tool):\n    \"\"\"Tool for retrieving and processing LiDAR data.\"\"\"\n    name: str = \"get_ros2_lidar_data\"\n    description: str = \"Retrieves and processes LiDAR data from the specified topic\"\n    args_schema: Type[GetROS2LidarDataToolInput] = GetROS2LidarDataToolInput\n\n    def _run(self, topic: str) -&gt; str:\n        try:\n            lidar_data = self.connector.receive_message(topic)\n            msg = cast(PointCloud2, lidar_data.payload)\n            # Process the LiDAR data\n            return f\"Successfully processed LiDAR data. Detected objects: ...\"\n        except Exception as e:\n            return f\"Failed to process LiDAR data: {str(e)}\"\n</code></pre> For more information on BaseROS2Tool refer to the source code"},{"location":"tutorials/tools/#rai.tools.ros2.base.BaseROS2Tool","title":"<code>rai.tools.ros2.base.BaseROS2Tool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>Base class for all ROS2 tools.</p> <p>Attributes:</p> Name Type Description <code>connector</code> <code>ROS2Connector</code> <p>The connector to the ROS 2 system.</p> <code>readable</code> <code>Optional[List[str]]</code> <p>The topics that can be read. If the list is not provided, all topics can be read.</p> <code>writable</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that can be written. If the list is not provided, all topics/actions/services can be written.</p> <code>forbidden</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that are forbidden to read and write.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>The description of the tool.</p> Source code in <code>rai/tools/ros2/base.py</code> <pre><code>class BaseROS2Tool(BaseTool):\n    \"\"\"\n    Base class for all ROS2 tools.\n\n    Attributes\n    ----------\n    connector : ROS2Connector\n        The connector to the ROS 2 system.\n    readable : Optional[List[str]]\n        The topics that can be read. If the list is not provided, all topics can be read.\n    writable : Optional[List[str]]\n        The names (topics/actions/services) that can be written. If the list is not provided, all topics/actions/services can be written.\n    forbidden : Optional[List[str]]\n        The names (topics/actions/services) that are forbidden to read and write.\n    name : str\n        The name of the tool.\n    description : str\n        The description of the tool.\n    \"\"\"\n\n    connector: ROS2Connector\n    readable: Optional[List[str]] = None\n    writable: Optional[List[str]] = None\n    forbidden: Optional[List[str]] = None\n\n    name: str = \"\"\n    description: str = \"\"\n\n    def is_readable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.readable is None:\n            return True\n        return topic in self.readable\n\n    def is_writable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.writable is None:\n            return True\n        return topic in self.writable\n\n    def _check_permission_and_include(\n        self, name: str, check_readable: bool = True\n    ) -&gt; Tuple[bool, bool, bool]:\n        \"\"\"\n        Check permissions and determine if resource should be included.\n\n        Args:\n            name: Resource name (topic/service/action)\n            check_readable: If False, only checks writable (for services/actions).\n                            If True, checks both readable and writable (for topics).\n\n        Returns:\n            (should_include, is_readable, is_writable)\n        \"\"\"\n        # Skip forbidden resources\n        if self.forbidden is not None and name in self.forbidden:\n            return (False, False, False)\n\n        is_readable_resource = self.is_readable(name) if check_readable else False\n        is_writable_resource = self.is_writable(name)\n\n        # Determine if resource should be included based on whitelist state\n        # If only readable is set: include only if resource is in readable list\n        # If only writable is set: include only if resource is in writable list\n        # If both are set: include if resource is in readable OR writable list\n        # If neither is set: include all resources (except forbidden)\n        should_include = True\n        if check_readable and self.readable is not None and self.writable is not None:\n            # Both whitelists are set, resource must be in at least one\n            should_include = is_readable_resource or is_writable_resource\n        elif check_readable and self.readable is not None:\n            # Only readable whitelist is set, resource must be readable\n            should_include = is_readable_resource\n        elif self.writable is not None:\n            # Only writable whitelist is set, resource must be writable\n            should_include = is_writable_resource\n\n        return (should_include, is_readable_resource, is_writable_resource)\n\n    def _categorize(self, is_readable: bool, is_writable: bool) -&gt; Optional[str]:\n        \"\"\"\n        Categorize resource into readable, writable, or both.\n\n        Returns:\n            Category name: \"readable_and_writable\", \"readable\", \"writable\", or None\n        \"\"\"\n        if is_readable and is_writable:\n            return \"readable_and_writable\"\n        elif is_readable:\n            return \"readable\"\n        elif is_writable:\n            return \"writable\"\n        return None\n</code></pre>"},{"location":"tutorials/tools/#tool-initialization","title":"Tool Initialization","text":"<p>Tools can be initialized with parameters such as a connector, enabling custom configurations for ROS 2 environments.</p> <pre><code>from rai.communication.ros2 import ROS2Connector\nfrom rai.tools.ros2 import (\n    GetROS2ImageTool,\n    GetROS2TopicsNamesAndTypesTool,\n    PublishROS2MessageTool,\n)\n\ndef initialize_tools(connector: ROS2Connector):\n    \"\"\"Initialize and configure ROS 2 tools.\n\n    Returns:\n        list: A list of configured tools.\n    \"\"\"\n    readable_names = [\"/color_image5\", \"/depth_image5\", \"/color_camera_info5\"]\n    forbidden_names = [\"cmd_vel\"]\n    writable_names = [\"/to_human\"]\n\n    return [\n        GetROS2ImageTool(\n            connector=connector, readable=readable_names, forbidden=forbidden_names\n        ),\n        GetROS2TopicsNamesAndTypesTool(\n            connector=connector,\n            readable=readable_names,\n            forbidden=forbidden_names,\n            writable=writable_names,\n        ),\n        PublishROS2MessageTool(\n            connector=connector, writable=writable_names, forbidden=forbidden_names\n        ),\n    ]\n</code></pre>"},{"location":"tutorials/tools/#using-tools-in-a-rai-agent-distributed-setup","title":"Using Tools in a RAI Agent (Distributed Setup)","text":"<pre><code>from rai.agents import ReActAgent\nfrom rai.communication import ROS2Connector, ROS2HRIConnector\nfrom rai.tools.ros2 import ROS2Toolkit\nfrom rai.communication.ros2 import ROS2Context\nfrom rai import AgentRunner\n\n@ROS2Context()\ndef main() -&gt; None:\n    \"\"\"Initialize and run the RAI agent with configured tools.\"\"\"\n    connector = ROS2HRIConnector()\n    ros2_connector = ROS2Connector()\n    agent = ReActAgent(\n        connectors={\"/to_human\": connector},\n        tools=initialize_tools(connector=ros2_connector),\n    )\n    runner = AgentRunner([agent])\n    runner.run_and_wait_for_shutdown()\n\n# Example:\n# ros2 topic pub /from_human rai_interfaces/msg/HRIMessage \"{\\\"text\\\": \\\"What do you see?\\\"}\"\n# ros2 topic echo /to_human rai_interfaces/msg/HRIMessage\n</code></pre>"},{"location":"tutorials/tools/#using-tools-in-langchainlanggraph-agent-local-setup","title":"Using Tools in LangChain/LangGraph Agent (Local Setup)","text":"<pre><code>from rai.agents.langchain import create_react_runnable\nfrom langchain.schema import HumanMessage\nfrom rai.communication.ros2 import ROS2Context\n\n@ROS2Context()\ndef main():\n    ros2_connector = ROS2Connector()\n    agent = create_react_runnable(\n        tools=initialize_tools(connector=ros2_connector),\n        system_prompt=\"You are a helpful assistant that can answer questions and help with tasks.\",\n    )\n    state = {'messages': []}\n    while True:\n        input_text = input(\"Enter a prompt: \")\n        state['messages'].append(HumanMessage(content=input_text))\n        response = agent.invoke(state)\n        print(response)\n</code></pre>"},{"location":"tutorials/tools/#related-topics","title":"Related Topics","text":"<ul> <li>Connectors</li> <li>ROS2Connector</li> <li>ROS2HRIConnector</li> </ul>"},{"location":"tutorials/voice_interface/","title":"Human Robot Interface via Voice","text":"<p>RAI provides two ROS enabled agents for Speech to Speech communication.</p>"},{"location":"tutorials/voice_interface/#automatic-speech-recognition-agent","title":"Automatic Speech Recognition Agent","text":"<p>See <code>examples/s2s/asr.py</code> for an example usage.</p> <p>The agent requires configuration of <code>sounddevice</code> and <code>ros2</code> connectors as well as a required voice activity detection (eg. <code>SileroVAD</code>) and transcription model e.g. (<code>LocalWhisper</code>), as well as optionally additional models to decide if the transcription should start (e.g. <code>OpenWakeWord</code>).</p> <p>The Agent publishes information on two topics:</p> <p><code>/from_human</code>: <code>rai_interfaces/msg/HRIMessages</code> - containing transcriptions of the recorded speech</p> <p><code>/voice_commands</code>: <code>std_msgs/msg/String</code> - containing control commands, to inform the consumer if speech is currently detected (<code>{\"data\": \"pause\"}</code>), was detected, and now it stopped (<code>{\"data\": \"play\"}</code>), and if speech was transcribed (<code>{\"data\": \"stop\"}</code>).</p> <p>The Agent utilises sounddevice module to access user's microphone, by default the <code>\"default\"</code> sound device is used. To get information about available sounddevices use:</p> <pre><code>python -c \"import sounddevice; print(sounddevice.query_devices())\"\n</code></pre> <p>The device can be identifed by name and passed to the configuration.</p>"},{"location":"tutorials/voice_interface/#texttospeechagent","title":"TextToSpeechAgent","text":"<p>See <code>examples/s2s/tts.py</code> for an example usage.</p> <p>The agent requires configuration of <code>sounddevice</code> and <code>ros2</code> connectors as well as a required TextToSpeech model (e.g. <code>OpenTTS</code>). The Agent listens for information on two topics:</p> <p><code>/to_human</code>: <code>rai_interfaces/msg/HRIMessages</code> - containing responses to be played to human. These responses are then transcribed and put into the playback queue.</p> <p><code>/voice_commands</code>: <code>std_msgs/msg/String</code> - containing control commands, to pause current playback (<code>{\"data\": \"pause\"}</code>), start/continue playback (<code>{\"data\": \"play\"}</code>), or stop the playback and drop the current playback queue (<code>{\"data\": \"play\"}</code>).</p> <p>The Agent utilises sounddevice module to access user's speaker, by default the <code>\"default\"</code> sound device is used. To get a list of names of available sound devices use:</p> <pre><code>python -c 'import sounddevice as sd; print([x[\"name\"] for x in list(sd.query_devices())])'\n</code></pre> <p>The device can be identifed by name and passed to the configuration.</p>"},{"location":"tutorials/voice_interface/#opentts","title":"OpenTTS","text":"<p>To run OpenTTS (and the example) a docker server containing the model must be running.</p> <p>To start it run:</p> <pre><code>docker run -it -p 5500:5500 synesthesiam/opentts:en --no-espeak\n</code></pre>"},{"location":"tutorials/voice_interface/#running-example","title":"Running example","text":"<p>To run the provided example of S2S configuration with a minimal LLM-based agent run in 4 separate terminals:</p> <pre><code>$ docker run -it -p 5500:5500 synesthesiam/opentts:en --no-espeak\n$ python ./examples/s2s/asr.py\n$ python ./examples/s2s/tts.py\n$ python ./examples/s2s/conversational.py\n</code></pre>"},{"location":"tutorials/walkthrough/","title":"RAI Walkthrough - from zero to a custom agent","text":""},{"location":"tutorials/walkthrough/#overview","title":"Overview","text":"<p>This guide demonstrates how to create and deploy a RAI agent on a ROS 2 enabled robot. You'll learn how to:</p> <ol> <li>Create a new, custom RAI Agent from scratch</li> <li>Implement a platform-specific tool for robot control</li> <li>Build a optimized system prompt using rai whoami</li> <li>Deploy and interact with the agent</li> </ol>"},{"location":"tutorials/walkthrough/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have:</p> <ul> <li>ROS 2 (humble or jazzy) installed and properly set up</li> <li>RAI installed and configured</li> </ul>"},{"location":"tutorials/walkthrough/#creating-a-custom-agent","title":"Creating a Custom Agent","text":"<p>In this section, we'll create a new agent from scratch using the ReAct agent. While pre-built ROS 2-compatible agents exist in RAI, this example will help you understand the underlying architecture and customization options.</p> <p>The underlying ReAct agent combines language models with a set of tools to solve complex problems. Our Agent implementation adds robot status monitoring, which controls agent execution based on the robot's state.</p> PandaAgent implementation<pre><code>from typing import List\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import BaseTool\nfrom rai.agents import BaseAgent\nfrom rai.agents.langchain import (\n    HRICallbackHandler,\n    ReActAgentState,\n    create_react_runnable,\n)\nfrom rai.communication.ros2 import (\n    ROS2Connector,\n    ROS2HRIConnector,\n    ROS2HRIMessage,\n    ROS2Message,\n)\nfrom rai.messages.multimodal import SystemMultimodalMessage\n\n\nclass PandaAgent(BaseAgent):\n    def __init__(\n        self,\n        connector: ROS2Connector,\n        tools: List[BaseTool],\n        system_prompt: SystemMultimodalMessage,\n        input_topic: str,\n        output_topic: str,\n    ):\n        super().__init__()\n        self.connector = connector\n\n        # Initialize the ROS 2 HRI (Human-Robot Interface) connector\n        # This handles bidirectional communication with human-facing topics\n        self.hri_connector = ROS2HRIConnector()\n\n        # Create a ReAct agent with the provided tools and system prompt\n        self.agent = create_react_runnable(\n            tools=tools,\n            system_prompt=system_prompt,\n        )\n        # Initialize the agent's state with an empty message history\n        self.state = ReActAgentState(messages=[])\n\n        # Set up the callback handler to route agent outputs to ROS 2 topics\n        self.callback_handler = HRICallbackHandler(\n            connectors={output_topic: self.hri_connector},\n        )\n\n        # Register the input callback for distributed operation\n        # For simpler setups, you can directly call self.agent.invoke() in a loop\n        self.hri_connector.register_callback(input_topic, self.input_callback)\n\n        # Monitor the robot's status topic to control agent execution\n        # The agent will only process messages when the arm is ready\n        self.arm_ready = False\n        self.connector.register_callback(\"/panda_status\", self.monitor_status_topic)\n\n    def monitor_status_topic(self, msg: ROS2Message):\n        \"\"\"Update the arm readiness status based on the status message.\"\"\"\n        if msg.payload.status == \"ready\":\n            self.arm_ready = True\n        else:\n            self.arm_ready = False\n\n    def input_callback(self, message: ROS2HRIMessage):\n        \"\"\"Process incoming messages when the arm is ready.\"\"\"\n        if self.arm_ready:\n            self.state[\"messages\"].append(message.to_langchain())\n            self.agent.invoke(\n                self.state, config=RunnableConfig(callbacks=[self.callback_handler])\n            )\n        else:\n            self.logger.warning(\"Arm is not ready, skipping the message\")\n\n    def run(self):\n        \"\"\"Main execution loop for the agent.\n\n        This implementation does not require any background processing, so the method is intentionally left empty.\n        \"\"\"\n        pass\n\n    def stop(self):\n        \"\"\"Clean up resources and shut down connections.\"\"\"\n        self.connector.shutdown()\n        self.hri_connector.shutdown()\n</code></pre>"},{"location":"tutorials/walkthrough/#implementing-platform-specific-tools","title":"Implementing Platform-Specific Tools","text":"<p>This section demonstrates how to implement a custom tool for safely shutting down the robot arm. The tool showcases how to integrate platform-specific functionality into the RAI framework.</p> <p>For comprehensive information about tool implementation in RAI, refer to the tools documentation.</p> Tools specific to the robot<pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel, Field\nfrom rai.communication.ros2 import ROS2Message\nfrom rai.tools.ros2.base import BaseROS2Tool\n\n\n# Define the input schema for the tool\nclass ShutdownArmToolInput(BaseModel):\n    level: Literal[\"soft\", \"hard\"] = Field(\n        default=\"soft\",\n        description=\"The level of shutdown to perform\",\n    )\n\n\n# Define the tool\nclass ShutdownArmTool(BaseROS2Tool):\n    \"\"\"Tool for safely shutting down the robot arm.\"\"\"\n    name: str = \"shutdown_arm\"\n    description: str = \"Shutdown the arm\"\n    args_schema: Type[ShutdownArmToolInput] = ShutdownArmToolInput\n\n    def _run(self, level: Literal[\"soft\", \"hard\"]) -&gt; str:\n        \"\"\"Execute the arm shutdown command.\"\"\"\n        response = self.connector.service_call(\n            ROS2Message(\n                payload={\"level\": level}\n            ),\n            target=\"/panda_arm/shutdown\",\n            msg_type=\"rai_interfaces/ShutdownArm\",\n        )\n        if response.payload.success:\n            return \"Arm shutdown successful\"\n        else:\n            return \"Arm shutdown failed\"\n</code></pre>"},{"location":"tutorials/walkthrough/#building-the-system-prompt","title":"Building the System Prompt","text":"<p>The <code>rai whoami</code> utility generates a system prompt based on your robot's specifications. It requires a directory containing:</p> <ul> <li>Documentation</li> <li>Images</li> <li>URDF files</li> </ul> <p>Directories are optional</p> <p>While each directory is optional, providing comprehensive information about your robot will result in a more accurate and effective system prompt.</p> <p>For extended guide on <code>rai whoami</code> see Create Robot's Whoami</p>"},{"location":"tutorials/walkthrough/#setting-up-the-robot-directory","title":"Setting Up the Robot Directory","text":"<p>Create a <code>panda/</code> directory with the following structure:</p> <ol> <li> <p>Images</p> <ul> <li>Save this image in <code>panda/images</code></li> </ul> </li> <li> <p>Documentation</p> <ul> <li>Save   this document   in <code>panda/documentation</code></li> </ul> </li> <li> <p>URDF</p> <ul> <li>Save   this URDF   in <code>panda/urdf</code></li> </ul> </li> </ol>"},{"location":"tutorials/walkthrough/#building-the-whoami","title":"Building the Whoami","text":"<p>Run the following command to build the whoami:</p> <pre><code>python src/rai_whoami/rai_whoami/build_whoami.py panda/ --build-vector-db\n</code></pre>"},{"location":"tutorials/walkthrough/#running-the-agent","title":"Running the Agent","text":"<pre><code>from rai_whoami.models import EmbodimentInfo\nfrom rai.agents import wait_for_shutdown\nfrom rai.communication.ros2 import ROS2Context, ROS2Connector\nfrom rai.tools.ros2 import ROS2Toolkit\n\n\n@ROS2Context()  # Initializes ROS 2 context and ensures proper cleanup on exit\ndef main():\n    # Load the robot's embodiment information from the whoami directory\n    whoami = EmbodimentInfo.from_directory(\"panda/\")\n    connector = ROS2Connector()\n\n    # Initialize tools with ROS 2 communication capabilities\n    # BaseROS2Tools require a ROS2Connector instance for communication\n    tools = [\n        ShutdownArmTool(connector=connector),\n        *ROS2Toolkit(connector=connector).get_tools(),\n    ]\n\n    # Create and configure the agent with all necessary components\n    agent = PandaAgent(\n        connector=connector,\n        tools=tools,\n        system_prompt=whoami.to_langchain(),\n        input_topic=\"/from_human\",\n        output_topic=\"/to_human\",\n    )\n\n    # Start the agent and wait for shutdown signal (Ctrl+C)\n    wait_for_shutdown([agent])\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/walkthrough/#communicating-with-the-agent","title":"Communicating with the Agent","text":"<p>The agent exposes two main communication channels:</p> <ol> <li> <p>Sending Commands</p> <pre><code>ros2 topic pub /from_human rai_interfaces/msg/HRIMessage \"{\\\"text\\\": \\\"Move the arm to 0, 0, 0?\\\"}\"\n</code></pre> </li> <li> <p>Receiving Responses</p> <pre><code>ros2 topic echo /to_human rai_interfaces/msg/HRIMessage\n</code></pre> </li> </ol>"}]}