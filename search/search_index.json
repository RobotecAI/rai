{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAI introduction","text":"<p>RAI is a flexible agentic framework for developing and deploying Embodied AI features for your robots.</p> <p>RAI enables you to control your robots using natural language and helps them perceive and understand the world with AI. It integrates seamlessly with your current robotics stack to enhance your robots' capabilities.</p> <p></p>"},{"location":"#rai-framework","title":"RAI Framework","text":"<p>The RAI Framework provides a comprehensive, end-to-end solution for developing and deploying sophisticated AI-powered robotic systems. It supports the full lifecycle of embodied AI development, from initial configuration and testing to deployment and continuous improvement.</p> <p>Our suite of integrated packages enables developers to seamlessly transition from concept to production, offering:</p> <ul> <li>Complete Development Lifecycle: From initial agent development to deployment</li> <li>Modular Architecture: Choose and combine components based on your specific needs</li> <li>Production-Ready Tools: Enterprise-grade packages for simulation, testing, and deployment</li> <li>Extensible Platform: Easy integration with existing robotics infrastructure and custom solutions</li> <li>Advanced Human-Robot Interaction: Through text, speech, and multimodal interfaces</li> <li>Rich Multimodal Capabilities: Seamless integration of voice, vision, and sensor data with real-time processing of multiple input/output streams, native handling of diverse data types, and unified multi-sensory perception and action framework.</li> </ul> <p>The framework's components work in perfect harmony to deliver a robust foundation for your robotics projects:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive into RAI? Start with a quick-setup guide.</p> <p>Here are two ways to begin your journey:</p>"},{"location":"#option-1-try-our-demos","title":"Option 1: Try Our Demos","text":"<p>Experience RAI in action through our interactive demos. These showcase real-world applications across different robotic platforms:</p> <ul> <li>\ud83e\udd16 Manipulation Tasks - Watch RAI control a Franka Panda arm using natural language</li> <li>\ud83d\ude97 Autonomous Navigation - Explore RAI's capabilities with the ROSbot XL platform</li> <li>\ud83d\ude9c Agricultural Robotics - See how RAI handles complex decision-making in orchard environments</li> </ul>"},{"location":"#option-2-build-your-own-solution","title":"Option 2: Build Your Own Solution","text":"<p>Follow our comprehensive walkthrough to:</p> <ul> <li>Deploy RAI on your robot and enable natural language interactions</li> <li>Extend the framework with custom tools and capabilities</li> <li>Implement complex, multi-step tasks using RAI's advanced reasoning</li> </ul>"},{"location":"#communication-protocols","title":"Communication Protocols","text":"<p>RAI provides first-class support for ROS 2 Humble and Jazzy distributions. While ROS 2 serves as our Tier 1 communication protocol, RAI's architecture includes a powerful abstraction layer that:</p> <ul> <li>Simplifies communication across different networks and protocols</li> <li>Enables seamless integration with various communication backends</li> <li>Allows for future protocol extensions while maintaining a consistent interface</li> </ul> <p>This design philosophy means that while RAI is fully compatible with ROS 2, most of its features can be utilized independently of the ROS 2 environment. The framework's modular architecture makes it suitable not only for different robotic platforms but also for non-robotic applications, offering flexibility in deployment across various domains.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>You are welcome to contribute to RAI! Please see our Contribution Guide.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find our work helpful for your research, please consider citing the following BibTeX entry.</p> <pre><code>@misc{rachwa\u01422025raiflexibleagentframework,\n      title={RAI: Flexible Agent Framework for Embodied AI},\n      author={Kajetan Rachwa\u0142 and Maciej Majek and Bart\u0142omiej Boczek and Kacper D\u0105browski and Pawe\u0142 Liberadzki and Adam D\u0105browski and Maria Ganzha},\n      year={2025},\n      eprint={2505.07532},\n      archivePrefix={arXiv},\n      primaryClass={cs.MA},\n      url={https://arxiv.org/abs/2505.07532},\n}\n</code></pre> <p>Want to know more?</p> RAI at ROSCon 2024 <p></p> RAI demos at ROSCon 2024 <p></p>"},{"location":"#community","title":"Community","text":""},{"location":"#embodied-ai-community-group","title":"Embodied AI Community Group","text":"<p>RAI is one of the main projects in focus of the Embodied AI Community Group. If you would like to join the next meeting, look for it in the ROS Community Calendar.</p>"},{"location":"#rai-faq","title":"RAI FAQ","text":"<p>Please take a look at FAQ.</p>"},{"location":"API_documentation/overview/","title":"RAI API Documentation","text":""},{"location":"API_documentation/overview/#introduction","title":"Introduction","text":"<p>rai core provides a comprehensive set of tools and components for developing intelligent robotic systems powered by multimodal LLMs. This package bridges the gap between advanced AI capabilities and robotic platforms, enabling natural language understanding, reasoning, and multimodal interactions in robotic applications.</p>"},{"location":"API_documentation/overview/#core-components","title":"Core Components","text":"<p>RAI consists of several key components that work together to create intelligent robotic systems:</p> Component Description Agents Agents are the central components that encapsulate specific functionalities and behaviors. Connectors Connectors provide a unified way to interact with various communication systems e.g., ROS 2. Aggregators Aggregators collect and process messages from various sources, transforming them into summarized or analyzed information. Runners Manage the lifecycle of agents. <p>On top of that, RAI implements two major integrations: ROS 2 and LangChain.</p> Component Description ROS 2 Integration RAI provides a set of tools to interact with ROS 2. LangChain Integration RAI leverages LangChain to bridge the gap between large language models and robotic systems. RAI extends LangChain with seamless multimodal message support."},{"location":"API_documentation/overview/#getting-started","title":"Getting Started","text":"<p>For practical examples and tutorials on using RAI, refer to the tutorials section. The API documentation provides detailed information about each component, its purpose, and usage patterns.</p>"},{"location":"API_documentation/overview/#best-practices","title":"Best Practices","text":"<p>When working with RAI:</p> <ol> <li>Design agents with clear responsibilities and interfaces</li> <li>Use appropriate connectors for your target platforms</li> <li>Leverage aggregators to process complex sensor data</li> <li>Follow established patterns for tool development</li> <li>Consider performance implications for real-time robotic applications</li> </ol>"},{"location":"API_documentation/agents/overview/","title":"Agents","text":""},{"location":"API_documentation/agents/overview/#overview","title":"Overview","text":"<p>Agents in RAI are modular components that encapsulate specific functionalities and behaviors. They follow a consistent interface defined by the <code>BaseAgent</code> class and can be combined to create complex robotic systems.</p>"},{"location":"API_documentation/agents/overview/#baseagent","title":"BaseAgent","text":"<p><code>BaseAgent</code> is the abstract base class for all agent implementations in the RAI framework. It defines the minimal interface that all agents must implement while providing common functionality like logging.</p>"},{"location":"API_documentation/agents/overview/#class-definition","title":"Class Definition","text":"BaseAgent class definition"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent","title":"<code>rai.agents.base.BaseAgent</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>rai/agents/base.py</code> <pre><code>class BaseAgent(ABC):\n    def __init__(self):\n        \"\"\"Initializes a new agent instance and sets up logging with the class name.\"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def run(self):\n        \"\"\"Starts the agent's main execution loop.\n        In some cases, concrete run implementation may not be needed.\n        In that case use pass as a placeholder.\"\"\"\n        pass\n\n    @abstractmethod\n    def stop(self):\n        \"\"\"Gracefully terminates the agent's execution and cleans up resources.\"\"\"\n        pass\n</code></pre>"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new agent instance and sets up logging with the class name.</p> Source code in <code>rai/agents/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a new agent instance and sets up logging with the class name.\"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Starts the agent's main execution loop. In some cases, concrete run implementation may not be needed. In that case use pass as a placeholder.</p> Source code in <code>rai/agents/base.py</code> <pre><code>@abstractmethod\ndef run(self):\n    \"\"\"Starts the agent's main execution loop.\n    In some cases, concrete run implementation may not be needed.\n    In that case use pass as a placeholder.\"\"\"\n    pass\n</code></pre>"},{"location":"API_documentation/agents/overview/#rai.agents.base.BaseAgent.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Gracefully terminates the agent's execution and cleans up resources.</p> Source code in <code>rai/agents/base.py</code> <pre><code>@abstractmethod\ndef stop(self):\n    \"\"\"Gracefully terminates the agent's execution and cleans up resources.\"\"\"\n    pass\n</code></pre>"},{"location":"API_documentation/agents/overview/#purpose","title":"Purpose","text":"<p>The <code>BaseAgent</code> class serves as the cornerstone of RAI's agent architecture, establishing a uniform interface for various agent implementations. This enables:</p> <ul> <li>Consistent lifecycle management (starting/stopping)</li> <li>Standardized logging mechanisms</li> <li>Interoperability between different agent types</li> <li>Integration with management utilities like <code>AgentRunner</code></li> </ul>"},{"location":"API_documentation/agents/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Management: Always clean up resources in the <code>stop()</code> method</li> <li>Thread Safety: Use locks for shared resources when implementing multi-threaded agents</li> <li>Error Handling: Implement proper exception handling in long-running agent threads</li> <li>Logging: Use the provided <code>self.logger</code> for consistent logging</li> <li>Graceful Shutdown: Handle interruptions and cleanup properly</li> </ol>"},{"location":"API_documentation/agents/overview/#architecture","title":"Architecture","text":"<p>In the RAI framework, agents typically interact with:</p> <ul> <li>Connectors: For communication (ROS2, audio devices, etc.)</li> <li>Aggregators: For processing and summarizing input data</li> <li>Models: For AI capabilities (LLMs, vision models, speech recognition)</li> <li>Tools: For implementing specific actions an agent can take</li> </ul>"},{"location":"API_documentation/agents/overview/#see-also","title":"See Also","text":"<ul> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/","title":"ROS2 Aggregators","text":"<p>RAI provides several specialized aggregators for ROS2 messages:</p> Aggregator Description Example Usage <code>ROS2LogsAggregator</code> Processes ROS2 log messages, removing duplicates while maintaining order <code>aggregator = ROS2LogsAggregator()</code> <code>ROS2GetLastImageAggregator</code> Returns the most recent image from the buffer as a base64-encoded string <code>aggregator = ROS2GetLastImageAggregator()</code> <code>ROS2ImgVLMDescriptionAggregator</code> Uses a Vision Language Model to analyze and describe the most recent image <code>aggregator = ROS2ImgVLMDescriptionAggregator(llm=chat_model)</code> <code>ROS2ImgVLMDiffAggregator</code> Compares multiple images (first, middle, and last) to identify changes over time <code>aggregator = ROS2ImgVLMDiffAggregator(llm=chat_model)</code>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/#usage-in-state-based-agents","title":"Usage in State-Based Agents","text":"<p>Aggregators are typically used in state-based agents to maintain and update the agent's state:</p> <pre><code>config = StateBasedConfig(\n    aggregators={\n        (\"/camera/camera/color/image_raw\", \"sensor_msgs/msg/Image\"): [\n            ROS2ImgVLMDiffAggregator()\n        ],\n        \"/rosout\": [\n            ROS2LogsAggregator()\n        ]\n    }\n)\n\nagent = ROS2StateBasedAgent(\n    config=config,\n    target_connectors={\"to_human\": hri_connector},\n    tools=tools\n)\n</code></pre>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/#direct-registration-via-ros2connector","title":"Direct Registration via ROS2Connector","text":"<p>Aggregators can also be registered directly with a connector using the <code>register_callback</code> method. This allows for more flexible message processing outside of state-based agents:</p> <pre><code># Create a connector\nconnector = ROS2Connector()\n\n# Create an aggregator\nimage_aggregator = ROS2GetLastImageAggregator()\n\n# Register the aggregator as a callback for a specific topic\nconnector.register_callback(\n    topic=\"/camera/camera/color/image_raw\",\n    msg_type=\"sensor_msgs/msg/Image\",\n    callback=image_aggregator\n)\n\n# The aggregator will now process all messages received on the topic\n# You can retrieve the aggregated result at any time\naggregated_message = image_aggregator.get()\n</code></pre> <p>This approach is useful when you need to:</p> <ul> <li>Process messages from specific topics independently</li> <li>Combine multiple aggregators for the same topic</li> <li>Use aggregators in non-state-based agents</li> <li>Have more control over when aggregation occurs</li> </ul>"},{"location":"API_documentation/aggregators/ROS_2_Aggregators/#see-also","title":"See Also","text":"<ul> <li>Aggregators Overview: For more information on the base aggregator class</li> <li>Agents: For more information on the different types of agents in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/aggregators/overview/","title":"Aggregators","text":""},{"location":"API_documentation/aggregators/overview/#overview","title":"Overview","text":"<p>Aggregators in RAI are components that collect and process messages from various sources, transforming them into summarized or analyzed information. They are particularly useful in state-based agents where they help maintain and update the agent's state through periodic aggregation.</p>"},{"location":"API_documentation/aggregators/overview/#baseaggregator","title":"BaseAggregator","text":"<p><code>BaseAggregator</code> is the abstract base class for all aggregator implementations in the RAI framework. It provides a generic interface for collecting and processing messages of a specific type.</p>"},{"location":"API_documentation/aggregators/overview/#class-definition","title":"Class Definition","text":""},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator","title":"<code>rai.aggregators.base.BaseAggregator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Interface for aggregators.</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>class BaseAggregator(ABC, Generic[T]):\n    \"\"\"\n    Interface for aggregators.\n    \"\"\"\n\n    def __init__(self, max_size: int | None = None) -&gt; None:\n        super().__init__()\n        self._buffer: Deque[T] = deque()\n        self.max_size = max_size\n\n    def __call__(self, msg: T) -&gt; None:\n        if self.max_size is not None and len(self._buffer) &gt;= self.max_size:\n            self._buffer.popleft()\n        self._buffer.append(msg)\n\n    @abstractmethod\n    def get(self) -&gt; BaseMessage | None:\n        \"\"\"Returns the outcome of processing the aggregated message\"\"\"\n        pass\n\n    def clear_buffer(self) -&gt; None:\n        \"\"\"Clears the buffer of messages\"\"\"\n        self._buffer.clear()\n\n    def get_buffer(self) -&gt; List[T]:\n        \"\"\"Returns a copy of the buffer of messages\"\"\"\n        return list(self._buffer)\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(len={len(self._buffer)})\"\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator.clear_buffer","title":"<code>clear_buffer()</code>","text":"<p>Clears the buffer of messages</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>def clear_buffer(self) -&gt; None:\n    \"\"\"Clears the buffer of messages\"\"\"\n    self._buffer.clear()\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator.get","title":"<code>get()</code>  <code>abstractmethod</code>","text":"<p>Returns the outcome of processing the aggregated message</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>@abstractmethod\ndef get(self) -&gt; BaseMessage | None:\n    \"\"\"Returns the outcome of processing the aggregated message\"\"\"\n    pass\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#rai.aggregators.base.BaseAggregator.get_buffer","title":"<code>get_buffer()</code>","text":"<p>Returns a copy of the buffer of messages</p> Source code in <code>rai/aggregators/base.py</code> <pre><code>def get_buffer(self) -&gt; List[T]:\n    \"\"\"Returns a copy of the buffer of messages\"\"\"\n    return list(self._buffer)\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#purpose","title":"Purpose","text":"<p>The <code>BaseAggregator</code> class serves as the foundation for message aggregation in RAI, providing:</p> <ul> <li>A buffer for collecting messages</li> <li>Size management to prevent memory overflow</li> <li>A consistent interface for processing and returning aggregated results</li> <li>Type safety through generics</li> </ul>"},{"location":"API_documentation/aggregators/overview/#usage-in-state-based-agents","title":"Usage in State-Based Agents","text":"<p>Aggregators are typically used in state-based agents to maintain and update the agent's state:</p> <pre><code>config = StateBasedConfig(\n    aggregators={\n        (\"/camera/camera/color/image_raw\", \"sensor_msgs/msg/Image\"): [\n            ROS2ImgVLMDiffAggregator()\n        ],\n        \"/rosout\": [\n            ROS2LogsAggregator()\n        ]\n    }\n)\n\nagent = ROS2StateBasedAgent(\n    config=config,\n    target_connectors={\"to_human\": hri_connector},\n    tools=tools\n)\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#direct-registration-via-connector","title":"Direct Registration via Connector","text":"<p>Aggregators can also be registered directly with a connector using the <code>register_callback</code> method. This allows for more flexible message processing outside of state-based agents:</p> <pre><code># Create a connector\nconnector = ROS2Connector()\n\n# Create an aggregator\nimage_aggregator = ROS2GetLastImageAggregator()\n\n# Register the aggregator as a callback for a specific topic\nconnector.register_callback(\n    topic=\"/camera/camera/color/image_raw\",\n    msg_type=\"sensor_msgs/msg/Image\",\n    callback=image_aggregator\n)\n\n# The aggregator will now process all messages received on the topic\n# You can retrieve the aggregated result at any time\naggregated_message = image_aggregator.get()\n</code></pre> <p>This approach is useful when you need to:</p> <ul> <li>Process messages from specific topics independently</li> <li>Combine multiple aggregators for the same topic</li> <li>Use aggregators in non-state-based agents</li> <li>Have more control over when aggregation occurs</li> </ul>"},{"location":"API_documentation/aggregators/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Buffer Management: Set appropriate max_size to prevent memory issues</li> <li>Resource Cleanup: Clear buffers when no longer needed</li> <li>Error Handling: Handle empty buffers and processing errors gracefully</li> <li>Type Safety: Use appropriate generic types for message types</li> <li>Performance: Consider the computational cost of aggregation operations</li> </ol>"},{"location":"API_documentation/aggregators/overview/#implementation-example","title":"Implementation Example","text":"<pre><code>class CustomAggregator(BaseAggregator[CustomMessage]):\n    def get(self) -&gt; HumanMessage | None:\n        msgs = self.get_buffer()\n        if not msgs:\n            return None\n\n        # Process messages\n        summary = process_messages(msgs)\n\n        # Clear buffer after processing\n        self.clear_buffer()\n\n        return HumanMessage(content=summary)\n</code></pre>"},{"location":"API_documentation/aggregators/overview/#see-also","title":"See Also","text":"<ul> <li>ROS 2 Aggregators: For more information on the different types of ROS 2 aggregators in RAI</li> <li>Agents: For more information on the different types of agents in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/connectors/ROS_2_Connectors/","title":"ROS2 Connectors","text":"<p>RAI provides robust connectors for interacting with ROS 2 middleware, supporting both standard and human-robot interaction (HRI) message flows.</p> Connector Description Example Usage <code>ROS2Connector</code> Standard connector for generic ROS 2 topics, services, and actions. <code>connector = ROS2Connector()</code> <code>ROS2HRIConnector</code> Connector for multimodal HRI messages over ROS 2, combining ROS2BaseConnector and HRIConnector. <code>hri_connector = ROS2HRIConnector()</code>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#ros2connector","title":"<code>ROS2Connector</code>","text":"<p>The <code>ROS2Connector</code> is the main interface for publishing, subscribing, and calling services/actions in a ROS 2 system. It is a concrete implementation of <code>ROS2BaseConnector</code> for standard ROS 2 messages.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#class-definition","title":"Class definition","text":"ROS2BaseConnector class definition <p>ROS2Connector vs ROS2BaseConnector</p> <p><code>ROS2Connector</code> is a simple alias for <code>ROS2BaseConnector</code>. It exists mainly to provide a more intuitive and consistent class name for users, but does not add any new functionality.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector","title":"<code>rai.communication.ros2.connectors.base.ROS2BaseConnector</code>","text":"<p>               Bases: <code>ROS2ActionMixin</code>, <code>ROS2ServiceMixin</code>, <code>BaseConnector[T]</code></p> <p>ROS2-specific implementation of the BaseConnector.</p> <p>This connector provides functionality for ROS2 communication through topics, services, and actions, as well as TF (Transform) operations.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Name of the ROS2 node. If not provided, generates a unique name with UUID.</p> <code>f'rai_ros2_connector_{str(uuid4())[-12:]}'</code> <code>destroy_subscribers</code> <code>bool</code> <p>Whether to destroy subscribers after receiving a message, by default False.</p> <code>False</code> <code>executor_type</code> <code>Literal['single_threaded', 'multi_threaded']</code> <p>Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".</p> <code>'multi_threaded'</code> <code>use_sim_time</code> <code>bool</code> <p>Whether to use simulation time or system time, by default False.</p> <code>False</code> <p>Methods:</p> Name Description <code>get_topics_names_and_types</code> <p>Get list of available topics and their message types.</p> <code>get_services_names_and_types</code> <p>Get list of available services and their types.</p> <code>get_actions_names_and_types</code> <p>Get list of available actions and their types.</p> <code>send_message</code> <p>Send a message to a specified topic.</p> <code>receive_message</code> <p>Receive a message from a specified topic.</p> <code>wait_for_transform</code> <p>Wait for a transform to become available.</p> <code>get_transform</code> <p>Get the transform between two frames.</p> <code>create_service</code> <p>Create a ROS2 service.</p> <code>create_action</code> <p>Create a ROS2 action server.</p> <code>shutdown</code> <p>Clean up resources and shut down the connector.</p> Notes <p>Threading Model:     The connector creates an executor that runs in a dedicated thread.     This executor processes all ROS2 callbacks and operations asynchronously.</p> <p>Subscriber Lifecycle:     The <code>destroy_subscribers</code> parameter controls subscriber cleanup behavior:     - True: Subscribers are destroyed after receiving a message         - Pros: Better resource utilization         - Cons: Known stability issues (see: https://github.com/ros2/rclpy/issues/1142)     - False (default): Subscribers remain active after message reception         - Pros: More stable operation, avoids potential crashes         - Cons: May lead to memory/performance overhead from inactive subscribers</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>class ROS2BaseConnector(ROS2ActionMixin, ROS2ServiceMixin, BaseConnector[T]):\n    \"\"\"ROS2-specific implementation of the BaseConnector.\n\n    This connector provides functionality for ROS2 communication through topics,\n    services, and actions, as well as TF (Transform) operations.\n\n    Parameters\n    ----------\n    node_name : str, optional\n        Name of the ROS2 node. If not provided, generates a unique name with UUID.\n    destroy_subscribers : bool, optional\n        Whether to destroy subscribers after receiving a message, by default False.\n    executor_type : Literal[\"single_threaded\", \"multi_threaded\"], optional\n        Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".\n    use_sim_time : bool, optional\n        Whether to use simulation time or system time, by default False.\n\n    Methods\n    -------\n    get_topics_names_and_types()\n        Get list of available topics and their message types.\n    get_services_names_and_types()\n        Get list of available services and their types.\n    get_actions_names_and_types()\n        Get list of available actions and their types.\n    send_message(message, target, msg_type, auto_qos_matching=True, qos_profile=None, **kwargs)\n        Send a message to a specified topic.\n    receive_message(source, timeout_sec=1.0, msg_type=None, auto_topic_type=True, **kwargs)\n        Receive a message from a specified topic.\n    wait_for_transform(tf_buffer, target_frame, source_frame, timeout_sec=1.0)\n        Wait for a transform to become available.\n    get_transform(target_frame, source_frame, timeout_sec=5.0)\n        Get the transform between two frames.\n    create_service(service_name, on_request, on_done=None, service_type, **kwargs)\n        Create a ROS2 service.\n    create_action(action_name, generate_feedback_callback, action_type, **kwargs)\n        Create a ROS2 action server.\n    shutdown()\n        Clean up resources and shut down the connector.\n\n    Notes\n    -----\n    Threading Model:\n        The connector creates an executor that runs in a dedicated thread.\n        This executor processes all ROS2 callbacks and operations asynchronously.\n\n    Subscriber Lifecycle:\n        The `destroy_subscribers` parameter controls subscriber cleanup behavior:\n        - True: Subscribers are destroyed after receiving a message\n            - Pros: Better resource utilization\n            - Cons: Known stability issues (see: https://github.com/ros2/rclpy/issues/1142)\n        - False (default): Subscribers remain active after message reception\n            - Pros: More stable operation, avoids potential crashes\n            - Cons: May lead to memory/performance overhead from inactive subscribers\n    \"\"\"\n\n    def __init__(\n        self,\n        node_name: str = f\"rai_ros2_connector_{str(uuid.uuid4())[-12:]}\",\n        destroy_subscribers: bool = False,\n        executor_type: Literal[\"single_threaded\", \"multi_threaded\"] = \"multi_threaded\",\n        use_sim_time: bool = False,\n    ):\n        \"\"\"Initialize the ROS2BaseConnector.\n\n        Parameters\n        ----------\n        node_name : str, optional\n            Name of the ROS2 node. If not provided, generates a unique name with UUID.\n        destroy_subscribers : bool, optional\n            Whether to destroy subscribers after receiving a message, by default False.\n        executor_type : Literal[\"single_threaded\", \"multi_threaded\"], optional\n            Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".\n\n        Raises\n        ------\n        ValueError\n            If an invalid executor type is provided.\n        \"\"\"\n        super().__init__()\n\n        if not rclpy.ok():\n            rclpy.init()\n            self.logger.warning(\n                \"Auto-initializing ROS2, but manual initialization is recommended. \"\n                \"For better control and predictability, call rclpy.init() or ROS2Context before creating this connector.\"\n            )\n        self._executor_type = executor_type\n        self._node = Node(node_name)\n        if use_sim_time:\n            self._node.set_parameters(\n                [Parameter(\"use_sim_time\", Parameter.Type.BOOL, True)]\n            )\n        self._topic_api = ROS2TopicAPI(self._node, destroy_subscribers)\n        self._service_api = ROS2ServiceAPI(self._node)\n        self._actions_api = ROS2ActionAPI(self._node)\n        self._tf_buffer = Buffer(node=self._node)\n        self._tf_listener = TransformListener(self._tf_buffer, self._node)\n\n        self._executor_performance_time_delta = 1.0\n        self._executor_performance_timer = self._node.create_timer(\n            self._executor_performance_time_delta, self._executor_performance_callback\n        )\n        self._performance_warning_threshold_multiplier: Final[float] = 1.1\n        self._available_executors: Final[set[str]] = {\n            \"MultiThreadedExecutor\",\n            \"SingleThreadedExecutor\",\n        }\n        if self._executor_type == \"multi_threaded\":\n            self._executor = MultiThreadedExecutor()\n        elif self._executor_type == \"single_threaded\":\n            self._executor = SingleThreadedExecutor()\n        else:\n            raise ValueError(f\"Invalid executor type: {self._executor_type}\")\n\n        self._executor.add_node(self._node)\n        self._thread = threading.Thread(target=self._executor.spin)\n        self._thread.start()\n        self.last_executor_performance_time = time.time()\n\n        # cache for last received messages\n        self.last_msg: Dict[str, T] = {}\n\n    def _executor_performance_callback(self) -&gt; None:\n        \"\"\"Monitor executor performance and log warnings if it falls behind schedule.\n\n        This callback checks if the executor is running slower than expected and logs\n        a warning with suggestions for alternative executors if performance issues\n        are detected.\n        \"\"\"\n        current_time = time.time()\n        time_behind = (\n            current_time\n            - self.last_executor_performance_time\n            - self._executor_performance_time_delta\n        )\n        threshold = (\n            self._executor_performance_time_delta\n            * self._performance_warning_threshold_multiplier\n        )\n\n        if time_behind &gt; threshold:\n            alternative_executors = self._available_executors - {\n                self._executor.__class__.__name__\n            }\n\n            self.logger.warning(\n                f\"{self._executor.__class__.__name__} is {time_behind:.2f} seconds behind. \"\n                f\"If you see this message frequently, consider switching to {', '.join(alternative_executors)}.\"\n            )\n            self.last_executor_performance_time = current_time\n        else:\n            self.last_executor_performance_time = current_time\n\n    def _last_message_callback(self, source: str, msg: T):\n        \"\"\"Store the last received message for a given source.\n\n        Parameters\n        ----------\n        source : str\n            The topic source identifier.\n        msg : T\n            The received message.\n        \"\"\"\n        self.last_msg[source] = msg\n\n    def get_topics_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n        \"\"\"Get list of available topics and their message types.\n\n        Returns\n        -------\n        List[Tuple[str, List[str]]]\n            List of tuples containing topic names and their corresponding message types.\n        \"\"\"\n        return self._topic_api.get_topic_names_and_types()\n\n    def get_services_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n        \"\"\"Get list of available services and their types.\n\n        Returns\n        -------\n        List[Tuple[str, List[str]]]\n            List of tuples containing service names and their corresponding types.\n        \"\"\"\n        return self._service_api.get_service_names_and_types()\n\n    def get_actions_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n        \"\"\"Get list of available actions and their types.\n\n        Returns\n        -------\n        List[Tuple[str, List[str]]]\n            List of tuples containing action names and their corresponding types.\n        \"\"\"\n        return self._actions_api.get_action_names_and_types()\n\n    def send_message(\n        self,\n        message: T,\n        target: str,\n        *,\n        msg_type: str,\n        auto_qos_matching: bool = True,\n        qos_profile: Optional[QoSProfile] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Send a message to a specified topic.\n\n        Parameters\n        ----------\n        message : T\n            The message to send.\n        target : str\n            The target topic name.\n        msg_type : str\n            The ROS2 message type.\n        auto_qos_matching : bool, optional\n            Whether to automatically match QoS profiles, by default True.\n        qos_profile : Optional[QoSProfile], optional\n            Custom QoS profile to use, by default None.\n        **kwargs : Any\n            Additional keyword arguments.\n        \"\"\"\n        self._topic_api.publish(\n            topic=target,\n            msg_content=message.payload,\n            msg_type=msg_type,\n            auto_qos_matching=auto_qos_matching,\n            qos_profile=qos_profile,\n        )\n\n    def general_callback_preprocessor(self, message: Any) -&gt; T:\n        \"\"\"Preprocess a raw ROS2 message into a connector message.\n\n        Parameters\n        ----------\n        message : Any\n            The raw ROS2 message.\n\n        Returns\n        -------\n        T\n            The preprocessed message.\n        \"\"\"\n        return self.T_class(payload=message, metadata={\"msg_type\": str(type(message))})\n\n    def register_callback(\n        self,\n        source: str,\n        callback: Callable[[T | Any], None],\n        raw: bool = False,\n        *,\n        msg_type: Optional[str] = None,\n        qos_profile: Optional[QoSProfile] = None,\n        auto_qos_matching: bool = True,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Register a callback for a topic.\n\n        Parameters\n        ----------\n        source : str\n            The topic to subscribe to.\n        callback : Callable[[T | Any], None]\n            The callback function to execute when a message is received.\n        raw : bool, optional\n            Whether to pass raw messages to the callback, by default False.\n        msg_type : Optional[str], optional\n            The ROS2 message type, by default None.\n        qos_profile : Optional[QoSProfile], optional\n            Custom QoS profile to use, by default None.\n        auto_qos_matching : bool, optional\n            Whether to automatically match QoS profiles, by default True.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The callback ID.\n        \"\"\"\n        exists = self._topic_api.subscriber_exists(source)\n        if not exists:\n            self._topic_api.create_subscriber(\n                topic=source,\n                msg_type=msg_type,\n                callback=partial(self.general_callback, source),\n                qos_profile=qos_profile,\n                auto_qos_matching=auto_qos_matching,\n            )\n        return super().register_callback(source, callback, raw=raw)\n\n    def receive_message(\n        self,\n        source: str,\n        timeout_sec: float = 1.0,\n        *,\n        msg_type: Optional[str] = None,\n        qos_profile: Optional[QoSProfile] = None,\n        auto_qos_matching: bool = True,\n        **kwargs: Any,\n    ) -&gt; T:\n        \"\"\"Receive a message from a topic.\n\n        Parameters\n        ----------\n        source : str\n            The topic to receive from.\n        timeout_sec : float, optional\n            Timeout in seconds, by default 1.0.\n        msg_type : Optional[str], optional\n            The ROS2 message type, by default None.\n        qos_profile : Optional[QoSProfile], optional\n            Custom QoS profile to use, by default None.\n        auto_qos_matching : bool, optional\n            Whether to automatically match QoS profiles, by default True.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        T\n            The received message.\n\n        Raises\n        ------\n        TimeoutError\n            If no message is received within the timeout period.\n        \"\"\"\n        if self._topic_api.subscriber_exists(source):\n            # trying to hit cache first\n            if source in self.last_msg:\n                if self.last_msg[source].timestamp &gt; time.time() - timeout_sec:\n                    return self.last_msg[source]\n        else:\n            self._topic_api.create_subscriber(\n                topic=source,\n                callback=partial(self.general_callback, source),\n                msg_type=msg_type,\n                qos_profile=qos_profile,\n                auto_qos_matching=auto_qos_matching,\n            )\n            self.register_callback(source, partial(self._last_message_callback, source))\n\n        start_time = time.time()\n        # wait for the message to be received\n        while time.time() - start_time &lt; timeout_sec:\n            if source in self.last_msg:\n                return self.last_msg[source]\n            time.sleep(0.1)\n        else:\n            raise TimeoutError(\n                f\"Message from {source} not received in {timeout_sec} seconds\"\n            )\n\n    @staticmethod\n    def wait_for_transform(\n        tf_buffer: Buffer,\n        target_frame: str,\n        source_frame: str,\n        timeout_sec: float = 1.0,\n    ) -&gt; bool:\n        \"\"\"Wait for a transform to become available.\n\n        Parameters\n        ----------\n        tf_buffer : Buffer\n            The TF buffer to check.\n        target_frame : str\n            The target frame.\n        source_frame : str\n            The source frame.\n        timeout_sec : float, optional\n            Timeout in seconds, by default 1.0.\n\n        Returns\n        -------\n        bool\n            True if the transform is available, False otherwise.\n        \"\"\"\n        start_time = time.time()\n        while time.time() - start_time &lt; timeout_sec:\n            if tf_buffer.can_transform(target_frame, source_frame, rclpy.time.Time()):\n                return True\n            time.sleep(0.1)\n        return False\n\n    def get_transform(\n        self,\n        target_frame: str,\n        source_frame: str,\n        timeout_sec: float = 5.0,\n    ) -&gt; TransformStamped:\n        \"\"\"Get the transform between two frames.\n\n        Parameters\n        ----------\n        target_frame : str\n            The target frame.\n        source_frame : str\n            The source frame.\n        timeout_sec : float, optional\n            Timeout in seconds, by default 5.0.\n\n        Returns\n        -------\n        TransformStamped\n            The transform between the frames.\n\n        Raises\n        ------\n        LookupException\n            If the transform is not available within the timeout period.\n        \"\"\"\n        transform_available = self.wait_for_transform(\n            self._tf_buffer, target_frame, source_frame, timeout_sec\n        )\n        if not transform_available:\n            raise LookupException(\n                f\"Could not find transform from {source_frame} to {target_frame} in {timeout_sec} seconds\"\n            )\n        transform: TransformStamped = self._tf_buffer.lookup_transform(\n            target_frame,\n            source_frame,\n            rclpy.time.Time(),\n            timeout=Duration(seconds=int(timeout_sec)),\n        )\n\n        return transform\n\n    def create_service(\n        self,\n        service_name: str,\n        on_request: Callable[[Any, Any], Any],\n        on_done: Optional[Callable[[Any, Any], Any]] = None,\n        *,\n        service_type: str,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Create a ROS2 service.\n\n        Parameters\n        ----------\n        service_name : str\n            The name of the service.\n        on_request : Callable[[Any, Any], Any]\n            Callback function to handle service requests.\n        on_done : Optional[Callable[[Any, Any], Any]], optional\n            Callback function called when service is terminated, by default None.\n        service_type : str\n            The ROS2 service type.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The service handle.\n        \"\"\"\n        return self._service_api.create_service(\n            service_name=service_name,\n            callback=on_request,\n            service_type=service_type,\n            **kwargs,\n        )\n\n    def create_action(\n        self,\n        action_name: str,\n        generate_feedback_callback: Callable,\n        *,\n        action_type: str,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Create a ROS2 action server.\n\n        Parameters\n        ----------\n        action_name : str\n            The name of the action.\n        generate_feedback_callback : Callable\n            Callback function to generate feedback during action execution.\n        action_type : str\n            The ROS2 action type.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The action handle.\n        \"\"\"\n        return self._actions_api.create_action_server(\n            action_name=action_name,\n            action_type=action_type,\n            execute_callback=generate_feedback_callback,\n            **kwargs,\n        )\n\n    @property\n    def node(self) -&gt; Node:\n        \"\"\"Get the ROS2 node.\n\n        Returns\n        -------\n        Node\n            The ROS2 node instance.\n        \"\"\"\n        return self._node\n\n    def shutdown(self):\n        \"\"\"Shutdown the connector and clean up resources.\n\n        This method:\n        1. Unregisters the TF listener\n        2. Destroys the ROS2 node\n        3. Shuts down the action API\n        4. Shuts down the topic API\n        5. Shuts down the executor\n        6. Joins the executor thread\n        \"\"\"\n        self._tf_listener.unregister()\n        self._node.destroy_node()\n        self._actions_api.shutdown()\n        self._topic_api.shutdown()\n        self._executor.shutdown()\n        self._thread.join()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.node","title":"<code>node</code>  <code>property</code>","text":"<p>Get the ROS2 node.</p> <p>Returns:</p> Type Description <code>Node</code> <p>The ROS2 node instance.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.__init__","title":"<code>__init__(node_name=f'rai_ros2_connector_{str(uuid.uuid4())[-12:]}', destroy_subscribers=False, executor_type='multi_threaded', use_sim_time=False)</code>","text":"<p>Initialize the ROS2BaseConnector.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Name of the ROS2 node. If not provided, generates a unique name with UUID.</p> <code>f'rai_ros2_connector_{str(uuid4())[-12:]}'</code> <code>destroy_subscribers</code> <code>bool</code> <p>Whether to destroy subscribers after receiving a message, by default False.</p> <code>False</code> <code>executor_type</code> <code>Literal['single_threaded', 'multi_threaded']</code> <p>Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".</p> <code>'multi_threaded'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid executor type is provided.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def __init__(\n    self,\n    node_name: str = f\"rai_ros2_connector_{str(uuid.uuid4())[-12:]}\",\n    destroy_subscribers: bool = False,\n    executor_type: Literal[\"single_threaded\", \"multi_threaded\"] = \"multi_threaded\",\n    use_sim_time: bool = False,\n):\n    \"\"\"Initialize the ROS2BaseConnector.\n\n    Parameters\n    ----------\n    node_name : str, optional\n        Name of the ROS2 node. If not provided, generates a unique name with UUID.\n    destroy_subscribers : bool, optional\n        Whether to destroy subscribers after receiving a message, by default False.\n    executor_type : Literal[\"single_threaded\", \"multi_threaded\"], optional\n        Type of executor to use for processing ROS2 callbacks, by default \"multi_threaded\".\n\n    Raises\n    ------\n    ValueError\n        If an invalid executor type is provided.\n    \"\"\"\n    super().__init__()\n\n    if not rclpy.ok():\n        rclpy.init()\n        self.logger.warning(\n            \"Auto-initializing ROS2, but manual initialization is recommended. \"\n            \"For better control and predictability, call rclpy.init() or ROS2Context before creating this connector.\"\n        )\n    self._executor_type = executor_type\n    self._node = Node(node_name)\n    if use_sim_time:\n        self._node.set_parameters(\n            [Parameter(\"use_sim_time\", Parameter.Type.BOOL, True)]\n        )\n    self._topic_api = ROS2TopicAPI(self._node, destroy_subscribers)\n    self._service_api = ROS2ServiceAPI(self._node)\n    self._actions_api = ROS2ActionAPI(self._node)\n    self._tf_buffer = Buffer(node=self._node)\n    self._tf_listener = TransformListener(self._tf_buffer, self._node)\n\n    self._executor_performance_time_delta = 1.0\n    self._executor_performance_timer = self._node.create_timer(\n        self._executor_performance_time_delta, self._executor_performance_callback\n    )\n    self._performance_warning_threshold_multiplier: Final[float] = 1.1\n    self._available_executors: Final[set[str]] = {\n        \"MultiThreadedExecutor\",\n        \"SingleThreadedExecutor\",\n    }\n    if self._executor_type == \"multi_threaded\":\n        self._executor = MultiThreadedExecutor()\n    elif self._executor_type == \"single_threaded\":\n        self._executor = SingleThreadedExecutor()\n    else:\n        raise ValueError(f\"Invalid executor type: {self._executor_type}\")\n\n    self._executor.add_node(self._node)\n    self._thread = threading.Thread(target=self._executor.spin)\n    self._thread.start()\n    self.last_executor_performance_time = time.time()\n\n    # cache for last received messages\n    self.last_msg: Dict[str, T] = {}\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.create_action","title":"<code>create_action(action_name, generate_feedback_callback, *, action_type, **kwargs)</code>","text":"<p>Create a ROS2 action server.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>The name of the action.</p> required <code>generate_feedback_callback</code> <code>Callable</code> <p>Callback function to generate feedback during action execution.</p> required <code>action_type</code> <code>str</code> <p>The ROS2 action type.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The action handle.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def create_action(\n    self,\n    action_name: str,\n    generate_feedback_callback: Callable,\n    *,\n    action_type: str,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Create a ROS2 action server.\n\n    Parameters\n    ----------\n    action_name : str\n        The name of the action.\n    generate_feedback_callback : Callable\n        Callback function to generate feedback during action execution.\n    action_type : str\n        The ROS2 action type.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The action handle.\n    \"\"\"\n    return self._actions_api.create_action_server(\n        action_name=action_name,\n        action_type=action_type,\n        execute_callback=generate_feedback_callback,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.create_service","title":"<code>create_service(service_name, on_request, on_done=None, *, service_type, **kwargs)</code>","text":"<p>Create a ROS2 service.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>The name of the service.</p> required <code>on_request</code> <code>Callable[[Any, Any], Any]</code> <p>Callback function to handle service requests.</p> required <code>on_done</code> <code>Optional[Callable[[Any, Any], Any]]</code> <p>Callback function called when service is terminated, by default None.</p> <code>None</code> <code>service_type</code> <code>str</code> <p>The ROS2 service type.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The service handle.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def create_service(\n    self,\n    service_name: str,\n    on_request: Callable[[Any, Any], Any],\n    on_done: Optional[Callable[[Any, Any], Any]] = None,\n    *,\n    service_type: str,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Create a ROS2 service.\n\n    Parameters\n    ----------\n    service_name : str\n        The name of the service.\n    on_request : Callable[[Any, Any], Any]\n        Callback function to handle service requests.\n    on_done : Optional[Callable[[Any, Any], Any]], optional\n        Callback function called when service is terminated, by default None.\n    service_type : str\n        The ROS2 service type.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The service handle.\n    \"\"\"\n    return self._service_api.create_service(\n        service_name=service_name,\n        callback=on_request,\n        service_type=service_type,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.general_callback_preprocessor","title":"<code>general_callback_preprocessor(message)</code>","text":"<p>Preprocess a raw ROS2 message into a connector message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>The raw ROS2 message.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The preprocessed message.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def general_callback_preprocessor(self, message: Any) -&gt; T:\n    \"\"\"Preprocess a raw ROS2 message into a connector message.\n\n    Parameters\n    ----------\n    message : Any\n        The raw ROS2 message.\n\n    Returns\n    -------\n    T\n        The preprocessed message.\n    \"\"\"\n    return self.T_class(payload=message, metadata={\"msg_type\": str(type(message))})\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_actions_names_and_types","title":"<code>get_actions_names_and_types()</code>","text":"<p>Get list of available actions and their types.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str]]]</code> <p>List of tuples containing action names and their corresponding types.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_actions_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n    \"\"\"Get list of available actions and their types.\n\n    Returns\n    -------\n    List[Tuple[str, List[str]]]\n        List of tuples containing action names and their corresponding types.\n    \"\"\"\n    return self._actions_api.get_action_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_services_names_and_types","title":"<code>get_services_names_and_types()</code>","text":"<p>Get list of available services and their types.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str]]]</code> <p>List of tuples containing service names and their corresponding types.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_services_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n    \"\"\"Get list of available services and their types.\n\n    Returns\n    -------\n    List[Tuple[str, List[str]]]\n        List of tuples containing service names and their corresponding types.\n    \"\"\"\n    return self._service_api.get_service_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_topics_names_and_types","title":"<code>get_topics_names_and_types()</code>","text":"<p>Get list of available topics and their message types.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, List[str]]]</code> <p>List of tuples containing topic names and their corresponding message types.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_topics_names_and_types(self) -&gt; List[Tuple[str, List[str]]]:\n    \"\"\"Get list of available topics and their message types.\n\n    Returns\n    -------\n    List[Tuple[str, List[str]]]\n        List of tuples containing topic names and their corresponding message types.\n    \"\"\"\n    return self._topic_api.get_topic_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.get_transform","title":"<code>get_transform(target_frame, source_frame, timeout_sec=5.0)</code>","text":"<p>Get the transform between two frames.</p> <p>Parameters:</p> Name Type Description Default <code>target_frame</code> <code>str</code> <p>The target frame.</p> required <code>source_frame</code> <code>str</code> <p>The source frame.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds, by default 5.0.</p> <code>5.0</code> <p>Returns:</p> Type Description <code>TransformStamped</code> <p>The transform between the frames.</p> <p>Raises:</p> Type Description <code>LookupException</code> <p>If the transform is not available within the timeout period.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def get_transform(\n    self,\n    target_frame: str,\n    source_frame: str,\n    timeout_sec: float = 5.0,\n) -&gt; TransformStamped:\n    \"\"\"Get the transform between two frames.\n\n    Parameters\n    ----------\n    target_frame : str\n        The target frame.\n    source_frame : str\n        The source frame.\n    timeout_sec : float, optional\n        Timeout in seconds, by default 5.0.\n\n    Returns\n    -------\n    TransformStamped\n        The transform between the frames.\n\n    Raises\n    ------\n    LookupException\n        If the transform is not available within the timeout period.\n    \"\"\"\n    transform_available = self.wait_for_transform(\n        self._tf_buffer, target_frame, source_frame, timeout_sec\n    )\n    if not transform_available:\n        raise LookupException(\n            f\"Could not find transform from {source_frame} to {target_frame} in {timeout_sec} seconds\"\n        )\n    transform: TransformStamped = self._tf_buffer.lookup_transform(\n        target_frame,\n        source_frame,\n        rclpy.time.Time(),\n        timeout=Duration(seconds=int(timeout_sec)),\n    )\n\n    return transform\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.receive_message","title":"<code>receive_message(source, timeout_sec=1.0, *, msg_type=None, qos_profile=None, auto_qos_matching=True, **kwargs)</code>","text":"<p>Receive a message from a topic.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The topic to receive from.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds, by default 1.0.</p> <code>1.0</code> <code>msg_type</code> <code>Optional[str]</code> <p>The ROS2 message type, by default None.</p> <code>None</code> <code>qos_profile</code> <code>Optional[QoSProfile]</code> <p>Custom QoS profile to use, by default None.</p> <code>None</code> <code>auto_qos_matching</code> <code>bool</code> <p>Whether to automatically match QoS profiles, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The received message.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If no message is received within the timeout period.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def receive_message(\n    self,\n    source: str,\n    timeout_sec: float = 1.0,\n    *,\n    msg_type: Optional[str] = None,\n    qos_profile: Optional[QoSProfile] = None,\n    auto_qos_matching: bool = True,\n    **kwargs: Any,\n) -&gt; T:\n    \"\"\"Receive a message from a topic.\n\n    Parameters\n    ----------\n    source : str\n        The topic to receive from.\n    timeout_sec : float, optional\n        Timeout in seconds, by default 1.0.\n    msg_type : Optional[str], optional\n        The ROS2 message type, by default None.\n    qos_profile : Optional[QoSProfile], optional\n        Custom QoS profile to use, by default None.\n    auto_qos_matching : bool, optional\n        Whether to automatically match QoS profiles, by default True.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    T\n        The received message.\n\n    Raises\n    ------\n    TimeoutError\n        If no message is received within the timeout period.\n    \"\"\"\n    if self._topic_api.subscriber_exists(source):\n        # trying to hit cache first\n        if source in self.last_msg:\n            if self.last_msg[source].timestamp &gt; time.time() - timeout_sec:\n                return self.last_msg[source]\n    else:\n        self._topic_api.create_subscriber(\n            topic=source,\n            callback=partial(self.general_callback, source),\n            msg_type=msg_type,\n            qos_profile=qos_profile,\n            auto_qos_matching=auto_qos_matching,\n        )\n        self.register_callback(source, partial(self._last_message_callback, source))\n\n    start_time = time.time()\n    # wait for the message to be received\n    while time.time() - start_time &lt; timeout_sec:\n        if source in self.last_msg:\n            return self.last_msg[source]\n        time.sleep(0.1)\n    else:\n        raise TimeoutError(\n            f\"Message from {source} not received in {timeout_sec} seconds\"\n        )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.register_callback","title":"<code>register_callback(source, callback, raw=False, *, msg_type=None, qos_profile=None, auto_qos_matching=True, **kwargs)</code>","text":"<p>Register a callback for a topic.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The topic to subscribe to.</p> required <code>callback</code> <code>Callable[[T | Any], None]</code> <p>The callback function to execute when a message is received.</p> required <code>raw</code> <code>bool</code> <p>Whether to pass raw messages to the callback, by default False.</p> <code>False</code> <code>msg_type</code> <code>Optional[str]</code> <p>The ROS2 message type, by default None.</p> <code>None</code> <code>qos_profile</code> <code>Optional[QoSProfile]</code> <p>Custom QoS profile to use, by default None.</p> <code>None</code> <code>auto_qos_matching</code> <code>bool</code> <p>Whether to automatically match QoS profiles, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The callback ID.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def register_callback(\n    self,\n    source: str,\n    callback: Callable[[T | Any], None],\n    raw: bool = False,\n    *,\n    msg_type: Optional[str] = None,\n    qos_profile: Optional[QoSProfile] = None,\n    auto_qos_matching: bool = True,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Register a callback for a topic.\n\n    Parameters\n    ----------\n    source : str\n        The topic to subscribe to.\n    callback : Callable[[T | Any], None]\n        The callback function to execute when a message is received.\n    raw : bool, optional\n        Whether to pass raw messages to the callback, by default False.\n    msg_type : Optional[str], optional\n        The ROS2 message type, by default None.\n    qos_profile : Optional[QoSProfile], optional\n        Custom QoS profile to use, by default None.\n    auto_qos_matching : bool, optional\n        Whether to automatically match QoS profiles, by default True.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The callback ID.\n    \"\"\"\n    exists = self._topic_api.subscriber_exists(source)\n    if not exists:\n        self._topic_api.create_subscriber(\n            topic=source,\n            msg_type=msg_type,\n            callback=partial(self.general_callback, source),\n            qos_profile=qos_profile,\n            auto_qos_matching=auto_qos_matching,\n        )\n    return super().register_callback(source, callback, raw=raw)\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.send_message","title":"<code>send_message(message, target, *, msg_type, auto_qos_matching=True, qos_profile=None, **kwargs)</code>","text":"<p>Send a message to a specified topic.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>T</code> <p>The message to send.</p> required <code>target</code> <code>str</code> <p>The target topic name.</p> required <code>msg_type</code> <code>str</code> <p>The ROS2 message type.</p> required <code>auto_qos_matching</code> <code>bool</code> <p>Whether to automatically match QoS profiles, by default True.</p> <code>True</code> <code>qos_profile</code> <code>Optional[QoSProfile]</code> <p>Custom QoS profile to use, by default None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def send_message(\n    self,\n    message: T,\n    target: str,\n    *,\n    msg_type: str,\n    auto_qos_matching: bool = True,\n    qos_profile: Optional[QoSProfile] = None,\n    **kwargs: Any,\n):\n    \"\"\"Send a message to a specified topic.\n\n    Parameters\n    ----------\n    message : T\n        The message to send.\n    target : str\n        The target topic name.\n    msg_type : str\n        The ROS2 message type.\n    auto_qos_matching : bool, optional\n        Whether to automatically match QoS profiles, by default True.\n    qos_profile : Optional[QoSProfile], optional\n        Custom QoS profile to use, by default None.\n    **kwargs : Any\n        Additional keyword arguments.\n    \"\"\"\n    self._topic_api.publish(\n        topic=target,\n        msg_content=message.payload,\n        msg_type=msg_type,\n        auto_qos_matching=auto_qos_matching,\n        qos_profile=qos_profile,\n    )\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.shutdown","title":"<code>shutdown()</code>","text":"<p>Shutdown the connector and clean up resources.</p> <p>This method: 1. Unregisters the TF listener 2. Destroys the ROS2 node 3. Shuts down the action API 4. Shuts down the topic API 5. Shuts down the executor 6. Joins the executor thread</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>def shutdown(self):\n    \"\"\"Shutdown the connector and clean up resources.\n\n    This method:\n    1. Unregisters the TF listener\n    2. Destroys the ROS2 node\n    3. Shuts down the action API\n    4. Shuts down the topic API\n    5. Shuts down the executor\n    6. Joins the executor thread\n    \"\"\"\n    self._tf_listener.unregister()\n    self._node.destroy_node()\n    self._actions_api.shutdown()\n    self._topic_api.shutdown()\n    self._executor.shutdown()\n    self._thread.join()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#rai.communication.ros2.connectors.base.ROS2BaseConnector.wait_for_transform","title":"<code>wait_for_transform(tf_buffer, target_frame, source_frame, timeout_sec=1.0)</code>  <code>staticmethod</code>","text":"<p>Wait for a transform to become available.</p> <p>Parameters:</p> Name Type Description Default <code>tf_buffer</code> <code>Buffer</code> <p>The TF buffer to check.</p> required <code>target_frame</code> <code>str</code> <p>The target frame.</p> required <code>source_frame</code> <code>str</code> <p>The source frame.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds, by default 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transform is available, False otherwise.</p> Source code in <code>rai/communication/ros2/connectors/base.py</code> <pre><code>@staticmethod\ndef wait_for_transform(\n    tf_buffer: Buffer,\n    target_frame: str,\n    source_frame: str,\n    timeout_sec: float = 1.0,\n) -&gt; bool:\n    \"\"\"Wait for a transform to become available.\n\n    Parameters\n    ----------\n    tf_buffer : Buffer\n        The TF buffer to check.\n    target_frame : str\n        The target frame.\n    source_frame : str\n        The source frame.\n    timeout_sec : float, optional\n        Timeout in seconds, by default 1.0.\n\n    Returns\n    -------\n    bool\n        True if the transform is available, False otherwise.\n    \"\"\"\n    start_time = time.time()\n    while time.time() - start_time &lt; timeout_sec:\n        if tf_buffer.can_transform(target_frame, source_frame, rclpy.time.Time()):\n            return True\n        time.sleep(0.1)\n    return False\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#key-features","title":"Key Features","text":"<ul> <li>Manages ROS 2 node lifecycle and threading (via <code>MultiThreadedExecutor</code>, see ROS 2 Executors)</li> <li>Supports topic-based message passing (publish/subscribe, see ROS 2 Topics)</li> <li>Service calls (request/response, see ROS 2 Services)</li> <li>Actions (long-running operations with feedback, see ROS 2 Actions)</li> <li>TF (Transform) operations, see ROS 2 TF</li> <li>Callback registration for asynchronous notifications</li> </ul>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#example-usage","title":"Example Usage","text":"<pre><code>from rai.communication.ros2.connectors import ROS2Connector\n\nconnector = ROS2Connector()\n\n# Send a message to a topic\nconnector.send_message(\n    message=my_msg,  # ROS2Message\n    target=\"/my_topic\",\n    msg_type=\"std_msgs/msg/String\"\n)\n\n# Register a callback for a topic\nconnector.register_callback(\n    source=\"/my_topic\",\n    callback=my_callback,\n    msg_type=\"std_msgs/msg/String\"\n)\n\n# Call a service\nresponse = connector.service_call(\n    message=my_request_msg,\n    target=\"/my_service\",\n    msg_type=\"my_package/srv/MyService\"\n)\n\n# Start an action\nhandle = connector.start_action(\n    action_data=my_goal_msg,\n    target=\"/my_action\",\n    msg_type=\"my_package/action/MyAction\",\n    on_feedback=feedback_cb,\n    on_done=done_cb\n)\n\n# Get available topics\ntopics = connector.get_topics_names_and_types()\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#node-lifecycle-and-threading","title":"Node Lifecycle and Threading","text":"<p>The connector creates a dedicated ROS 2 node and runs it in a background thread, using a <code>MultiThreadedExecutor</code> for asynchronous operations. This allows for concurrent message handling and callback execution.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#ros2hriconnector","title":"<code>ROS2HRIConnector</code>","text":"<p>The <code>ROS2HRIConnector</code> extends <code>ROS2BaseConnector</code> and implements the <code>HRIConnector</code> interface for multimodal human-robot interaction messages. It is specialized for exchanging <code>ROS2HRIMessage</code> objects, which can contain text, images, and audio.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#key-features_1","title":"Key Features","text":"<ul> <li>Publishes and subscribes to <code>rai_interfaces/msg/HRIMessage</code> topics</li> <li>Converts between ROS 2 HRI messages and the internal multimodal format</li> <li>Supports all standard connector operations (topics, services, actions)</li> <li>Suitable for integrating AI agents with human-facing ROS 2 interfaces</li> </ul>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#example-usage_1","title":"Example Usage","text":"<pre><code>from rai.communication.ros2.connectors import ROS2HRIConnector\n\nhri_connector = ROS2HRIConnector()\n\n# Send a multimodal HRI message to a topic\nhri_connector.send_message(\n    message=my_hri_msg,  # ROS2HRIMessage\n    target=\"/to_human\"\n)\n\n# Register a callback for incoming HRI messages\nhri_connector.register_callback(\n    source=\"/from_human\",\n    callback=on_human_message\n)\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#message-conversion","title":"Message Conversion","text":"<p>The <code>ROS2HRIConnector</code> automatically converts between ROS 2 <code>rai_interfaces/msg/HRIMessage</code> and the internal <code>ROS2HRIMessage</code> format for seamless multimodal communication.</p>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#usage-in-agents","title":"Usage in Agents","text":"<p>Both connectors are commonly used in RAI agents to interface with ROS 2 environments. Example:</p> <pre><code>from rai.communication.ros2.connectors import ROS2Connector, ROS2HRIConnector\n\nros2_connector = ROS2Connector()\nhri_connector = ROS2HRIConnector()\n\n# Use with tools or agents\nagent = ReActAgent(\n    target_connectors={\"/to_human\": hri_connector},\n    tools=ROS2Toolkit(connector=ros2_connector).get_tools()\n)\n\n# Subscribe to human input\nagent.subscribe_source(\"/from_human\", hri_connector)\n</code></pre>"},{"location":"API_documentation/connectors/ROS_2_Connectors/#see-also","title":"See Also","text":"<ul> <li>Connectors Overview</li> <li>ROS 2 Aggregators</li> <li>ROS 2 Tools</li> </ul>"},{"location":"API_documentation/connectors/overview/","title":"Connectors","text":"<p>Connectors are a set of abstract interfaces and implementations designed to provide a unified way to interact with various communication systems, including robot middleware like ROS2, sound devices, and other I/O systems.</p>"},{"location":"API_documentation/connectors/overview/#connector-architecture","title":"Connector Architecture","text":"<p>The connector architecture is built on a hierarchy of abstract base classes and concrete implementations:</p>"},{"location":"API_documentation/connectors/overview/#base-classes","title":"Base Classes","text":""},{"location":"API_documentation/connectors/overview/#baseconnectort","title":"BaseConnector[T]","text":"BaseConnector class definition <p>The foundation interface that defines common communication patterns:</p> <ul> <li>Message passing (publish/subscribe)</li> <li>Service calls (request/response)</li> <li>Actions (long-running operations with feedback)</li> <li>Callback registration for asynchronous notifications</li> </ul>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector","title":"<code>rai.communication.base_connector.BaseConnector</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>class BaseConnector(Generic[T]):\n    def __init__(self, callback_max_workers: int = 4):\n        self.callback_max_workers = callback_max_workers\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.registered_callbacks: Dict[str, Dict[str, ParametrizedCallback[T]]] = (\n            defaultdict(dict)\n        )\n        self.callback_id_mapping: Dict[str, tuple[str, ParametrizedCallback[T]]] = {}\n        self.callback_executor = ThreadPoolExecutor(\n            max_workers=self.callback_max_workers\n        )\n\n        if not hasattr(self, \"__orig_bases__\"):\n            self.__orig_bases__ = {}\n            raise ConnectorException(\n                f\"Error while instantiating {str(self.__class__)}: \"\n                \"Message type T derived from BaseMessage needs to be provided\"\n                \" e.g. Connector[MessageType]()\"\n            )\n        self.T_class: Type[T] = get_args(self.__orig_bases__[-1])[0]\n\n    def _generate_handle(self) -&gt; str:\n        return str(uuid4())\n\n    def send_message(self, message: T, target: str, **kwargs: Optional[Any]) -&gt; None:\n        \"\"\"Implements publish pattern.\n\n        Sends a message to one or more subscribers. The target parameter\n        can be used to specify the destination or topic.\n\n        Parameters\n        ----------\n        message : T\n            The message to send.\n        target : str\n            The destination or topic to send the message to.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Raises\n        ------\n        ConnectorException\n            If the message cannot be sent.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def receive_message(\n        self, source: str, timeout_sec: float, **kwargs: Optional[Any]\n    ) -&gt; T:\n        \"\"\"Implements subscribe pattern.\n\n        Receives a message from a publisher. The source parameter\n        can be used to specify the source or topic to subscribe to.\n\n        Parameters\n        ----------\n        source : str\n            The source or topic to receive the message from.\n        timeout_sec : float\n            Timeout in seconds for receiving the message.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        T\n            The received message.\n\n        Raises\n        ------\n        ConnectorException\n            If the message cannot be received.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def register_callback(\n        self,\n        source: str,\n        callback: Callable[[T | Any], None],\n        raw: bool = False,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Implements register callback.\n\n        Registers a callback to be called when a message is received from a source.\n        If raw is False, the callback will receive a T object.\n        If raw is True, the callback will receive the raw message.\n\n        Parameters\n        ----------\n        source : str\n            The source to register the callback for.\n        callback : Callable[[T | Any], None]\n            The callback function to register.\n        raw : bool, optional\n            Whether to pass raw message to callback, by default False.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The ID of the registered callback.\n\n        Raises\n        ------\n        ConnectorException\n            If the callback cannot be registered.\n        \"\"\"\n        parametrized_callback = ParametrizedCallback[T](callback=callback, raw=raw)\n        self.registered_callbacks[source][parametrized_callback.id] = (\n            parametrized_callback\n        )\n        self.callback_id_mapping[parametrized_callback.id] = (\n            source,\n            parametrized_callback,\n        )\n        return parametrized_callback.id\n\n    def unregister_callback(self, callback_id: str) -&gt; None:\n        \"\"\"Unregisters a callback from a source.\n\n        Parameters\n        ----------\n        callback_id : str\n            The id of the callback to unregister.\n\n        Raises\n        ------\n        ConnectorException\n            If the callback cannot be unregistered.\n        \"\"\"\n        if callback_id not in self.callback_id_mapping:\n            raise ConnectorException(f\"Callback with id {callback_id} not found.\")\n\n        source, _ = self.callback_id_mapping[callback_id]\n        del self.registered_callbacks[source][callback_id]\n        del self.callback_id_mapping[callback_id]\n\n    def _safe_callback_wrapper(self, callback: Callable[[T], None], message: T) -&gt; None:\n        \"\"\"Safely execute a callback with error handling.\n\n        Parameters\n        ----------\n        callback : Callable[[T], None]\n            The callback function to execute.\n        message : T\n            The message to pass to the callback.\n        \"\"\"\n        try:\n            callback(message)\n        except Exception as e:\n            self.logger.error(f\"Error in callback: {str(e)}\")\n\n    def general_callback(self, source: str, message: Any) -&gt; None:\n        processed_message = self.general_callback_preprocessor(message)\n        for parametrized_callback in self.registered_callbacks.get(source, {}).values():\n            payload = message if parametrized_callback.raw else processed_message\n            self.callback_executor.submit(\n                self._safe_callback_wrapper, parametrized_callback.callback, payload\n            )\n\n    def general_callback_preprocessor(self, message: Any) -&gt; T:\n        \"\"\"Preprocessor for general callback used to transform any message to a BaseMessage.\n\n        Parameters\n        ----------\n        message : Any\n            The message to preprocess.\n\n        Returns\n        -------\n        T\n            The preprocessed message.\n\n        Raises\n        ------\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def call_service(\n        self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n    ) -&gt; BaseMessage:\n        \"\"\"\n        Alias for service_call method.\n        \"\"\"\n        return self.service_call(message, target, timeout_sec, **kwargs)\n\n    def service_call(\n        self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n    ) -&gt; BaseMessage:\n        \"\"\"Implements request-response pattern.\n\n        Sends a request and waits for a response. The target parameter\n        specifies the service endpoint to call.\n\n        Parameters\n        ----------\n        message : T\n            The request message to send.\n        target : str\n            The service endpoint to call.\n        timeout_sec : float\n            Timeout in seconds for the service call.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        BaseMessage\n            The response message.\n\n        Raises\n        ------\n        ConnectorException\n            If the service call cannot be made.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def create_service(\n        self,\n        service_name: str,\n        on_request: Callable,\n        on_done: Optional[Callable] = None,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Sets up a service endpoint for handling requests.\n\n        Creates a service that can receive and process requests.\n        The on_request callback handles incoming requests,\n        and on_done (if provided) is called when the service is terminated.\n\n        Parameters\n        ----------\n        service_name : str\n            Name of the service to create.\n        on_request : Callable\n            Callback function to handle incoming requests.\n        on_done : Optional[Callable], optional\n            Callback function called when service is terminated, by default None.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The handle of the created service.\n\n        Raises\n        ------\n        ConnectorException\n            If the service cannot be created.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def create_action(\n        self,\n        action_name: str,\n        generate_feedback_callback: Callable,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Sets up an action endpoint for long-running operations.\n\n        Creates an action that can be started and monitored.\n        The generate_feedback_callback is used to provide progress updates\n        during the action's execution.\n\n        Parameters\n        ----------\n        action_name : str\n            Name of the action to create.\n        generate_feedback_callback : Callable\n            Callback function to generate feedback during action execution.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The handle of the created action.\n\n        Raises\n        ------\n        ConnectorException\n            If the action cannot be created.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def start_action(\n        self,\n        action_data: Optional[T],\n        target: str,\n        on_feedback: Callable,\n        on_done: Callable,\n        timeout_sec: float,\n        **kwargs: Optional[Any],\n    ) -&gt; str:\n        \"\"\"Initiates a long-running operation with feedback.\n\n        Starts an action and provides callbacks for feedback and completion.\n        The on_feedback callback receives progress updates,\n        and on_done is called when the action completes.\n\n        Parameters\n        ----------\n        action_data : Optional[T]\n            Data to pass to the action.\n        target : str\n            The action endpoint to start.\n        on_feedback : Callable\n            Callback function to receive progress updates.\n        on_done : Callable\n            Callback function called when action completes.\n        timeout_sec : float\n            Timeout in seconds for the action.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        str\n            The handle of the started action.\n\n        Raises\n        ------\n        ConnectorException\n            If the action cannot be started.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def terminate_action(self, action_handle: str, **kwargs: Optional[Any]) -&gt; Any:\n        \"\"\"Cancels an ongoing action.\n\n        Stops the execution of a previously started action.\n        The action_handle identifies which action to terminate.\n\n        Parameters\n        ----------\n        action_handle : str\n            The handle of the action to terminate.\n        **kwargs : Optional[Any]\n            Additional keyword arguments.\n\n        Returns\n        -------\n        Any\n            Result of the termination operation.\n\n        Raises\n        ------\n        ConnectorException\n            If the action cannot be terminated.\n        NotImplementedError\n            If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented by the subclass.\")\n\n    def shutdown(self):\n        \"\"\"Shuts down the connector and releases all resources.\"\"\"\n        self.callback_executor.shutdown(wait=True)\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.call_service","title":"<code>call_service(message, target, timeout_sec, **kwargs)</code>","text":"<p>Alias for service_call method.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def call_service(\n    self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n) -&gt; BaseMessage:\n    \"\"\"\n    Alias for service_call method.\n    \"\"\"\n    return self.service_call(message, target, timeout_sec, **kwargs)\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.create_action","title":"<code>create_action(action_name, generate_feedback_callback, **kwargs)</code>","text":"<p>Sets up an action endpoint for long-running operations.</p> <p>Creates an action that can be started and monitored. The generate_feedback_callback is used to provide progress updates during the action's execution.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action to create.</p> required <code>generate_feedback_callback</code> <code>Callable</code> <p>Callback function to generate feedback during action execution.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The handle of the created action.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the action cannot be created.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def create_action(\n    self,\n    action_name: str,\n    generate_feedback_callback: Callable,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Sets up an action endpoint for long-running operations.\n\n    Creates an action that can be started and monitored.\n    The generate_feedback_callback is used to provide progress updates\n    during the action's execution.\n\n    Parameters\n    ----------\n    action_name : str\n        Name of the action to create.\n    generate_feedback_callback : Callable\n        Callback function to generate feedback during action execution.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The handle of the created action.\n\n    Raises\n    ------\n    ConnectorException\n        If the action cannot be created.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.create_service","title":"<code>create_service(service_name, on_request, on_done=None, **kwargs)</code>","text":"<p>Sets up a service endpoint for handling requests.</p> <p>Creates a service that can receive and process requests. The on_request callback handles incoming requests, and on_done (if provided) is called when the service is terminated.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Name of the service to create.</p> required <code>on_request</code> <code>Callable</code> <p>Callback function to handle incoming requests.</p> required <code>on_done</code> <code>Optional[Callable]</code> <p>Callback function called when service is terminated, by default None.</p> <code>None</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The handle of the created service.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the service cannot be created.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def create_service(\n    self,\n    service_name: str,\n    on_request: Callable,\n    on_done: Optional[Callable] = None,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Sets up a service endpoint for handling requests.\n\n    Creates a service that can receive and process requests.\n    The on_request callback handles incoming requests,\n    and on_done (if provided) is called when the service is terminated.\n\n    Parameters\n    ----------\n    service_name : str\n        Name of the service to create.\n    on_request : Callable\n        Callback function to handle incoming requests.\n    on_done : Optional[Callable], optional\n        Callback function called when service is terminated, by default None.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The handle of the created service.\n\n    Raises\n    ------\n    ConnectorException\n        If the service cannot be created.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.general_callback_preprocessor","title":"<code>general_callback_preprocessor(message)</code>","text":"<p>Preprocessor for general callback used to transform any message to a BaseMessage.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Any</code> <p>The message to preprocess.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The preprocessed message.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def general_callback_preprocessor(self, message: Any) -&gt; T:\n    \"\"\"Preprocessor for general callback used to transform any message to a BaseMessage.\n\n    Parameters\n    ----------\n    message : Any\n        The message to preprocess.\n\n    Returns\n    -------\n    T\n        The preprocessed message.\n\n    Raises\n    ------\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.receive_message","title":"<code>receive_message(source, timeout_sec, **kwargs)</code>","text":"<p>Implements subscribe pattern.</p> <p>Receives a message from a publisher. The source parameter can be used to specify the source or topic to subscribe to.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source or topic to receive the message from.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds for receiving the message.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The received message.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the message cannot be received.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def receive_message(\n    self, source: str, timeout_sec: float, **kwargs: Optional[Any]\n) -&gt; T:\n    \"\"\"Implements subscribe pattern.\n\n    Receives a message from a publisher. The source parameter\n    can be used to specify the source or topic to subscribe to.\n\n    Parameters\n    ----------\n    source : str\n        The source or topic to receive the message from.\n    timeout_sec : float\n        Timeout in seconds for receiving the message.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    T\n        The received message.\n\n    Raises\n    ------\n    ConnectorException\n        If the message cannot be received.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.register_callback","title":"<code>register_callback(source, callback, raw=False, **kwargs)</code>","text":"<p>Implements register callback.</p> <p>Registers a callback to be called when a message is received from a source. If raw is False, the callback will receive a T object. If raw is True, the callback will receive the raw message.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source to register the callback for.</p> required <code>callback</code> <code>Callable[[T | Any], None]</code> <p>The callback function to register.</p> required <code>raw</code> <code>bool</code> <p>Whether to pass raw message to callback, by default False.</p> <code>False</code> <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The ID of the registered callback.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the callback cannot be registered.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def register_callback(\n    self,\n    source: str,\n    callback: Callable[[T | Any], None],\n    raw: bool = False,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Implements register callback.\n\n    Registers a callback to be called when a message is received from a source.\n    If raw is False, the callback will receive a T object.\n    If raw is True, the callback will receive the raw message.\n\n    Parameters\n    ----------\n    source : str\n        The source to register the callback for.\n    callback : Callable[[T | Any], None]\n        The callback function to register.\n    raw : bool, optional\n        Whether to pass raw message to callback, by default False.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The ID of the registered callback.\n\n    Raises\n    ------\n    ConnectorException\n        If the callback cannot be registered.\n    \"\"\"\n    parametrized_callback = ParametrizedCallback[T](callback=callback, raw=raw)\n    self.registered_callbacks[source][parametrized_callback.id] = (\n        parametrized_callback\n    )\n    self.callback_id_mapping[parametrized_callback.id] = (\n        source,\n        parametrized_callback,\n    )\n    return parametrized_callback.id\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.send_message","title":"<code>send_message(message, target, **kwargs)</code>","text":"<p>Implements publish pattern.</p> <p>Sends a message to one or more subscribers. The target parameter can be used to specify the destination or topic.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>T</code> <p>The message to send.</p> required <code>target</code> <code>str</code> <p>The destination or topic to send the message to.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the message cannot be sent.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def send_message(self, message: T, target: str, **kwargs: Optional[Any]) -&gt; None:\n    \"\"\"Implements publish pattern.\n\n    Sends a message to one or more subscribers. The target parameter\n    can be used to specify the destination or topic.\n\n    Parameters\n    ----------\n    message : T\n        The message to send.\n    target : str\n        The destination or topic to send the message to.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Raises\n    ------\n    ConnectorException\n        If the message cannot be sent.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.service_call","title":"<code>service_call(message, target, timeout_sec, **kwargs)</code>","text":"<p>Implements request-response pattern.</p> <p>Sends a request and waits for a response. The target parameter specifies the service endpoint to call.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>T</code> <p>The request message to send.</p> required <code>target</code> <code>str</code> <p>The service endpoint to call.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds for the service call.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseMessage</code> <p>The response message.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the service call cannot be made.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def service_call(\n    self, message: T, target: str, timeout_sec: float, **kwargs: Optional[Any]\n) -&gt; BaseMessage:\n    \"\"\"Implements request-response pattern.\n\n    Sends a request and waits for a response. The target parameter\n    specifies the service endpoint to call.\n\n    Parameters\n    ----------\n    message : T\n        The request message to send.\n    target : str\n        The service endpoint to call.\n    timeout_sec : float\n        Timeout in seconds for the service call.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    BaseMessage\n        The response message.\n\n    Raises\n    ------\n    ConnectorException\n        If the service call cannot be made.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.shutdown","title":"<code>shutdown()</code>","text":"<p>Shuts down the connector and releases all resources.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def shutdown(self):\n    \"\"\"Shuts down the connector and releases all resources.\"\"\"\n    self.callback_executor.shutdown(wait=True)\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.start_action","title":"<code>start_action(action_data, target, on_feedback, on_done, timeout_sec, **kwargs)</code>","text":"<p>Initiates a long-running operation with feedback.</p> <p>Starts an action and provides callbacks for feedback and completion. The on_feedback callback receives progress updates, and on_done is called when the action completes.</p> <p>Parameters:</p> Name Type Description Default <code>action_data</code> <code>Optional[T]</code> <p>Data to pass to the action.</p> required <code>target</code> <code>str</code> <p>The action endpoint to start.</p> required <code>on_feedback</code> <code>Callable</code> <p>Callback function to receive progress updates.</p> required <code>on_done</code> <code>Callable</code> <p>Callback function called when action completes.</p> required <code>timeout_sec</code> <code>float</code> <p>Timeout in seconds for the action.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The handle of the started action.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the action cannot be started.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def start_action(\n    self,\n    action_data: Optional[T],\n    target: str,\n    on_feedback: Callable,\n    on_done: Callable,\n    timeout_sec: float,\n    **kwargs: Optional[Any],\n) -&gt; str:\n    \"\"\"Initiates a long-running operation with feedback.\n\n    Starts an action and provides callbacks for feedback and completion.\n    The on_feedback callback receives progress updates,\n    and on_done is called when the action completes.\n\n    Parameters\n    ----------\n    action_data : Optional[T]\n        Data to pass to the action.\n    target : str\n        The action endpoint to start.\n    on_feedback : Callable\n        Callback function to receive progress updates.\n    on_done : Callable\n        Callback function called when action completes.\n    timeout_sec : float\n        Timeout in seconds for the action.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    str\n        The handle of the started action.\n\n    Raises\n    ------\n    ConnectorException\n        If the action cannot be started.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.terminate_action","title":"<code>terminate_action(action_handle, **kwargs)</code>","text":"<p>Cancels an ongoing action.</p> <p>Stops the execution of a previously started action. The action_handle identifies which action to terminate.</p> <p>Parameters:</p> Name Type Description Default <code>action_handle</code> <code>str</code> <p>The handle of the action to terminate.</p> required <code>**kwargs</code> <code>Optional[Any]</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result of the termination operation.</p> <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the action cannot be terminated.</p> <code>NotImplementedError</code> <p>If the method is not implemented by the subclass.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def terminate_action(self, action_handle: str, **kwargs: Optional[Any]) -&gt; Any:\n    \"\"\"Cancels an ongoing action.\n\n    Stops the execution of a previously started action.\n    The action_handle identifies which action to terminate.\n\n    Parameters\n    ----------\n    action_handle : str\n        The handle of the action to terminate.\n    **kwargs : Optional[Any]\n        Additional keyword arguments.\n\n    Returns\n    -------\n    Any\n        Result of the termination operation.\n\n    Raises\n    ------\n    ConnectorException\n        If the action cannot be terminated.\n    NotImplementedError\n        If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass.\")\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.base_connector.BaseConnector.unregister_callback","title":"<code>unregister_callback(callback_id)</code>","text":"<p>Unregisters a callback from a source.</p> <p>Parameters:</p> Name Type Description Default <code>callback_id</code> <code>str</code> <p>The id of the callback to unregister.</p> required <p>Raises:</p> Type Description <code>ConnectorException</code> <p>If the callback cannot be unregistered.</p> Source code in <code>rai/communication/base_connector.py</code> <pre><code>def unregister_callback(self, callback_id: str) -&gt; None:\n    \"\"\"Unregisters a callback from a source.\n\n    Parameters\n    ----------\n    callback_id : str\n        The id of the callback to unregister.\n\n    Raises\n    ------\n    ConnectorException\n        If the callback cannot be unregistered.\n    \"\"\"\n    if callback_id not in self.callback_id_mapping:\n        raise ConnectorException(f\"Callback with id {callback_id} not found.\")\n\n    source, _ = self.callback_id_mapping[callback_id]\n    del self.registered_callbacks[source][callback_id]\n    del self.callback_id_mapping[callback_id]\n</code></pre>"},{"location":"API_documentation/connectors/overview/#hriconnectort","title":"HRIConnector[T]","text":"HRIConnector class definition <p>Extends BaseConnector with Human-Robot Interaction capabilities:</p> <ul> <li>Supports multimodal messages (text, images, audio)</li> <li>Provides conversion to/from Langchain message formats</li> <li>Handles message sequencing and conversation IDs</li> </ul>"},{"location":"API_documentation/connectors/overview/#rai.communication.hri_connector.HRIConnector","title":"<code>rai.communication.hri_connector.HRIConnector</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>BaseConnector[T]</code></p> <p>Base class for Human-Robot Interaction (HRI) connectors. Used for sending and receiving messages between human and robot from various sources.</p> Source code in <code>rai/communication/hri_connector.py</code> <pre><code>class HRIConnector(Generic[T], BaseConnector[T]):\n    \"\"\"\n    Base class for Human-Robot Interaction (HRI) connectors.\n    Used for sending and receiving messages between human and robot from various sources.\n    \"\"\"\n\n    def build_message(\n        self,\n        message: LangchainBaseMessage | RAIMultimodalMessage,\n        communication_id: Optional[str] = None,\n        seq_no: int = 0,\n        seq_end: bool = False,\n    ) -&gt; T:\n        \"\"\"\n        Build a new message object from a given input message.\n\n        Parameters\n        ----------\n        message : LangchainBaseMessage or RAIMultimodalMessage\n            The source message to transform into the target type.\n        communication_id : str, optional\n            An optional identifier for the communication session. Defaults to `None`.\n        seq_no : int, optional\n            The sequence number of the message in the communication stream. Defaults to `0`.\n        seq_end : bool, optional\n            Flag indicating whether this message is the final one in the sequence. Defaults to `False`.\n\n        Returns\n        -------\n        T\n            A message instance of type `T` compatible with the Connector, created from the provided input.\n\n        Notes\n        -----\n        This method uses `self.T_class.from_langchain` for conversion and assumes compatibility.\n        \"\"\"\n        return self.T_class.from_langchain(message, communication_id, seq_no, seq_end)  # type: ignore\n</code></pre>"},{"location":"API_documentation/connectors/overview/#rai.communication.hri_connector.HRIConnector.build_message","title":"<code>build_message(message, communication_id=None, seq_no=0, seq_end=False)</code>","text":"<p>Build a new message object from a given input message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>BaseMessage or MultimodalMessage</code> <p>The source message to transform into the target type.</p> required <code>communication_id</code> <code>str</code> <p>An optional identifier for the communication session. Defaults to <code>None</code>.</p> <code>None</code> <code>seq_no</code> <code>int</code> <p>The sequence number of the message in the communication stream. Defaults to <code>0</code>.</p> <code>0</code> <code>seq_end</code> <code>bool</code> <p>Flag indicating whether this message is the final one in the sequence. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>T</code> <p>A message instance of type <code>T</code> compatible with the Connector, created from the provided input.</p> Notes <p>This method uses <code>self.T_class.from_langchain</code> for conversion and assumes compatibility.</p> Source code in <code>rai/communication/hri_connector.py</code> <pre><code>def build_message(\n    self,\n    message: LangchainBaseMessage | RAIMultimodalMessage,\n    communication_id: Optional[str] = None,\n    seq_no: int = 0,\n    seq_end: bool = False,\n) -&gt; T:\n    \"\"\"\n    Build a new message object from a given input message.\n\n    Parameters\n    ----------\n    message : LangchainBaseMessage or RAIMultimodalMessage\n        The source message to transform into the target type.\n    communication_id : str, optional\n        An optional identifier for the communication session. Defaults to `None`.\n    seq_no : int, optional\n        The sequence number of the message in the communication stream. Defaults to `0`.\n    seq_end : bool, optional\n        Flag indicating whether this message is the final one in the sequence. Defaults to `False`.\n\n    Returns\n    -------\n    T\n        A message instance of type `T` compatible with the Connector, created from the provided input.\n\n    Notes\n    -----\n    This method uses `self.T_class.from_langchain` for conversion and assumes compatibility.\n    \"\"\"\n    return self.T_class.from_langchain(message, communication_id, seq_no, seq_end)  # type: ignore\n</code></pre>"},{"location":"API_documentation/connectors/overview/#concrete-implementations","title":"Concrete Implementations","text":"Connector Description Documentation Link ROS 2 Connectors Robot Operating System 2 integration ROS2 Connectors"},{"location":"API_documentation/connectors/overview/#key-features","title":"Key Features","text":""},{"location":"API_documentation/connectors/overview/#message-types","title":"Message Types","text":"<p>Connectors are generic over message types derived from BaseMessage:</p> <ul> <li>BaseMessage: Foundation message type with payload and metadata</li> <li>ROS2Message: Message type for ROS 2 communication</li> <li>HRIMessage: Multimodal message type with text, images, and audio</li> <li>ROS2HRIMessage: HRIMessage specialized for ROS 2 transport</li> <li>SoundDeviceMessage: Specialized message for audio operations</li> </ul>"},{"location":"API_documentation/connectors/overview/#communication-patterns","title":"Communication Patterns","text":"<p>Connectors support multiple communication patterns:</p> <ol> <li> <p>Publish/Subscribe</p> <ul> <li><code>send_message(message, target, **kwargs)</code>: Send a message to a target</li> <li><code>receive_message(source, timeout_sec, **kwargs)</code>: Receive a message from a source</li> <li><code>register_callback(source, callback, **kwargs)</code>: Register for asynchronous notifications</li> </ul> </li> <li> <p>Request/Response</p> <ul> <li><code>service_call(message, target, timeout_sec, **kwargs)</code>: Make a synchronous service call</li> </ul> </li> <li> <p>Actions</p> <ul> <li><code>start_action(action_data, target, on_feedback, on_done, timeout_sec, **kwargs)</code>: Start a     long-running action</li> <li><code>terminate_action(action_handle, **kwargs)</code>: Cancel an ongoing action</li> <li><code>create_action(action_name, generate_feedback_callback, **kwargs)</code>: Create an action server</li> </ul> </li> </ol>"},{"location":"API_documentation/connectors/overview/#threading-model","title":"Threading Model","text":"<p>Connectors implement thread-safe operations:</p> <ul> <li>ROS 2 connectors use a dedicated thread with MultiThreadedExecutor</li> <li>Callbacks are executed in a ThreadPoolExecutor for concurrent processing</li> <li>Proper synchronization for shared resources</li> <li>Clean shutdown handling for all resources</li> </ul>"},{"location":"API_documentation/connectors/overview/#usage-examples","title":"Usage Examples","text":"Connector Example Usage Documentation ROS 2 ROS2 Connectors"},{"location":"API_documentation/connectors/overview/#error-handling","title":"Error Handling","text":"<p>Connectors implement robust error handling:</p> <ul> <li>All operations have appropriate timeout parameters</li> <li>Exceptions are properly propagated and documented</li> <li>Callbacks are executed in a protected manner to prevent crashes</li> <li>Resources are properly cleaned up during shutdown</li> </ul>"},{"location":"API_documentation/connectors/overview/#see-also","title":"See Also","text":"<ul> <li>Agents: For more information on the different types of agents in RAI</li> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/","title":"Tools","text":"<p>RAI provides various ROS 2 tools, both generic (mimics ros2cli) and specific (e.g., nav2, moveit2, etc.)</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#class-definition","title":"Class Definition","text":"<p><code>BaseROS2Tool</code> is the base class for all ROS 2 tools. It provides a common interface for all ROS 2 tools including name allowlist/blocklist for all the communication protocols (messages, services, actions)</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#class-definition_1","title":"Class Definition","text":""},{"location":"API_documentation/langchain_integration/ROS_2_tools/#rai.tools.ros2.base.BaseROS2Tool","title":"<code>rai.tools.ros2.base.BaseROS2Tool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>Base class for all ROS2 tools.</p> <p>Attributes:</p> Name Type Description <code>connector</code> <code>ROS2Connector</code> <p>The connector to the ROS 2 system.</p> <code>readable</code> <code>Optional[List[str]]</code> <p>The topics that can be read. If the list is not provided, all topics can be read.</p> <code>writable</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that can be written. If the list is not provided, all topics can be written.</p> <code>forbidden</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that are forbidden to read and write.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>The description of the tool.</p> Source code in <code>rai/tools/ros2/base.py</code> <pre><code>class BaseROS2Tool(BaseTool):\n    \"\"\"\n    Base class for all ROS2 tools.\n\n    Attributes\n    ----------\n    connector : ROS2Connector\n        The connector to the ROS 2 system.\n    readable : Optional[List[str]]\n        The topics that can be read. If the list is not provided, all topics can be read.\n    writable : Optional[List[str]]\n        The names (topics/actions/services) that can be written. If the list is not provided, all topics can be written.\n    forbidden : Optional[List[str]]\n        The names (topics/actions/services) that are forbidden to read and write.\n    name : str\n        The name of the tool.\n    description : str\n        The description of the tool.\n    \"\"\"\n\n    connector: ROS2Connector\n    readable: Optional[List[str]] = None\n    writable: Optional[List[str]] = None\n    forbidden: Optional[List[str]] = None\n\n    name: str = \"\"\n    description: str = \"\"\n\n    def is_readable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.readable is None:\n            return True\n        return topic in self.readable\n\n    def is_writable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.writable is None:\n            return True\n        return topic in self.writable\n</code></pre>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#usage-example","title":"Usage example","text":"<pre><code>from rai.communication.ros2 import ROS2Connector\nfrom rai.tools.ros2.base import BaseROS2Tool\n\nconnector = ROS2Connector()\n\nBaseROS2Tool( # BaseROS2Tool cannot be used directly, this is just an example\n    connector=connector,\n    readable=[\"/odom\", \"/scan\"], # readable topics, services and actions\n    writable=[\"/robot_position\"], # writable topics, services and actions\n    forbidden=[\"/cmd_vel\"], # forbidden topics, services and actions\n)\n</code></pre>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#generic-ros-2-tools","title":"Generic ROS 2 Tools","text":"<p>RAI provides a generic ROS 2 toolkit, which allows the Agent to interact with any ROS 2 topics, services and actions.</p> <pre><code>from rai.tools.ros2 import ROS2Toolkit\nfrom rai.communication.ros2 import ROS2Connector\n\nconnector = ROS2Connector()\ntools = ROS2Toolkit(connector=connector).get_tools()\n</code></pre> <p>Below is the list of tools provided by the generic ROS 2 toolkit that can also be used as standalone tools (except for the ROS 2 action tools, which should be used via the <code>ROS2ActionToolkit</code> as they share a state):</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#topics","title":"Topics","text":"Tool Name Description <code>PublishROS2MessageTool</code> Tool for publishing messages to ROS 2 topics <code>ReceiveROS2MessageTool</code> Tool for receiving messages from ROS 2 topics <code>GetROS2ImageTool</code> Tool for retrieving image data from topics <code>GetROS2TopicsNamesAndTypesTool</code> Tool for listing all available topics and their types <code>GetROS2MessageInterfaceTool</code> Tool for getting message interface information <code>GetROS2TransformTool</code> Tool for retrieving transform data"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#services","title":"Services","text":"Tool Name Description <code>GetROS2ServicesNamesAndTypesTool</code> Tool for listing all available services and their types <code>CallROS2ServiceTool</code> Tool for calling ROS 2 services"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#actions","title":"Actions","text":"Tool Name Description <code>GetROS2ActionsNamesAndTypesTool</code> Tool for listing all available actions and their types <code>StartROS2ActionTool</code> Tool for starting ROS 2 actions <code>GetROS2ActionFeedbackTool</code> Tool for retrieving action feedback <code>GetROS2ActionResultTool</code> Tool for retrieving action results <code>CancelROS2ActionTool</code> Tool for canceling running actions <code>GetROS2ActionIDsTool</code> Tool for getting action IDs"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#specific-ros-2-tools","title":"Specific ROS 2 Tools","text":"<p>RAI provides specific ROS 2 tools for certain ROS 2 packages.</p>"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#nav2","title":"Nav2","text":"Tool Name Description <code>NavigateToPoseTool</code> Tool for navigating to a pose <code>GetNavigateToPoseFeedbackTool</code> Tool for retrieving the feedback of a navigate to pose action <code>GetNavigateToPoseResultTool</code> Tool for retrieving the result of a navigate to pose action <code>CancelNavigateToPoseTool</code> Tool for canceling a navigate to pose action <code>GetOccupancyGridTool</code> Tool for retrieving the occupancy grid"},{"location":"API_documentation/langchain_integration/ROS_2_tools/#custom-tools","title":"Custom Tools","text":"Tool Name Description <code>MoveToPointTool</code> Tool for moving to a point <code>MoveObjectFromToTool</code> Tool for moving an object from one point to another <code>GetObjectPositionsTool</code> Tool for retrieving the positions of objects"},{"location":"API_documentation/langchain_integration/multimodal_messages/","title":"Multimodality support","text":"<p>RAI implements <code>MultimodalMessage</code> that allows using image and audio* information in langchain.</p> <p>Audio is not fully supported yet</p> <p>Audio is currently added as a placeholder for future implementation.</p>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#class-definition","title":"Class Definition","text":"<p>LangChain supports multimodal data by default. This is done by expanding the content section from string to dictionary, containing specific keys. To make it easier to use, RAI implements a <code>MultimodalMessage</code> class, which is a wrapper around the <code>BaseMessage</code> class.</p>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#class-definition_1","title":"Class Definition","text":""},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.MultimodalMessage","title":"<code>rai.messages.multimodal.MultimodalMessage</code>","text":"<p>               Bases: <code>BaseMessage</code></p> <p>Base class for multimodal messages.</p> <p>Attributes:</p> Name Type Description <code>images</code> <code>Optional[List[str]]</code> <p>List of base64 encoded images.</p> <code>audios</code> <code>Optional[Any]</code> <p>List of base64 encoded audios.</p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class MultimodalMessage(BaseMessage):\n    \"\"\"\n    Base class for multimodal messages.\n\n    Attributes\n    ----------\n    images : Optional[List[str]]\n        List of base64 encoded images.\n    audios : Optional[Any]\n        List of base64 encoded audios.\n    \"\"\"\n\n    images: Optional[List[str]] = None\n    audios: Optional[Any] = None\n\n    def __init__(\n        self,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)  # type: ignore\n\n        if self.audios not in [None, []]:\n            raise ValueError(\"Audio is not yet supported\")\n\n        _content: List[Union[str, Dict[str, Union[Dict[str, str], str]]]] = []\n\n        if isinstance(self.content, str):\n            _content.append({\"type\": \"text\", \"text\": self.content})\n        else:\n            raise ValueError(\"Content must be a string\")  # for now, to guarantee compat\n\n        if isinstance(self.images, list):\n            _image_content = [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{image}\",\n                    },\n                }\n                for image in self.images\n            ]\n            _content.extend(_image_content)\n        self.content = _content\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#subclasses","title":"Subclasses","text":""},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.HumanMultimodalMessage","title":"<code>rai.messages.multimodal.HumanMultimodalMessage</code>","text":"<p>               Bases: <code>HumanMessage</code>, <code>MultimodalMessage</code></p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class HumanMultimodalMessage(HumanMessage, MultimodalMessage):\n    def __repr_args__(self) -&gt; Any:\n        args = super().__repr_args__()\n        new_args = []\n        for k, v in args:\n            if k == \"content\":\n                v = [c for c in v if c[\"type\"] != \"image_url\"]\n            elif k == \"images\":\n                imgs_summary = [image[0:10] + \"...\" for image in v]\n                v = f\"{len(v)} base64 encoded images: [{', '.join(imgs_summary)}]\"\n            new_args.append((k, v))\n        return new_args\n\n    def _no_img_content(self):\n        return [c for c in self.content if c[\"type\"] != \"image_url\"]\n\n    def pretty_repr(self, html: bool = False) -&gt; str:\n        title = get_msg_title_repr(self.type.title() + \" Message\", bold=html)\n        # TODO: handle non-string content.\n        if self.name is not None:\n            title += f\"\\nName: {self.name}\"\n        return f\"{title}\\n\\n{self._no_img_content()}\"\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.AIMultimodalMessage","title":"<code>rai.messages.multimodal.AIMultimodalMessage</code>","text":"<p>               Bases: <code>AIMessage</code>, <code>MultimodalMessage</code></p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class AIMultimodalMessage(AIMessage, MultimodalMessage):\n    pass\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.SystemMultimodalMessage","title":"<code>rai.messages.multimodal.SystemMultimodalMessage</code>","text":"<p>               Bases: <code>SystemMessage</code>, <code>MultimodalMessage</code></p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class SystemMultimodalMessage(SystemMessage, MultimodalMessage):\n    pass\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.messages.multimodal.ToolMultimodalMessage","title":"<code>rai.messages.multimodal.ToolMultimodalMessage</code>","text":"<p>               Bases: <code>ToolMessage</code>, <code>MultimodalMessage</code></p> Note <p>When any subclass of this class is used with LangGraph agents, use <code>rai.agents.langchain.core import ToolRunner</code> as the tool runner, as it automatically handles multimodal ToolMessages as well as converts them to a format that is compatible with the vendor. </p> Source code in <code>rai/messages/multimodal.py</code> <pre><code>class ToolMultimodalMessage(ToolMessage, MultimodalMessage):\n    \"\"\"\n\n    Note\n    ----\n    When any subclass of this class is used with LangGraph agents, use\n    `rai.agents.langchain.core import ToolRunner` as the tool runner, as it automatically\n    handles multimodal ToolMessages as well as converts them to a format\n    that is compatible with the vendor.\n    ::: rai.agents.langchain.core.ToolRunner\n    \"\"\"\n\n    def postprocess(self, format: Literal[\"openai\", \"bedrock\"] = \"openai\"):\n        if format == \"openai\":\n            return self._postprocess_openai()\n        elif format == \"bedrock\":\n            return self._postprocess_bedrock()\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n    def _postprocess_openai(self):\n        \"\"\"OpenAI does not allow images in the tool message.\n        Functions dumps the message into human multimodal message and tool message.\n        \"\"\"\n        if isinstance(self.images, list):\n            human_message = HumanMultimodalMessage(\n                content=f\"Image returned by a tool call {self.tool_call_id}\",\n                images=self.images,\n                tool_call_id=self.tool_call_id,\n            )\n            # at this point self.content is a list of dicts\n            # we need to extract the text from each dict\n            tool_message = ToolMultimodalMessage(\n                tool_call_id=self.tool_call_id,\n                name=self.name,\n                content=\" \".join([part.get(\"text\", \"\") for part in self.content]),\n            )\n            return [tool_message, human_message]\n        else:\n            # TODO(maciejmajek): find out if content can be a list\n            return ToolMessage(tool_call_id=self.tool_call_id, content=self.content)\n\n    def _postprocess_bedrock(self):\n        return self._postprocess_openai()\n        # https://github.com/langchain-ai/langchain-aws/issues/75\n        # at this moment im not sure if bedrock supports images in the tool message\n        content = self.content\n        # bedrock expects image and not image_url\n        content[1][\"type\"] = \"image\"\n        content[1][\"image\"] = content[1].pop(\"image_url\")\n        content[1][\"image\"][\"source\"] = content[1][\"image\"].pop(\"url\")\n\n        return ToolMessage(tool_call_id=self.tool_call_id, content=content)\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.agents.langchain.core.ToolRunner","title":"<code>rai.agents.langchain.core.ToolRunner</code>","text":"<p>               Bases: <code>RunnableCallable</code></p> Source code in <code>rai/agents/langchain/core/tool_runner.py</code> <pre><code>class ToolRunner(RunnableCallable):\n    def __init__(\n        self,\n        tools: Sequence[Union[BaseTool, Callable]],\n        *,\n        name: str = \"tools\",\n        tags: Optional[list[str]] = None,\n        logger: Optional[logging.Logger] = None,\n    ) -&gt; None:\n        super().__init__(self._func, name=name, tags=tags, trace=False)\n        self.logger = logger or logging.getLogger(__name__)\n        self.tools_by_name: Dict[str, BaseTool] = {}\n        for tool_ in tools:\n            if not isinstance(tool_, BaseTool):\n                tool_ = create_tool(tool_)\n            self.tools_by_name[tool_.name] = tool_\n\n    def get_messages(self, input: dict[str, Any]) -&gt; List:\n        \"\"\"Get fields from from input that will be processed.\"\"\"\n        return input.get(\"messages\", [])\n\n    def update_input_with_outputs(\n        self, input: dict[str, Any], outputs: List[Any]\n    ) -&gt; None:\n        \"\"\"Update input with tool outputs.\"\"\"\n        input[\"messages\"].extend(outputs)\n\n    def _func(self, input: dict[str, Any], config: RunnableConfig) -&gt; Any:\n        config[\"max_concurrency\"] = (\n            1  # TODO(maciejmajek): use better mechanism for task queueing\n        )\n        messages = self.get_messages(input)\n        if not messages:\n            raise ValueError(\"No message found in input\")\n\n        message = messages[-1]\n        if not isinstance(message, AIMessage):\n            raise ValueError(\"Last message is not an AIMessage\")\n\n        def run_one(call: ToolCall):\n            self.logger.info(f\"Running tool: {call['name']}, args: {call['args']}\")\n            artifact = None\n\n            try:\n                ts = time.perf_counter()\n                output = self.tools_by_name[call[\"name\"]].invoke(call, config)  # type: ignore\n                te = time.perf_counter() - ts\n                self.logger.info(\n                    f\"Tool {call['name']} completed in {te:.2f} seconds. Tool output: {str(output.content)[:100]}{'...' if len(str(output.content)) &gt; 100 else ''}\"\n                )\n                self.logger.debug(\n                    f\"Tool {call['name']} output: \\n\\n{str(output.content)}\"\n                )\n            except ValidationError as e:\n                errors = e.errors()\n                for error in errors:\n                    error.pop(\n                        \"url\"\n                    )  # get rid of the  https://errors.pydantic.dev/... url\n\n                error_message = f\"\"\"\n                                    Validation error in tool {call[\"name\"]}:\n                                    {e.title}\n                                    Number of errors: {e.error_count()}\n                                    Errors:\n                                    {json.dumps(errors, indent=2)}\n                                \"\"\"\n                self.logger.info(error_message)\n                output = ToolMessage(\n                    content=error_message,\n                    name=call[\"name\"],\n                    tool_call_id=call[\"id\"],\n                    status=\"error\",\n                )\n            except Exception as e:\n                self.logger.info(f'Error in \"{call[\"name\"]}\", error: {e}')\n                output = ToolMessage(\n                    content=f\"Failed to run tool. Error: {e}\",\n                    name=call[\"name\"],\n                    tool_call_id=call[\"id\"],\n                    status=\"error\",\n                )\n\n            if output.artifact is not None:\n                artifact = output.artifact\n                if not isinstance(artifact, dict):\n                    raise ValueError(\n                        \"Artifact must be a dictionary with optional keys: 'images', 'audios'\"\n                    )\n\n                artifact = cast(MultimodalArtifact, artifact)\n                store_artifacts(output.tool_call_id, [artifact])\n\n            if artifact is not None and (\n                len(artifact.get(\"images\", [])) &gt; 0\n                or len(artifact.get(\"audios\", [])) &gt; 0\n            ):  # multimodal case, we currently support images and audios artifacts\n                return ToolMultimodalMessage(\n                    content=msg_content_output(output.content),\n                    name=call[\"name\"],\n                    tool_call_id=call[\"id\"],\n                    images=artifact.get(\"images\", []),\n                    audios=artifact.get(\"audios\", []),\n                )\n\n            return output\n\n        with get_executor_for_config(config) as executor:\n            raw_outputs = [*executor.map(run_one, message.tool_calls)]\n            outputs: List[Any] = []\n            for raw_output in raw_outputs:\n                if isinstance(raw_output, ToolMultimodalMessage):\n                    outputs.extend(\n                        raw_output.postprocess()\n                    )  # openai please allow tool messages with images!\n                else:\n                    outputs.append(raw_output)\n\n            # because we can't answer an aiMessage with an alternating sequence of tool and human messages\n            # we sort the messages by type so that the tool messages are sent first\n            # for more information see implementation of ToolMultimodalMessage.postprocess\n            outputs.sort(key=lambda x: x.__class__.__name__, reverse=True)\n\n            self.update_input_with_outputs(input, outputs)\n            return input\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.agents.langchain.core.ToolRunner.get_messages","title":"<code>get_messages(input)</code>","text":"<p>Get fields from from input that will be processed.</p> Source code in <code>rai/agents/langchain/core/tool_runner.py</code> <pre><code>def get_messages(self, input: dict[str, Any]) -&gt; List:\n    \"\"\"Get fields from from input that will be processed.\"\"\"\n    return input.get(\"messages\", [])\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#rai.agents.langchain.core.ToolRunner.update_input_with_outputs","title":"<code>update_input_with_outputs(input, outputs)</code>","text":"<p>Update input with tool outputs.</p> Source code in <code>rai/agents/langchain/core/tool_runner.py</code> <pre><code>def update_input_with_outputs(\n    self, input: dict[str, Any], outputs: List[Any]\n) -&gt; None:\n    \"\"\"Update input with tool outputs.\"\"\"\n    input[\"messages\"].extend(outputs)\n</code></pre>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#usage","title":"Usage","text":"<p>Example:</p> <pre><code>from rai.messages import HumanMultimodalMessage, preprocess_image\nfrom rai import get_llm_model # initialize your model of choice defined in config.toml\n\nbase64_image = preprocess_image('https://raw.githubusercontent.com/RobotecAI/RobotecGPULidar/develop/docs/image/rgl-logo.png')\n\nllm = get_llm_model(model_type='complex_model') # initialize your vendor of choice in config.toml\nmsg = [HumanMultimodalMessage(content='Describe the image', images=[base64_image])]\nllm.invoke(msg).pretty_print()\n\n# ================================== Ai Message ==================================\n#\n# The image features the words \"Robotec,\" \"GPU,\" and \"Lidar\" displayed in a stylized,\n# multicolored font against a black background. The text has a wavy, striped pattern,\n# incorporating red, green, and blue colors that give it a vibrantly layered appearance.\n</code></pre> <p>Implementation of the following messages is identical: HumanMultimodalMessage, SystemMultimodalMessage, AIMultimodalMessage.</p> <p>ToolMultimodalMessage usage</p> <p>Most of the vendors, do not support multimodal tool messages. <code>ToolMultimodalMessage</code> has an addition of <code>postprocess</code> method, which converts the <code>ToolMultimodalMessage</code> into format that is compatible with a chosen vendor.</p>"},{"location":"API_documentation/langchain_integration/multimodal_messages/#see-also","title":"See Also","text":"<ul> <li>Agents: For more information on the different types of agents in RAI</li> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Runners: For more information on the different types of runners in RAI</li> </ul>"},{"location":"API_documentation/langchain_integration/overview/","title":"Langchain Integration","text":""},{"location":"API_documentation/langchain_integration/overview/#overview","title":"Overview","text":"<p>RAI integrates with Langchain to enable natural language, reasoning, and multimodal capabilities in robotic applications. This API documentation describes how Langchain is used and extended within RAI, and how to leverage it for agent and tool development.</p>"},{"location":"API_documentation/langchain_integration/overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Standardized Interfaces: Consistent APIs for LLMs and tools.</li> <li>Tool Ecosystem: Use and extend tools for text and multimodal operations.</li> <li>Agent-Based Reasoning: Build agents that combine LLMs, tools, and context/memory.</li> <li>Multimodal Communication: Support for images, audio, and sensor data within messages.</li> <li>Robotic Integration: ROS 2 communication and robotic toolsets.</li> </ul>"},{"location":"API_documentation/langchain_integration/overview/#getting-started","title":"Getting Started","text":"<pre><code>from rai import get_llm_model\n\n# Initialize LLM configured in config.toml\nllm = get_llm_model(model_type='complex_model')\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#agent-and-tool-patterns","title":"Agent and Tool Patterns","text":""},{"location":"API_documentation/langchain_integration/overview/#agent-based-systems","title":"Agent-Based Systems","text":"<pre><code>from rai.agents.langchain.runnables import create_react_runnable\n\nagent = create_react_runnable(\n    llm=llm,\n    tools=[ros2_topic, get_image]\n)\nagent.invoke({\"messages\": [HumanMessage(content=\"Analyze this image\")]})\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#multimodal-communication","title":"Multimodal Communication","text":"<pre><code>from rai.messages import HumanMultimodalMessage, preprocess_image\n\nmessage = HumanMultimodalMessage(\n    content=\"Analyze this image\",\n    images=[preprocess_image(image_uri)]\n)\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#tool-integration","title":"Tool Integration","text":"<pre><code>from langchain_core.tools import tool\n\n@tool\ndef custom_operation(input: str) -&gt; str:\n    # Tool implementation\n    return result\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#multimodal-tool-example","title":"Multimodal Tool Example","text":"<pre><code>from langchain_core.tools import tool\nfrom rai.messages import MultimodalArtifact\n\n@tool(response_format=\"content_and_artifact\")\ndef custom_operation(input: str) -&gt; str:\n    # Tool implementation\n    return result, MultimodalArtifact(images=[base64_encoded_png_image])\n</code></pre>"},{"location":"API_documentation/langchain_integration/overview/#best-practices","title":"Best Practices","text":"<ul> <li>Use appropriate message types for text and media</li> <li>Follow Langchain tool patterns and document capabilities</li> <li>Keep agents focused and specialized</li> </ul>"},{"location":"API_documentation/langchain_integration/overview/#see-also","title":"See Also","text":"<ul> <li>Tool tutorial: For more information on how to create custom LangChain tools</li> <li>Agents</li> <li>Aggregators</li> <li>Connectors</li> <li>Multimodal messages</li> <li>Runners</li> </ul>"},{"location":"API_documentation/runners/overview/","title":"Runners","text":"<p>There are two ways to manage the agents in RAI:</p> <ol> <li><code>AgentRunner</code> - a class for starting and stopping the agents</li> <li><code>wait_for_shutdown</code> - a function for waiting for interruption signals</li> </ol> <p>Usage of <code>Runners</code> is optional</p> <p>You can start and stop the agents manually for more control over the agents lifecycle.</p>"},{"location":"API_documentation/runners/overview/#agentrunner-class-definition","title":"AgentRunner class definition","text":""},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner","title":"<code>rai.agents.runner.AgentRunner</code>","text":"<p>Runs the agents in the background.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>List[BaseAgent]</code> <p>List of agent instances</p> required Source code in <code>rai/agents/runner.py</code> <pre><code>class AgentRunner:\n    \"\"\"Runs the agents in the background.\n\n    Parameters\n    ----------\n    agents : List[BaseAgent]\n        List of agent instances\n    \"\"\"\n\n    def __init__(self, agents: List[BaseAgent]):\n        \"\"\"Initialize the AgentRunner with a list of agents.\n\n        Parameters\n        ----------\n        agents : List[BaseAgent]\n            List of agent instances to be managed by the runner.\n        \"\"\"\n        self.agents = agents\n        self.logger = logging.getLogger(__name__)\n\n    def run(self):\n        \"\"\"Run all agents in the background.\n\n        Notes\n        -----\n        This method starts all agents by calling their `run` method.\n        It is experimental; if agents do not run properly, consider running them in separate processes.\n        \"\"\"\n        self.logger.info(\n            f\"{self.__class__.__name__}.{self.run.__name__} is an experimental function. \\\n                            If you believe that your agents are not running properly, \\\n                            please run them separately (in different processes).\"\n        )\n        for agent in self.agents:\n            agent.run()\n\n    def run_and_wait_for_shutdown(self):\n        \"\"\"Run all agents and block until a shutdown signal is received.\n\n        Notes\n        -----\n        This method starts all agents and waits for a shutdown signal (SIGINT or SIGTERM), ensuring graceful termination.\n        \"\"\"\n        self.run()\n        self.wait_for_shutdown()\n\n    def wait_for_shutdown(self):\n        \"\"\"Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.\n\n        Notes\n        -----\n        Installs signal handlers to capture SIGINT and SIGTERM. On receiving a signal, stops all managed agents.\n        \"\"\"\n        shutdown_event = Event()\n\n        def signal_handler(signum: int, frame: Optional[FrameType]):\n            shutdown_event.set()\n\n        signal.signal(signal.SIGINT, signal_handler)\n        signal.signal(signal.SIGTERM, signal_handler)\n\n        try:\n            shutdown_event.wait()\n        finally:\n            for agent in self.agents:\n                agent.stop()\n\n    def stop(self):\n        \"\"\"Stop all managed agents by calling their `stop` method.\"\"\"\n        for agent in self.agents:\n            agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.__init__","title":"<code>__init__(agents)</code>","text":"<p>Initialize the AgentRunner with a list of agents.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>List[BaseAgent]</code> <p>List of agent instances to be managed by the runner.</p> required Source code in <code>rai/agents/runner.py</code> <pre><code>def __init__(self, agents: List[BaseAgent]):\n    \"\"\"Initialize the AgentRunner with a list of agents.\n\n    Parameters\n    ----------\n    agents : List[BaseAgent]\n        List of agent instances to be managed by the runner.\n    \"\"\"\n    self.agents = agents\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.run","title":"<code>run()</code>","text":"<p>Run all agents in the background.</p> Notes <p>This method starts all agents by calling their <code>run</code> method. It is experimental; if agents do not run properly, consider running them in separate processes.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def run(self):\n    \"\"\"Run all agents in the background.\n\n    Notes\n    -----\n    This method starts all agents by calling their `run` method.\n    It is experimental; if agents do not run properly, consider running them in separate processes.\n    \"\"\"\n    self.logger.info(\n        f\"{self.__class__.__name__}.{self.run.__name__} is an experimental function. \\\n                        If you believe that your agents are not running properly, \\\n                        please run them separately (in different processes).\"\n    )\n    for agent in self.agents:\n        agent.run()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.run_and_wait_for_shutdown","title":"<code>run_and_wait_for_shutdown()</code>","text":"<p>Run all agents and block until a shutdown signal is received.</p> Notes <p>This method starts all agents and waits for a shutdown signal (SIGINT or SIGTERM), ensuring graceful termination.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def run_and_wait_for_shutdown(self):\n    \"\"\"Run all agents and block until a shutdown signal is received.\n\n    Notes\n    -----\n    This method starts all agents and waits for a shutdown signal (SIGINT or SIGTERM), ensuring graceful termination.\n    \"\"\"\n    self.run()\n    self.wait_for_shutdown()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.stop","title":"<code>stop()</code>","text":"<p>Stop all managed agents by calling their <code>stop</code> method.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def stop(self):\n    \"\"\"Stop all managed agents by calling their `stop` method.\"\"\"\n    for agent in self.agents:\n        agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#rai.agents.runner.AgentRunner.wait_for_shutdown","title":"<code>wait_for_shutdown()</code>","text":"<p>Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.</p> Notes <p>Installs signal handlers to capture SIGINT and SIGTERM. On receiving a signal, stops all managed agents.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def wait_for_shutdown(self):\n    \"\"\"Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.\n\n    Notes\n    -----\n    Installs signal handlers to capture SIGINT and SIGTERM. On receiving a signal, stops all managed agents.\n    \"\"\"\n    shutdown_event = Event()\n\n    def signal_handler(signum: int, frame: Optional[FrameType]):\n        shutdown_event.set()\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    try:\n        shutdown_event.wait()\n    finally:\n        for agent in self.agents:\n            agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#wait_for_shutdown-function-definition","title":"wait_for_shutdown function definition","text":""},{"location":"API_documentation/runners/overview/#rai.agents.runner.wait_for_shutdown","title":"<code>rai.agents.runner.wait_for_shutdown(agents)</code>","text":"<p>Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>List[BaseAgent]</code> <p>List of running agents to be stopped on shutdown.</p> required Notes <p>This method ensures a graceful shutdown of both the agent and the ROS2 node upon receiving an interrupt signal (SIGINT, e.g., Ctrl+C) or SIGTERM. It installs signal handlers to capture these events and invokes the agent's <code>stop()</code> method as part of the shutdown process.</p> Source code in <code>rai/agents/runner.py</code> <pre><code>def wait_for_shutdown(agents: List[BaseAgent]):\n    \"\"\"Block until a shutdown signal (SIGINT or SIGTERM) is received, ensuring graceful termination.\n\n    Parameters\n    ----------\n    agents : List[BaseAgent]\n        List of running agents to be stopped on shutdown.\n\n    Notes\n    -----\n    This method ensures a graceful shutdown of both the agent and the ROS2 node upon receiving\n    an interrupt signal (SIGINT, e.g., Ctrl+C) or SIGTERM. It installs signal handlers to\n    capture these events and invokes the agent's ``stop()`` method as part of the shutdown process.\n    \"\"\"\n    shutdown_event = Event()\n\n    def signal_handler(signum, frame):\n        shutdown_event.set()\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    try:\n        shutdown_event.wait()\n    finally:\n        for agent in agents:\n            agent.stop()\n</code></pre>"},{"location":"API_documentation/runners/overview/#usage-examples","title":"Usage examples","text":""},{"location":"API_documentation/runners/overview/#agentrunner","title":"AgentRunner","text":"<pre><code>from rai.agents import AgentRunner\nfrom rai.agents.ros2 import ROS2StateBasedAgent\nfrom rai.agents import ReActAgent\n\nstate_based_agent = ROS2StateBasedAgent()\nreact_agent = ReActAgent()\n\nrunner = AgentRunner([state_based_agent, react_agent])\n\nrunner.run_and_wait_for_shutdown() # starts the agents and blocks until the shutdown signal (Ctrl+C or SIGTERM)\n</code></pre>"},{"location":"API_documentation/runners/overview/#wait_for_shutdown","title":"wait_for_shutdown","text":"<pre><code>from rai.agents import wait_for_shutdown\nfrom rai.agents.ros2 import ROS2StateBasedAgent\nfrom rai.agents import ReActAgent\n\nstate_based_agent = ROS2StateBasedAgent()\nreact_agent = ReActAgent()\n\n# start the agents manually\nstate_based_agent.run()\nreact_agent.run()\n\n# blocks until the shutdown signal (Ctrl+C or SIGTERM)\nwait_for_shutdown([state_based_agent, react_agent])\n</code></pre>"},{"location":"API_documentation/runners/overview/#see-also","title":"See Also","text":"<ul> <li>Agents: For more information on the different types of agents in RAI</li> <li>Aggregators: For more information on the different types of aggregators in RAI</li> <li>Connectors: For more information on the different types of connectors in RAI</li> <li>Langchain Integration: For more information on the LangChain integration within RAI</li> <li>Multimodal messages: For more information on the multimodal LangChain messages in RAI</li> </ul>"},{"location":"ROS_2/ros_packages/","title":"ROS Packages","text":"<p>RAI includes multiple configurable ROS 2 packages.</p> Package Description Documentation rai_open_set_vision Package enabling use of GroundingDINO and GroundedSAM -- an open-set detection model with ROS 2. rai_open_set_vision rai_nomad Package integrating NoMaD -- an exploration model with ROS2. rai_nomad rai_interfaces Definition of custom messages and services used in RAI. rai_bringup Launch files to run RAI."},{"location":"demos/agriculture/","title":"Autonomous Tractor Demo","text":"<p>This demo showcases autonomous tractors operating in an agricultural field using ROS 2. The tractor is controlled using a conventional navigation stack. Sometimes, due to the ever-changing environment, the tractor may encounter unexpected situations. The conventional stack is not designed to handle these situations and usually ends up replanning the path. The tractor can handle this optimally by calling the RAI agent to decide what to do.</p>"},{"location":"demos/agriculture/#quick-start","title":"Quick Start","text":"<p>Remain in sourced shell</p> <p>Ensure that every command is run in a sourced shell using <code>source setup_shell.sh</code> Ensure ROS 2 is sourced.</p> <ol> <li> <p>Follow the RAI setup instructions in the quick setup guide.</p> </li> <li> <p>Download the Latest Release</p> <pre><code>./scripts/download_demo.sh agriculture\n</code></pre> </li> <li> <p>Run the Simulation</p> <pre><code>./demo_assets/agriculture/RAIAgricultureDemo/RAIAgricultureDemo.GameLauncher -bg_ConnectToAssetProcessor=0\n</code></pre> </li> <li> <p>Start the Tractor Node</p> <pre><code>python examples/agriculture-demo.py --tractor_number 1\n</code></pre> </li> </ol> <p>You are now ready to run the demo and see the tractor in action!</p>"},{"location":"demos/agriculture/#running-the-demo","title":"Running the Demo","text":"<p>The demo simulates a scenario where the tractor stops due to an unexpected situation. The RAI Agent decides the next action based on the current state.</p>"},{"location":"demos/agriculture/#rai-agent-decisions","title":"RAI Agent decisions","text":"<p>RAI Agent's mission is to decide the next action based on the current state of the anomaly. There are three exposed services to control the tractor:</p> <ul> <li>continue</li> </ul> <p>Used when the anomaly is flagged as a false positive.</p> <ul> <li>flash</li> </ul> <p>Used to flash the lights on the tractor to e.g. get the attention of the animals</p> <ul> <li>replan</li> </ul> <p>Used to replan the path/skip the alley.</p>"},{"location":"demos/agriculture/#what-happens-in-the-demo","title":"What Happens in the Demo?","text":"<ul> <li>The node listens for the tractor's state and calls the RaiNode using ROS 2 action when an anomaly     is detected.</li> <li>The RaiNode decides the next action based on the current state.</li> <li>The tractor performs the action and the demo continues.</li> </ul> <p>For more details on configuring RAI for specific robots, refer to the API documentation.</p> <p>Building from source</p> <p>If you are having trouble running the binary, you can build it from source here.</p>"},{"location":"demos/debugging_assistant/","title":"ROS 2 Debugging Assistant","text":"<p>The ROS 2 Debugging Assistant is an interactive tool that helps developers inspect and troubleshoot their ROS 2 systems using natural language. It provides a chat-like interface powered by Streamlit where you can ask questions about your ROS 2 setup and execute common debugging commands.</p>"},{"location":"demos/debugging_assistant/#features","title":"Features","text":"<ul> <li>Interactive chat interface for debugging ROS 2 systems</li> <li>Real-time streaming of responses and tool executions</li> <li>Support for common ROS 2 debugging commands:<ul> <li><code>ros2 topic</code>: topic inspection and manipulation</li> <li><code>ros2 service</code>: service inspection and calling</li> <li><code>ros2 node</code>: node information</li> <li><code>ros2 action</code>: action server details and goal sending</li> <li><code>ros2 interface</code>: interface inspection</li> <li><code>ros2 param</code>: checking and setting parameters</li> </ul> </li> </ul>"},{"location":"demos/debugging_assistant/#running-the-assistant","title":"Running the Assistant","text":"<ol> <li> <p>Follow the RAI setup instructions in the quick setup guide.</p> </li> <li> <p>Launch the debugging assistant:</p> </li> </ol> <pre><code>source setup_shell.sh\nstreamlit run examples/debugging_assistant.py\n</code></pre>"},{"location":"demos/debugging_assistant/#usage-examples","title":"Usage Examples","text":"<p>Here are some example queries you can try:</p> <ul> <li>\"What topics are currently available?\"</li> <li>\"Show me the message type for /cmd_vel\"</li> <li>\"List all active nodes\"</li> <li>\"What services does the /robot_state_publisher node provide?\"</li> <li>\"Show me information about the /navigate_to_pose action\"</li> </ul>"},{"location":"demos/debugging_assistant/#how-it-works","title":"How it Works","text":"<p>The debugging assistant uses RAI's conversational agent capabilities combined with ROS 2 debugging tools. The key components are:</p> <ol> <li>Streamlit Interface: Provides the chat UI and displays tool execution results</li> <li>ROS 2 Tools: Collection of debugging tools that wrap common ROS 2 CLI commands</li> <li>Streaming Callbacks: Real-time updates of LLM responses and tool executions</li> </ol>"},{"location":"demos/debugging_assistant/#limitations","title":"Limitations","text":"<ul> <li>The assistant can only execute safe, read-only commands by default</li> <li>Some complex debugging scenarios may require manual intervention</li> <li>Performance depends on the chosen LLM vendor and model</li> </ul>"},{"location":"demos/manipulation/","title":"Manipulation tasks with natural language","text":"<p>This demo showcases the capabilities of RAI in performing manipulation tasks using natural language commands. The demo utilizes a robot arm (Franka Emika Panda) in a simulated environment, demonstrating how RAI can interpret complex instructions and execute them using advanced vision and manipulation techniques.</p> <p></p>"},{"location":"demos/manipulation/#setup","title":"Setup","text":"<p>LLM model</p> <p>The demo uses the <code>complex_model</code> LLM configured in <code>config.toml</code>. The model should be a multimodal, tool-calling model. See Vendors.</p> <p>ROS 2 Sourced</p> <p>Make sure ROS 2 is sourced. (e.g. <code>source /opt/ros/humble/setup.bash</code>)</p> <ol> <li>Follow the RAI setup instructions in the quick setup guide.</li> <li> <p>Download additional dependencies:</p> <pre><code>poetry install --with openset\nvcs import &lt; demos.repos\nrosdep install --from-paths src/examples/rai-manipulation-demo/ros2_ws/src --ignore-src -r -y\n</code></pre> </li> <li> <p>Download the latest binary release</p> <pre><code>./scripts/download_demo.sh manipulation\n</code></pre> </li> <li> <p>Build the ROS 2 workspace:</p> <pre><code>colcon build --symlink-install\n</code></pre> </li> </ol>"},{"location":"demos/manipulation/#running-the-demo","title":"Running the Demo","text":"<p>Remain in sourced shell</p> <p>Ensure that every command is run in a sourced shell using <code>source setup_shell.sh</code> Ensure ROS 2 is sourced.</p> <ol> <li> <p>Start the demo</p> <pre><code>ros2 launch examples/manipulation-demo.launch.py game_launcher:=demo_assets/manipulation/RAIManipulationDemo/RAIManipulationDemo.GameLauncher\n</code></pre> </li> <li> <p>In the second terminal, run the streamlit interface:</p> <pre><code>streamlit run examples/manipulation-demo-streamlit.py\n</code></pre> <p>Alternatively, you can run the simpler command-line version, which also serves as an example of how to use the RAI API for you own applications:</p> <pre><code>python examples/manipulation-demo.py\n</code></pre> </li> <li> <p>Interact with the robot arm using natural language commands. For example:</p> <pre><code>Enter a prompt: Pick up the red cube and drop it on another cube\n</code></pre> </li> </ol> <p>Changing camera view</p> <p>To change camera in the simulation use 1-7 keys on your keyboard once it's window is focused.</p>"},{"location":"demos/manipulation/#how-it-works","title":"How it works","text":"<p>The manipulation demo utilizes several components:</p> <ol> <li>Vision processing using Grounded SAM 2 and Grounding DINO for object detection and segmentation.</li> <li>RAI agent to process the request and plan the manipulation sequence.</li> <li>Robot arm control for executing the planned movements.</li> </ol> <p>The main logic of the demo is implemented in the <code>create_agent</code> function, which can be found in:</p> <pre><code>examples/manipulation-demo.py\n</code></pre>"},{"location":"demos/manipulation/#known-limitations","title":"Known Limitations","text":"<ul> <li><code>Grounding DINO</code> can't distinguish colors.</li> <li>VLMs tend to struggle with spatial understanding (for example left/right concepts).</li> </ul> <p>Building from source</p> <p>If you are having trouble running the binary, you can build it from source here.</p>"},{"location":"demos/overview/","title":"RAI Demos Overview","text":"<p>This document provides an overview of the available RAI demos. Each demo showcases different capabilities of the RAI platform in various robotics applications.</p> Demo Description Key Features Environment Manipulation Robot arm manipulation using natural language - Object detection and segmentation- Natural language command processing- Robotic manipulation planning ROS 2, Franka Emika Panda ROSbot XL Mobile robot operating in an apartment environment - Natural language interaction- Navigation and perception- Streamlit GUI interface ROS 2, Open 3D Engine Autonomous Tractor Autonomous agricultural tractor operating in a field - Path planning and navigation- Anomaly detection and handling- RAI agent decision making ROS 2, Agricultural field simulation ROS2 Debugging Assistant Interactive chat interface for debugging ROS 2 systems - Interactive chat interface for debugging ROS 2 systems ROS 2"},{"location":"demos/overview/#common-requirements","title":"Common Requirements","text":"<p>All demos require:</p> <ul> <li>ROS 2 (Humble or Jazzy)</li> <li>RAI platform setup</li> </ul> <p>Make sure to follow the installation instructions.</p> <p>For detailed setup and running instructions, please refer to the individual demo documentation linked above.</p>"},{"location":"demos/rosbot_xl/","title":"Husarion Robot XL demo","text":"<p>This demo utilizes Open 3D Engine simulation and allows you to work with RAI on a small mobile platform in a nice apartment.</p> <p></p>"},{"location":"demos/rosbot_xl/#quick-start","title":"Quick start","text":"<p>LLM model</p> <p>The demo uses the <code>complex_model</code> LLM configured in <code>config.toml</code>. The model should be a multimodal, tool-calling model. See Vendors.</p> <p>ROS 2 Sourced</p> <p>Make sure ROS 2 is sourced. (e.g. <code>source /opt/ros/humble/setup.bash</code>)</p> <ol> <li> <p>Follow the RAI setup instructions in the quick setup guide.</p> </li> <li> <p>Download the newest binary release:</p> <pre><code>./scripts/download_demo.sh rosbot\n</code></pre> </li> <li> <p>Install and download required packages</p> <pre><code>sudo apt install ros-${ROS_DISTRO}-navigation2 ros-${ROS_DISTRO}-nav2-bringup\nvcs import &lt; demos.repos\nrosdep install --from-paths src --ignore-src -r -y\npoetry install --with openset\n</code></pre> </li> </ol> <p>Alternative: Demo source build</p> <p>If you would like more freedom to adapt the simulation to your needs, you can make changes using O3DE Editor and build the project yourself. Please refer to rai husarion rosbot xl demo for more details.</p>"},{"location":"demos/rosbot_xl/#running-rai","title":"Running RAI","text":"<ol> <li> <p>Running rai nodes and agents, navigation stack and O3DE simulation.</p> <pre><code>ros2 launch ./examples/rosbot-xl.launch.py game_launcher:=demo_assets/rosbot/RAIROSBotXLDemo/RAIROSBotXLDemo.GameLauncher\n</code></pre> </li> <li> <p>Run streamlit gui:</p> <pre><code>streamlit run examples/rosbot-xl-demo.py\n</code></pre> </li> <li> <p>Play with the demo, prompting the agent to perform tasks. Here are some examples:</p> <ul> <li>Where are you now?</li> <li>What do you see?</li> <li>What is the position of bed?</li> <li>Navigate to the kitchen.</li> <li>Please bring me coffee something from the kitchen (this one should be rejected thanks to robot embodiment module)</li> </ul> </li> </ol> <p>Changing camera view</p> <p>To change camera in the simulation use 1,2,3 keys on your keyboard once it's window is focused.</p>"},{"location":"demos/rosbot_xl/#how-it-works","title":"How it works","text":"<p>The rosbot demo utilizes several components:</p> <ol> <li>Vision processing using Grounded SAM 2 and Grounding DINO for object detection and segmentation. See RAI OpenSet Vision.</li> <li>RAI agent to process the request and interact with environment via tool-calling mechanism.</li> <li>Navigation is enabled via nav2 toolkit, which interacts with ROS 2 nav2 asynchronously by calling ros2 actions.</li> <li>Embodiment of the Rosbot is achieved using RAI Whoami module. This makes RAI agent aware of the hardware platform and its capabilities.</li> <li>Key informations can be provided to the agent in RAI Whoami. In this demo coordinates of <code>Kitchen</code> and <code>Living Room</code> are provided as <code>capabilities</code>.</li> </ol> <p>The details of the demo can be checked in <code>examples/rosbot-xl-demo.py</code>.</p>"},{"location":"extensions/nomad/","title":"RAI NoMaD","text":"<p>This package provides a ROS2 Node which loads and runs the NoMaD model, that can be dynamically activated and deactivated using ROS2 messages.</p>"},{"location":"extensions/nomad/#running-instructions","title":"Running instructions","text":""},{"location":"extensions/nomad/#1-setup-the-ros2-workspace","title":"1. Setup the ROS2 workspace","text":"<p>In the base directory of the <code>RAI</code> package install dependencies:</p> <pre><code>poetry install --with nomad\n</code></pre> <p>Source the ros installation</p> <pre><code>source /opt/ros/${ROS_DISTRO}/setup.bash\n</code></pre> <p>Run the build process:</p> <pre><code>colcon build\n</code></pre> <p>Setup your shell:</p> <pre><code>source ./setup_shell.sh\n</code></pre>"},{"location":"extensions/nomad/#2-run-the-node","title":"2. Run the node","text":"<p>Run the ROS2 node using <code>ros2 run</code>:</p> <pre><code>ros2 run rai_nomad nomad --ros-args -p image_topic:=&lt;your_image_topic&gt;\n</code></pre> <p>The model will be loaded and ready, but it will not run until you call the <code>/rai_nomad/start</code> service. Then it will start outputting velocity commands and your robot should start moving. You can then stop the model by calling the <code>/rai_nomad/stop</code> service.</p>"},{"location":"extensions/nomad/#parameters","title":"Parameters","text":"<ul> <li><code>model_path</code>: Path to where the model should be downloaded. By default it will be downloaded into the package's <code>share</code> directory.</li> <li><code>image_topic</code>: The topic where the camera images are being published.</li> <li><code>cmd_vel_topic</code>: The topic where the velocity commands are being published. Default: <code>/cmd_vel</code>.</li> <li><code>linear_vel</code>: Linear velocity scaling of the model output.</li> <li><code>angular_vel</code>: Angular velocity scaling of the model output.</li> <li><code>max_v</code> and <code>max_w</code>: Maximum linear and angular velocities that the model can output.</li> <li><code>rate</code>: The rate at which the model will run.</li> </ul>"},{"location":"extensions/openset/","title":"RAI Open Set Vision","text":"<p>This package provides a ROS2 Node which is an interface to the Idea-Research GroundingDINO Model. It allows for open-set detection.</p>"},{"location":"extensions/openset/#installation","title":"Installation","text":"<p>In your workspace you need to have an <code>src</code> folder containing this package <code>rai_open_set_vision</code> and the <code>rai_interfaces</code> package.</p>"},{"location":"extensions/openset/#preparing-the-groundingdino","title":"Preparing the GroundingDINO","text":"<p>Add required ROS dependencies:</p> <pre><code>rosdep install --from-paths src --ignore-src -r\n</code></pre>"},{"location":"extensions/openset/#build-and-run","title":"Build and run","text":"<p>In the base directory of the <code>RAI</code> package install dependencies:</p> <pre><code>poetry install --with openset\n</code></pre> <p>Source the ros installation</p> <pre><code>source /opt/ros/${ROS_DISTRO}/setup.bash\n</code></pre> <p>Run the build process:</p> <pre><code>colcon build --symlink-install\n</code></pre> <p>Source the environment</p> <pre><code>source setup_shell.sh\n</code></pre> <p>Run the <code>GroundedSamAgent</code> and <code>GroundingDinoAgent</code> agents.</p> <pre><code>python run_vision_agents.py\n</code></pre> <p>Agents create two ROS 2 Nodes: <code>grounding_dino</code> and <code>grounded_sam</code> using ROS2Connector. These agents can be triggered by ROS2 services:</p> <ul> <li><code>grounding_dino_classify</code>: <code>rai_interfaces/srv/RAIGroundingDino</code></li> <li><code>grounded_sam_segment</code>: <code>rai_interfaces/srv/RAIGroundedSam</code></li> </ul> <p>Tip</p> <p>If you wish to integrate open-set vision into your ros2 launch file, a premade launch file can be found in <code>rai/src/rai_bringup/launch/openset.launch.py</code></p> <p>Note</p> <p>The weights will be downloaded to <code>~/.cache/rai</code> directory.</p>"},{"location":"extensions/openset/#rai-tools","title":"RAI Tools","text":"<p><code>rai_open_set_vision</code> package contains tools that can be used by RAI LLM agents enhance their perception capabilities. For more information on RAI Tools see Tool use and development tutorial.</p>"},{"location":"extensions/openset/#getdetectiontool","title":"<code>GetDetectionTool</code>","text":"<p>This tool calls the grounding dino service to use the model to see if the message from the provided camera topic contains objects from a comma separated prompt.</p> <p>Tip</p> <p>you can try example below with rosbotxl demo binary. The binary exposes <code>/camera/camera/color/image_raw</code> and <code>/camera/camera/depth/image_raw</code> topics.</p> <p>Example call</p> <pre><code>from rai_open_set_vision.tools import GetDetectionTool\nfrom rai.communication.ros2 import ROS2Connector, ROS2Context\n\nwith ROS2Context():\n    connector=ROS2Connector(node_name=\"test_node\")\n    x = GetDetectionTool(connector=connector)._run(\n        camera_topic=\"/camera/camera/color/image_raw\",\n        object_names=[\"chair\", \"human\", \"plushie\", \"box\", \"ball\"],\n    )\n</code></pre> <p>Example output</p> <pre><code>I have detected the following items in the picture - chair, human\n</code></pre>"},{"location":"extensions/openset/#getdistancetoobjectstool","title":"<code>GetDistanceToObjectsTool</code>","text":"<p>This tool calls the grounding dino service to use the model to see if the message from the provided camera topic contains objects from a comma separated prompt. Then it utilises messages from depth camera to create an estimation of distance to a detected object.</p> <p>Example call</p> <pre><code>from rai_open_set_vision.tools import GetDetectionTool\nfrom rai.communication.ros2 import ROS2Connector, ROS2Context\n\nwith ROS2Context():\n    connector=ROS2Connector(node_name=\"test_node\")\n    connector.node.declare_parameter(\"conversion_ratio\", 1.0) # scale parameter for the depth map\n    x = GetDistanceToObjectsTool(connector=connector)._run(\n        camera_topic=\"/camera/camera/color/image_raw\",\n        depth_topic=\"/camera/camera/depth/image_rect_raw\",\n        object_names=[\"chair\", \"human\", \"plushie\", \"box\", \"ball\"],\n    )\n</code></pre> <p>Example output</p> <pre><code>I have detected the following items in the picture human: 3.77m away\n</code></pre>"},{"location":"extensions/openset/#simple-ros2-client-node-example","title":"Simple ROS2 Client Node Example","text":"<p>An example client is provided with the package as <code>rai_open_set_vision/talker.py</code></p> <p>You can see it working by running:</p> <pre><code>python run_vision_agents.py\ncd rai # rai repo BASE directory\nros2 run rai_open_set_vision talker --ros-args -p image_path:=src/rai_extensions/rai_open_set_vision/images/sample.jpg\n</code></pre> <p>If everything was set up properly you should see a couple of detections with classes <code>dinosaur</code>, <code>dragon</code>, and <code>lizard</code>.</p>"},{"location":"faq/ROS_2_Overview/","title":"Overview","text":"<p>RAI provides an abstraction layer for ROS 2 which streamlines the process of integrating ROS 2 with LLMs and other AI components. This integration bridge is essential for modern robotics systems that need to leverage artificial intelligence capabilities alongside traditional robotics frameworks.</p> <p>At the heart of this integration is the <code>rai.communication.ros2.ROS2Connector</code>, which provides a unified interface to ROS 2's subscription, service, and action APIs. This abstraction layer makes it straightforward to build ROS 2 agents and LangChain tools that can seamlessly communicate with both robotics and AI components.</p> Why is RAI not a ROS 2 package? <p>RAI was initially developed as a ROS 2 package, but this approach proved problematic for several key reasons:</p> <ol> <li> <p>Dependency Management</p> <ul> <li>ROS 2's dependency system (rosdep) is too vague and inflexible for RAI's needs</li> <li>RAI heavily relies on the Python ecosystem, particularly for AI/LLM integration</li> <li>Poetry provides more precise version control and dependency resolution, crucial for AI/ML components</li> </ul> </li> <li> <p>Architectural Separation</p> <ul> <li>The initial ROS 2 package approach made it difficult to maintain clear separation between:<ul> <li>Core AI/LLM functionality</li> <li>Communication layer</li> <li>Robot-specific implementations</li> </ul> </li> <li>This separation is crucial for maintainability and extensibility</li> </ul> </li> <li> <p>Development Workflow</p> <ul> <li>ROS 2's build system adds unnecessary complexity for Python-based AI development</li> <li>The mixed C++/Python ecosystem of ROS 2 doesn't align well with RAI's Python-first approach</li> <li>Faster development cycles are possible with a Python-centric architecture</li> </ul> </li> <li> <p>Flexibility and Portability</p> <ul> <li>RAI needs to work both with and without ROS 2 which allows vast amount of use cases which contribute to the RAI ecosystem</li> <li>The framework should be deployable in various environments</li> <li>Different communication protocols should be easily supported</li> </ul> </li> </ol> <p>The current architecture, with RAI as a Python framework that works with ROS 2 rather than as ROS 2, provides the best of both worlds:  - Full ROS 2 compatibility through the <code>ROS2Connector</code> abstraction  - Clean separation of concerns  - Flexible dependency management  - Better maintainability and extensibility</p>"},{"location":"faq/ROS_2_Overview/#key-features","title":"Key Features","text":"<ul> <li>Unified Communication Interface: Simple, generic interface for ROS 2 topics, services, and actions</li> <li>Automatic QoS Matching: Handles Quality of Service profiles automatically</li> <li>Thread-Safe Operations: Built-in thread safety for concurrent operations</li> </ul>"},{"location":"faq/ROS_2_Overview/#supported-ros-2-features","title":"Supported ROS 2 Features","text":"<ul> <li>Topics (publish/subscribe)</li> <li>Services (request/response)</li> <li>Actions (long-running operations with feedback)</li> <li>TF2 transforms</li> </ul>"},{"location":"faq/ROS_2_Overview/#integration-examples","title":"Integration Examples","text":"<pre><code>from rai.communication.ros2 import ROS2Connector, ROS2Context, ROS2Message\n\n\n@ROS2Context()\ndef main():\n    connector = ROS2Connector()\n\n    # Subscribe to a topic\n    def my_custom_callback(message: ROS2Message):\n        message.payload  # actual ROS 2 message\n\n    connector.register_callback(\"/topic\", my_custom_callback)\n\n    # Receive a message\n    message = connector.receive_message(\"/topic\")\n    message.payload  # actual ROS 2 message\n\n    # Publish a message\n    message = ROS2Message(payload={\"data\": \"Hello, ROS 2!\"})\n    connector.send_message(message, \"/topic\", msg_type=\"std_msgs/msg/String\")\n\n    # Call a service\n    request = ROS2Message(payload={\"data\": True})\n    response = connector.service_call(\n        message=request, target=\"/service\", msg_type=\"std_srvs/msg/SetBool\"\n    )\n\n    # Start an action\n    def my_custom_feedback_callback(feedback: ROS2Message):\n        feedback.payload  # actual ROS 2 feedback\n\n    message = ROS2Message(\n        payload={\"pose\": {\"position\": {\"x\": 1.0, \"y\": 2.0, \"z\": 0.0}}}\n    )\n    action_id = connector.start_action(\n        action_data=message,\n        target=\"/navigate_to_pose\",\n        msg_type=\"nav2_msgs/action/NavigateToPose\",\n        feedback_callback=my_custom_feedback_callback,\n    )\n\n    # Cancel an action\n    connector.terminate_action(action_id)\n</code></pre>"},{"location":"faq/ROS_2_Overview/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Context Management</p> <ul> <li>Always use <code>ROS2Context</code> as a decorator or context manager</li> <li>Ensures proper initialization and cleanup</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Use the connector's shutdown method when done</li> <li>Handle exceptions appropriately</li> </ul> </li> <li> <p>Performance Considerations</p> <ul> <li>Use appropriate QoS profiles</li> <li>Consider deregistering callbacks when not needed</li> </ul> </li> </ol>"},{"location":"faq/ROS_2_Overview/#common-use-cases","title":"Common Use Cases","text":"<ol> <li> <p>Robot Control</p> <ul> <li>Navigation commands</li> <li>Manipulation tasks</li> <li>Sensor data processing</li> </ul> </li> <li> <p>System Integration</p> <ul> <li>Connecting AI components to ROS 2</li> <li>Multi-agent coordination</li> </ul> </li> </ol>"},{"location":"faq/ROS_2_Overview/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Connection Problems</p> <ul> <li>Check ROS 2 network configuration</li> <li>Verify topic/service names</li> <li>Ensure proper QoS matching</li> </ul> </li> <li> <p>Performance Issues</p> <ul> <li>Monitor thread usage</li> <li>Check QoS settings</li> <li>Verify message sizes</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Proper cleanup of resources</li> <li>Handling of node lifecycle</li> <li>Memory management</li> </ul> </li> </ol>"},{"location":"faq/ROS_2_Overview/#future-developments","title":"Future Developments","text":"<ul> <li>Performance optimizations</li> </ul>"},{"location":"faq/faq/","title":"RAI FAQ","text":"What is RAI and how does it work? What is RAI? <p>RAI is a flexible AI agent framework to develop and deploy Embodied AI features for your robots.</p> How does RAI work? <p></p> <p>RAI connects human instructions to robot actions in four simple steps:</p> <ul> <li>Human gives a command: The user interacts with the robot using speech or text.</li> <li>RAI understands and reasons: RAI interprets the request, plans the response, and decides on the best action.</li> <li>RAI controls the robot: It sends commands to the robot through ROS 2 or the robot's SDK.</li> <li>Robot responds: The robot performs the action and can communicate back to the user, closing the loop.</li> </ul> Who is RAI for? <p>RAI is for robotics developers, researchers, educators, startups, companies, and hobbyists who want to add AI and natural language capabilities to robots, whether using ROS 2 or other robotics stacks. And anyone who\u2019s ever looked at a robot and thought, \u201cI wish you understood me.\u201d</p> What can I do with RAI? <p>With RAI, you can control robots using voice or text, integrate vision, speech, and sensor data, build and test complex robot behaviors, use ready-made demos, develop multi-agent systems, create custom tools, implement real-time decision making, and design interactive scenarios for human-robot interaction. Or just teach your robot to bring you snacks. We don\u2019t judge.</p> What demos are available for RAI? <p>RAI offers demos for manipulation tasks, autonomous navigation with ROSbot XL and agricultural robotics. See the documentation for details on each demo.</p> <p>Demos from ROSCon 2024</p> <p></p> How do I build my own solution with RAI? <p>Follow the step-by-step walkthrough to deploy RAI on your robot, add custom tools and features, and create advanced multi-agent systems for complex tasks.</p> Does using RAI cost money? <p>RAI is free and open source software. However, if you use RAI with cloud-based models or third-party services, those may incur costs depending on the provider, model, and your usage. Please review the terms and pricing of any external services you choose to use with RAI. To use RAI cost-free, make sure to use local models and services.</p> How can I get started with RAI? How do I install RAI? <p>Check out the Quick Setup Guide to install RAI, try a demo, or follow the walkthrough to build your own solution.</p> How do I try a demo? <p>Check out the Quick Setup Guide to install RAI, try a demo, or follow the walkthrough to build your own solution.</p> How do I build my own solution? <p>Follow the step-by-step walkthrough to deploy RAI on your robot, add custom tools and features, and create advanced multi-agent systems for complex tasks.</p> How do I extend RAI? Missing a feature? <p>Start by checking our API documentation to see if the feature you need already exists. If not, you're welcome to open a new issue or contribute your own solution! The step-by-step walkthrough is a great way to get familiar with RAI's structure. With these resources, you'll be well-equipped to add new features or customize RAI for your needs. If you have questions, don't hesitate to reach out to the community!</p> How do I integrate a new type of sensor? <p>Integration depends on what you want to achieve with your sensor data. If you'd like to use the data with an LLM or multimodal model, you'll need to write a small adapter that converts the sensor output into a format the model can understand (such as text or images). For other use cases, you can create a custom tool or agent that processes the sensor data as needed. If you need guidance, our community is happy to help\u2014just ask on Discord or open a discussion!</p> <p>Note</p> <p>At the moment, RAI does not implement a module for converting sensor data into text or images. These conversions are done on the application level/user side.</p> How do I contribute to RAI? Contributtion guide <p>To develop RAI, you just need to know how to run <code>pre-commit</code> (spoiler alert: just type <code>pre-commit</code>), and you\u2019re basically a core developer. For anything more complicated (or if you like reading rules), see the Contribution Guide.</p> Existing issues/bugs/something is missings <p>If you find an issue/bug/feature missing, please check the existing issues and if it is not already reported, create a new one.</p> I would like to contribute, but I don't know what <p>If you would like to contribute, but don't know what to do, please join our Discord and we will help you find something to work on. Alternatively, browse the issues for inspiration.</p> I've never contributed to open source before, how do I start? <p>Welcome! We love helping newcomers get started with open source. The RAI community is friendly and supportive\u2014no prior experience with RAI required. Check out our Contribution Guide for step-by-step instructions, and feel free to join our Discord to ask questions or get guidance. We're here to help you every step of the way!</p> How do I get support for RAI? <p>Where can I learn more about RAI?</p> <p>You can find support by joining our Discord, or visiting the Embodied AI Community Group. For talks and demos, see the ROSCon 2024 links in the documentation.</p> <p>Where can I get support?</p> <p>You can find support and more information in the Contribution Guide, on the Q&amp;A forum, issues, by joining our Discord, or visiting the Embodied AI Community Group. For talks and demos, see the ROSCon 2024 links in the documentation.</p> What do I need to get started with RAI? <p>What are the requirements?</p> <ul> <li>ROS 2 Humble or Jazzy</li> <li>Python 3.10 or 3.12</li> <li>Ubuntu 22.04 or 24.04</li> </ul> <p>How about a robot?</p> <p>RAI is compatible with any robot that has a ROS 2 interface or exposes another type of interface. The important thing is that your robot must already have a robotic stack set up. RAI builds on top of your existing robotic stack, but does not include the stack itself.</p> <p>Do I even need a robot?</p> <p>No, you can also run the demos on your computer or in custom-made simulations. Or just pretend. We won\u2019t tell.</p> RAI outside of robotics <p>Can I use RAI outside of robotics?</p> <p>Yes, you can use RAI in non-robotic applications. RAI supports ROS 2, but is not limited to it.</p> <p>How do I use RAI outside of robotics?</p> <p>RAI is designed to be flexible. There are multiple abstractions, that make working with multimodal data, multi-agent system, various communication protocols easy. For more information see the RAI API documentation</p> Is RAI limited to LLMs? <p>Can I use models other than LLMs in RAI?</p> <p>No, RAI is not limited to large language models. You can integrate any type of model\u2014such as vision models, speech recognition, classical AI, or custom algorithms\u2014into your RAI agents. The BaseAgent abstraction is designed to make it easy to plug in any form of \"intelligence\" or decision logic.</p> <p>How do I use other models in RAI?</p> <p>To use a different model, simply implement your logic within a custom agent using the BaseAgent abstraction. This allows you to connect vision, speech, or any other AI model to your robot or application. For more details and code examples, see the RAI API documentation.</p> Licensing &amp; Commercial Use <p>Can I use RAI commercially?</p> <p>Yes, RAI is licensed under the Apache 2.0 license.</p> <p>Are there any restrictions on commercial use?</p> <p>No, RAI is licensed under the Apache 2.0 license, which allows for commercial use. Make sure to follow the license terms and give credits to the original authors.</p> Security &amp; Privacy <p>How does RAI handle security and privacy?</p> <p>RAI can use both local and cloud models. For maximum privacy, we recommend using the most performing local models. For cloud models, we recommend using tested and trusted providers.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/","title":"RAI Contributors Code of Conduct","text":"<p>RAI adopts the ROS Code of Conduct maintained by the OSRF. We expand the scope of the Code of Conduct to all official RAI communication channels, including, but not limited to:</p> <ul> <li>RAI source code repositories</li> <li>RAI discord and other communication channels</li> </ul> <p>What follows is the adopted ROS Code of Conduct (retrieved: 2024-08-22). It also includes contact information to RAI internal Conduct Team.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#ros-code-of-conduct","title":"ROS Code of Conduct","text":"<ul> <li>ROS Code of Conduct</li> <li>Python Software Foundation Code of Conduct</li> <li>The RUST language Code of Conduct</li> <li>Contributor Covenant</li> <li>Frame Shift Consulting Code of Conduct Book</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#code-of-conduct-for-ros","title":"Code of Conduct for ROS","text":"<p>View the Project on GitHub</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#ros-community-code-of-conduct","title":"ROS Community Code of Conduct","text":""},{"location":"faq/contributing/CODE_OF_CONDUCT/#preamble-attribution","title":"Preamble / Attribution","text":"<p>The ROS Code of Conduct draws heavily from the work of other open source software communities and tries to synthesize their efforts to address the specific needs of the ROS community. In particular, we draw heavily from the following prior work:</p> <ul> <li>Python Software Foundation Code of Conduct</li> <li>The RUST Language Code of Conduct</li> <li>Contributor Covenant</li> <li>Frame Shift Consulting Code of Conduct Book</li> </ul> <p>It is worth noting that this is a living document to be used to protect community members. It should not be interpreted as a hard and fast legal guide for community behavior. Instead, it should be interpreted as a broad outline of acceptable and unacceptable behavior, how to report unacceptable behavior, and how it will be dealt with.</p> <p>The ROS Code of Conduct is released under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#scope-of-the-code-of-conduct","title":"Scope of the Code of Conduct","text":"<p>This guide applies to all people, events, web properties, communication media, and source code repositories administered by Open Robotics and the ROS 2 Technical Steering Committee. This includes but is not limited to:</p> <ul> <li>ROS source code repositories</li> <li>ROS Discourse</li> <li>ROS Wiki</li> <li>ROS Documentation</li> <li>ROS Answers</li> <li>ROSCon</li> <li>ROS.org</li> <li>All ROS 2 TSC working groups</li> <li>All Open Robotics administered e-mail lists</li> <li>Comments made on event video hosting services</li> <li>Comments made on the official event or ROS hashtags</li> </ul> <p>If you administer a ROS affiliated organization outside of the organizations listed above and would like to adopt this code of conduct for your own project, please contact us at conduct@robotec.ai. We will require the contact information for at least one administrator for your project.</p> <p>Outside organizations that have adopted this code of conduct include the following organizations:</p> <ul> <li>None at this time.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#code-of-conduct","title":"Code of Conduct","text":"<p>The ROS community is made up of members from around the globe with a diverse set of skills, personalities, and experiences. It is through these differences that our community experiences great successes and continued growth. When you\u2019re working with members of the community, this Code of Conduct will help steer your interactions and keep ROS a positive, successful, and growing community.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#our-community","title":"Our Community","text":"<p>Members of the ROS community are open, considerate, and respectful. Behaviors that reinforce these values contribute to a positive environment, and include:</p> <ul> <li>Being open. Members of the community are open to collaboration, whether it\u2019s on REPs, patches, problems, or otherwise.</li> <li>Focusing on what is best for the community. We\u2019re respectful of the processes set forth in the community, and we work within them.</li> <li>Acknowledging time and effort. We\u2019re respectful of the volunteer efforts that permeate the ROS community. We\u2019re thoughtful when addressing the efforts of others, keeping in mind that oftentimes the labor was completed simply for the good of the community.</li> <li>Being respectful of differing viewpoints and experiences. We\u2019re receptive to constructive comments and criticism, as the experiences and skill sets of other members contribute to the whole of our efforts.</li> <li>Showing empathy towards other community members. We\u2019re attentive in our communications, whether in person or online, and we\u2019re tactful when approaching differing views.</li> <li>Being considerate. Members of the community are considerate of their peers \u2013 other ROS users.</li> <li>Being respectful. We\u2019re respectful of others, their positions, their skills, their commitments, and their efforts.</li> <li>Gracefully accepting constructive criticism. When we disagree, we are courteous in raising our issues.</li> <li>Using welcoming and inclusive language. We\u2019re accepting of all who wish to take part in our activities, fostering an environment where anyone can participate and everyone can make a difference.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Every member of our community has the right to have their identity respected. The ROS community is dedicated to providing a positive experience for everyone, regardless of age, gender identity and expression, sexual orientation, disability, physical appearance, body size, ethnicity, nationality, race, or religion (or lack thereof), education, or socio-economic status.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#inappropriate-behavior","title":"Inappropriate Behavior","text":"<p>Examples of unacceptable behavior by participants include:</p> <ul> <li>Harassment of any participants in any form.</li> <li>Deliberate intimidation, stalking, or following.</li> <li>Logging or taking screenshots of online activity for harassment purposes.</li> <li>Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission.</li> <li>Violent threats or language directed against another person.</li> <li>Incitement of violence or harassment towards any individual, including encouraging a person to commit suicide or to engage in self-harm.</li> <li>Creating additional online accounts in order to harass another person or circumvent a ban.</li> <li>Sexual language and imagery in online communities or in any conference venue, including talks.</li> <li>Insults, put downs, or jokes that are based upon stereotypes, that are exclusionary, or that hold others up for ridicule.</li> <li>Excessive swearing.</li> <li>Unwelcome sexual attention or advances.</li> <li>Unwelcome physical contact, including simulated physical contact (e.g., textual descriptions like \u201chug\u201d or \u201cbackrub\u201d) without consent or after a request to stop.</li> <li>Pattern of inappropriate social contact, such as requesting/assuming inappropriate levels of intimacy with others.</li> <li>Sustained disruption of online community discussions, in-person presentations, or other in-person events.</li> <li>Continued one-on-one communication after requests to cease.</li> <li>Other conduct that is inappropriate for a professional audience, including people of many different backgrounds.</li> </ul> <p>Community members asked to stop any inappropriate behavior are expected to comply immediately.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#weapons-policy","title":"Weapons Policy","text":"<p>No weapons are allowed at ROS physical events. Weapons include but are not limited to explosives (including fireworks), guns, and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately and will only be allowed to return without the weapon.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>The Code of Conduct Team is responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>The Code of Conduct Team has the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#current-code-of-conduct-team","title":"Current Code of Conduct Team","text":"<p>The Code of Conduct Team consists of three volunteers from the ROS community. Optimally, these members are located at multiple locations across the globe to provide for a timely response to conduct questions and violations, and provide native language support as best we can. The Conduct Team members serve a two-year term with replacements nominated by the ROS 2 Technical Steering Committee. The Conduct Team works to adjudicate conduct violations using a consensus model. For most situations, that is those that don\u2019t require an immediate response, the Conduct Team will issue reports and enforcement actions representing the consensus of the team.</p> <p>The current Code of Conduct Team consists of:</p> <ul> <li>Wiktoria Siekierska wiktoria.siekierska@robotec.ai</li> <li>Adam Kuty\u0142owski adam.kutylowski@robotec.ai</li> <li>Maciej Majek maciej.majek@robotec.ai</li> </ul> <p>The entire team can be contacted using conduct@robotec.ai. The team can arrange for other means of communications after the initial contact. We recommend you use the conduct@robotec.ai address unless you wish to report a Conduct Team member, or you feel uncomfortable communicating with a certain team member.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the Conduct Team responsible for enforcement at conduct@robotec.ai. All complaints will be reviewed and responded to within 48 hours. The Conduct Team is obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>The Conduct Team will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#correction","title":"Correction","text":"<ul> <li>Your contact info (so we can get in touch with you if we need to follow up)</li> <li>Date and time of the incident</li> <li>Any links, screen shots, videos, or other media that may help</li> <li>Location of the incident</li> <li>Whether the incident is ongoing</li> <li>Description of the incident</li> <li>Identifying information of the reported person</li> <li>Additional circumstances surrounding the incident</li> <li> <p>Other people involved in or witnesses to the incident and their contact information or description</p> </li> <li> <p>Example Behavior: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> </li> <li>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#warning","title":"Warning","text":"<ul> <li>Example Behavior: A violation through a single incident or series of actions.</li> <li>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#temporary-ban","title":"Temporary Ban","text":"<ul> <li>Example Behavior: A serious violation of community standards, including sustained inappropriate behavior.</li> <li>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#permanent-ban","title":"Permanent Ban","text":"<ul> <li>Example Behavior: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</li> <li>Consequence: A permanent ban from any sort of public interaction within the community.</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#how-to-report-a-conduct-violation","title":"How To Report A Conduct Violation","text":"<p>If you believe someone is in physical danger, including from themselves, the most important thing is to get that person help. Please contact the appropriate crisis number, non-emergency number, or police number as appropriate. If you are at a ROS event, you can consult with a volunteer or staff member to help find an appropriate number.</p> <p>If you believe someone has violated the ROS Code of Conduct, we encourage you to report it. If you are unsure whether the incident is a violation, or whether the space where it happened is covered by the Code of Conduct, we encourage you to still report it. We are fine with receiving reports where we decide to take no action for the sake of creating a safer space.</p> <p>The ROS-related forums and events listed above should have a designated moderator or Code of Conduct point of contact. Larger gatherings, like conferences, may have several people to contact. Specific information should be available for each listed gathering, online or off.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#report-template","title":"Report Template","text":"<p>When you make a report via email or phone, please provide as much information as possible to help us make a fair and accurate decision about the appropriate response. The following template should serve as a guide for reporting:</p> <ul> <li>Your contact info (so we can get in touch with you if we need to follow up)</li> <li>Date and time of the incident</li> <li>Any links, screenshots, videos, or other media that may help</li> <li>Location of the incident</li> <li>Whether the incident is ongoing</li> <li>Description of the incident</li> <li>Identifying information of the reported person</li> <li>Additional circumstances surrounding the incident</li> <li>Other people involved in or witnesses to the incident and their contact information or description</li> </ul>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#confidentiality","title":"Confidentiality","text":"<p>All reports will be kept confidential. When we discuss incidents with people who are reported, we will anonymize details as much as we can to protect reporter privacy.</p> <p>However, some incidents happen in one-on-one interactions, and even if the details are anonymized, the reported person may be able to guess who made the report. If you have concerns about retaliation or your personal safety, please note those in your report. In some cases, we can compile several anonymized reports into a pattern of behavior and take action on that pattern.</p> <p>In some cases, we may determine that a public statement will need to be made. If that\u2019s the case, the identities of all victims and reporters will remain confidential unless those individuals instruct us otherwise.</p>"},{"location":"faq/contributing/CODE_OF_CONDUCT/#report-handling-procedure","title":"Report Handling Procedure","text":"<p>When you make a report to the Conduct Team, they will gather information about the incident. If the incident is ongoing and needs to be immediately addressed, any Conduct Team member may take appropriate action to ensure the safety of everyone involved. If the situation requires it, this may take the form of a referral to an appropriate agency, including the local police. The Conduct Team is not equipped to handle emergency situations.</p> <p>If the incident is less urgent, the report will be discussed by the Conduct Team to determine an appropriate response. You should receive a response from the Conduct Team within 48 hours confirming the receipt of the report, and potentially asking for follow-up information. Within one week of an incident report, a member of the Conduct Team will follow up with the person who made the report. The follow-up may include:</p> <ul> <li>An acknowledgment that the Code of Conduct responders discussed the situation</li> <li>Whether or not the report was determined to be a violation of the Code of Conduct</li> <li>What actions (if any) were taken to correct the reported behavior</li> </ul>"},{"location":"faq/contributing/CONTRIBUTING/","title":"Contributing","text":"<p>We are very happy you want to contribute to RAI, and we welcome all input. This document outlines guidelines for contributors to follow.</p> <p>Our philosophy is strongly aligned with the philosophy of the ROS development process. These guidelines have been therefore strongly influenced by the ROS2 Contributing document.</p>"},{"location":"faq/contributing/CONTRIBUTING/#tenets","title":"Tenets","text":"<ul> <li> <p>Engage Robotec.ai as early as possible</p> <ul> <li>Start discussions with Robotec.ai and the community early. Long time RAI contributors may have a     clearer vision of the big picture. If you implement a feature and send a pull request without     discussing with the community first, you are taking the risk of it being rejected, or you may be     asked to largely rethink your design.</li> <li>Opening issues or using Discourse to socialize an idea before starting the implementation is     generally preferable.</li> </ul> </li> <li> <p>Adopt community best-practices whenever possible instead of ad-hoc processes</p> <p>Think about the end-users experience when developing and contributing. Features accessible to a larger amount of potential users, utilising widely available solutions are more likely to be accepted.</p> </li> <li> <p>Think about the community as a whole</p> <p>Think about the bigger picture. There are developers building different robots with different constraints. The landscape of available AI models is rapidly changing, coming with different capabilities and constraints. RAI wants to accommodate requirements of the whole community.</p> </li> </ul> <p>There are a number of ways you can contribute to the RAI project.</p>"},{"location":"faq/contributing/CONTRIBUTING/#discussions-and-support","title":"Discussions and support","text":"<p>Some of the easiest ways to contribute to RAI involve engaging in community discussions and support. This can be done by creating Issues and RFCs on github.</p>"},{"location":"faq/contributing/CONTRIBUTING/#contributing-code","title":"Contributing code","text":""},{"location":"faq/contributing/CONTRIBUTING/#setting-up-the-development-environment","title":"Setting up the development environment","text":"<p>Set up your development environment following the instructions.</p> <p>Additionally, setup the pre-commit:</p> <pre><code>sudo apt install shellcheck\npre-commit install\npre-commit run -a # Run the checks before committing\n</code></pre> <p>Optionally, install all RAI dependencies and run the tests:</p> <pre><code>poetry install --all-groups\ncolcon build --symlink-install\npytest tests/\n</code></pre>"},{"location":"faq/contributing/CONTRIBUTING/#starting-the-discussion","title":"Starting the discussion","text":"<p>Always try to engage in discussion first. Browse Issues and RFCs or start a discussion on ROS Embodied AI Community Group Discord to see if a feature you want to propose (or a similar one) has already been mentioned. If that is the case feel free to offer that you'll work on it, and propose what changes/additions you will make. One of the project maintainers will assign the issue to you, and you can start working on the code.</p>"},{"location":"faq/contributing/CONTRIBUTING/#submitting-code-changes","title":"Submitting code changes","text":"<p>To submit a change begin by forking this repository and making the changes on the fork. Once the changes are ready to be proposed create a pull request back to the repository. In order to maintain a linear and clear commit history please:</p> <ul> <li>make sure that all commits have meaningful messages</li> <li>if batches of \"cleanup\" or similar commits are present - squash them together</li> <li>rebase onto the main branch of repository before making the PR</li> </ul> <p>We follow the Conventional Commits specification for our commit messages. This means that each commit message should be structured as follows:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Common types include:</p> <ul> <li><code>feat</code>: A new feature</li> <li><code>fix</code>: A bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>style</code>: Changes that do not affect the meaning of the code</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature</li> <li><code>test</code>: Adding missing tests or correcting existing tests</li> <li><code>chore</code>: Changes to the build process or auxiliary tools</li> </ul> <p>Always make sure that both all tests are passing before making the PR. Once this is done open your PR describing what changes have been made and how to test if it's working. Request review from the maintainer who assigned you the issue.</p> <p>If the review requires modifications please make them on your forked repository and go through the above process. Once the PR is accepted it will be merged into the repository. Congratulations, and thank you for your contributions to the development of RAI.</p>"},{"location":"faq/contributing/SECURITY/","title":"Security Policy","text":""},{"location":"faq/contributing/SECURITY/#supported-versions","title":"Supported Versions","text":"<p>Only the newest release for Ubuntu 22.04, Ubuntu 24.04; ROS 2 Humble, and ROS 2 Jazzy are supported. The policy will evolve as the project matures.</p> Version Supported 0.9.x 1.0.x 1.1.x"},{"location":"faq/contributing/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Treat and report vulnerability as any other bug.</p>"},{"location":"intro/what_is_rai/","title":"Welcome to RAI","text":"<p>RAI is an open-source framework that makes it easy to add AI-powered features\u2014like natural language control and smart perception to your robots. It's designed to bridge the gap between cutting-edge AI capabilities and practical robotics applications, enabling developers to create more intelligent and accessible robotic systems.</p> <p>Whether you're building industrial automation solutions, educational robots, or research prototypes, RAI provides the tools and infrastructure needed to bring advanced AI capabilities to your robotics projects. With its modular architecture and flexible integration options, RAI adapts to your needs while maintaining high performance and reliability.</p>"},{"location":"intro/what_is_rai/#what-is-rai","title":"What is RAI?","text":"<ul> <li>It helps you control robots using natural language and advanced AI, making them more intuitive and accessible to use.</li> <li>Works with popular robotics systems (like ROS 2), but can also be used on its own, giving you flexibility in how you integrate it.</li> <li>Combines vision, speech, and sensor data to create comprehensive robotic understanding and control.</li> </ul>"},{"location":"intro/what_is_rai/#how-does-rai-work","title":"How does RAI work?","text":"<p>RAI connects human instructions to robot actions in four simple steps:</p> <ul> <li>Human gives a command: The user interacts with the robot using speech or text.</li> <li>RAI understands and reasons: RAI interprets the request, plans the response, and decides on the best action.</li> <li>RAI controls the robot: It sends commands to the robot through ROS 2 or the robot\u2019s SDK.</li> <li>Robot responds: The robot performs the action and can communicate back to the user, closing the loop.</li> </ul>"},{"location":"intro/what_is_rai/#who-is-rai-for","title":"Who is RAI for?","text":"<ul> <li>Robotics developers and researchers looking to enhance their systems with AI capabilities</li> <li>Anyone who wants to add AI and natural language to robots with existing robotics stack</li> <li>Teams building real-world robotic applications that require intuitive human-robot interaction</li> <li>Educational institutions teaching robotics, agentic systems and AI integration</li> <li>Startups and companies developing next-generation robotic solutions</li> <li>Hobbyists interested in exploring AI-powered robotics</li> </ul>"},{"location":"intro/what_is_rai/#what-can-i-do-with-rai","title":"What can I do with RAI?","text":"<ul> <li>Control robots with voice or text commands, making them more accessible and intuitive to use</li> <li>Integrate vision, speech, and sensor data for comprehensive environmental understanding</li> <li>Build and test complex robot behaviors through natural language instructions</li> <li>Use ready-made demos or create your own solutions tailored to specific use cases</li> <li>Develop multi-agent systems that can collaborate on complex tasks</li> <li>Create custom tools and behaviors that extend RAI's core functionality</li> <li>Implement real-time decision making based on environmental inputs</li> <li>Design interactive scenarios where robots can understand and respond to human intentions</li> </ul>"},{"location":"intro/what_is_rai/#quick-start","title":"Quick Start","text":"<p>Ready to try RAI?</p> <ul> <li>Quick Setup Guide</li> <li>Try a Demo</li> <li>Build Your Own Solution</li> </ul>"},{"location":"intro/what_is_rai/#try-our-demos","title":"Try Our Demos","text":"<p>See RAI in action:</p> <ul> <li>\ud83e\udd16 Manipulation Tasks - Watch RAI control a Franka Panda arm using natural language</li> <li>\ud83d\ude97 Autonomous Navigation - Explore RAI's capabilities with the ROSbot XL platform</li> <li>\ud83d\ude9c Agricultural Robotics - See how RAI handles complex decision-making in orchard environments</li> </ul>"},{"location":"intro/what_is_rai/#build-your-own-solution","title":"Build Your Own Solution","text":"<p>Follow our step-by-step walkthrough to:</p> <ul> <li>Deploy RAI on your robot</li> <li>Add custom tools and features</li> <li>Create advanced, multi-agents systems to tackle complex tasks</li> </ul>"},{"location":"intro/what_is_rai/#how-rai-works","title":"How RAI Works","text":"<ul> <li>Modular: Use only the parts you need</li> <li>Works with ROS 2 (Humble, Jazzy) but is not limited to it</li> <li>Handles communication, perception, and reasoning</li> </ul>"},{"location":"intro/what_is_rai/#community-support","title":"Community &amp; Support","text":"<ul> <li>Contribution Guide</li> <li>FAQ</li> <li>Join our Discord</li> <li>Embodied AI Community Group</li> </ul>"},{"location":"intro/what_is_rai/#learn-more","title":"Learn More","text":"<ul> <li>RAI at ROSCon 2024 (Talk)</li> <li>RAI Demos at ROSCon 2024</li> </ul>"},{"location":"setup/install/","title":"Quick setup guide","text":"<p>Before going further, make sure you have ROS 2 (jazzy or humble) installed and sourced on your system.</p> <p>Docker images</p> <p>RAI has experimental docker images. See the docker for instructions.</p> <p>There are two ways to start using RAI:</p> <ol> <li> <p>Installing RAI using pip (recommended for end users)</p> </li> <li> <p>Setting up a developer environment using poetry (recommended for developers)</p> </li> </ol>"},{"location":"setup/install/#installing-rai","title":"Installing RAI","text":"Virtual environment <p>We recommend installing RAI in a virtual environment (e.g., virtualenv, uv, or poetry) to keep your dependencies organized. Make sure to use the same version of python as the one used for ROS 2 (typically <code>python3.10</code> for Humble and <code>python3.12</code> for Jazzy).</p> <p>If you plan to use ROS 2 commands (<code>ros2 run</code> or <code>ros2 launch</code>), you'll need to add your virtual environment's Python packages to your <code>$PYTHONPATH</code>. This step is only necessary for ROS 2 integration - if you're just running RAI directly with Python, you can skip this step.</p> <p>For reference, here's how to set this up when installing RAI from source: setup_shell.sh</p> <ol> <li> <p>Install core functionality:</p> <pre><code>pip install rai-core\n</code></pre> </li> <li> <p>Initialize the global configuration file:</p> <pre><code>rai-config-init\n</code></pre> </li> <li> <p>Optionally install ROS 2 dependencies:</p> <pre><code>sudo apt install ros-${ROS_DISTRO}-rai-interfaces\n</code></pre> </li> </ol> <p>Package availability</p> <p><code>rai_openset</code> and <code>rai_nomad</code> are not yet available through pip. If your workflow relies on openset detection or NoMaD integration, please refer to the developer environment instructions setup.</p> <p><code>rai_interfaces</code> is available as <code>apt</code> package. However, due to package distribution delays, the latest version may not be immediately available. If you encounter missing imports, please build <code>rai_interfaces</code> from source.</p> RAI modules <p>RAI is a modular framework. You can install only the modules you need.</p> Module Description Documentation rai-core Core functionality link rai-whoami Embodiment module link rai-s2s Speech-to-Speech module link rai-sim Simulation module link rai-bench Benchmarking module link RAI outside of ROS 2 <p>RAI can be used outside of ROS 2. This means that no ROS 2 related features will be available.</p> <p>You can still use RAI's core agent framework, tool system, message passing, and integrations such as LangChain, even if ROS 2 is not installed or sourced on your machine. This is useful for:</p> <ul> <li>Developing and testing AI logic, tools, and workflows independently of any robotics middleware</li> <li>Running RAI agents in simulation or cloud environments where ROS 2 is not present</li> <li>Using RAI as a generic multimodal agent framework for non-robotic applications</li> </ul> <p>If you later decide to integrate with ROS 2, you can simply install and source ROS 2, and all ROS 2-specific RAI features (such as connectors, aggregators, and tools) will become available automatically.</p>"},{"location":"setup/install/#setting-up-developer-environment","title":"Setting up developer environment","text":""},{"location":"setup/install/#11-install-poetry","title":"1.1 Install poetry","text":"<p>RAI uses Poetry(2.1+) for python packaging and dependency management. Install poetry with the following line:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Alternatively, you can opt to do so by following the official docs.</p>"},{"location":"setup/install/#12-clone-the-repository","title":"1.2 Clone the repository:","text":"<pre><code>git clone https://github.com/RobotecAI/rai.git\ncd rai\n</code></pre>"},{"location":"setup/install/#13-download-rai_interfaces","title":"1.3 Download rai_interfaces","text":"<pre><code>vcs import &lt; ros_deps.repos\n</code></pre>"},{"location":"setup/install/#14-create-poetry-virtual-environment-and-install-dependencies","title":"1.4 Create poetry virtual environment and install dependencies:","text":"<pre><code>poetry install\nrosdep install --from-paths src --ignore-src -r -y\n</code></pre> <p>Additional dependencies</p> <p>RAI is modular. If you want to use features such as speech-to-speech, simulation and benchmarking suite, openset detection, or NoMaD integration, install additional dependencies:</p> <pre><code>poetry install --with openset,nomad,s2s,simbench # or `--all-groups` for full setup\n</code></pre> Group Name Description Dependencies s2s Speech-to-Speech functionality rai_asr, rai_tts simbench Simulation and benchmarking tools rai_sim, rai_bench openset Open-set detection capabilities groundingdino, groundedsam nomad Visual Navigation - NoMaD integration visualnav_transformer docs Documentation-related dependencies mkdocs, mkdocs-material, pymdown-extensions"},{"location":"setup/install/#15-configure-rai","title":"1.5 Configure RAI","text":"<p>Run the configuration tool to set up your LLM vendor and other settings:</p> <pre><code>poetry run streamlit run src/rai_core/rai/frontend/configurator.py\n</code></pre> <p>Web browser</p> <p>If the web browser does not open automatically, open the URL displayed in the terminal manually.</p>"},{"location":"setup/install/#2-build-the-project","title":"2. Build the project:","text":""},{"location":"setup/install/#21-build-rai-workspace","title":"2.1 Build RAI workspace","text":"<pre><code>colcon build --symlink-install\n</code></pre>"},{"location":"setup/install/#22-activate-the-virtual-environment","title":"2.2 Activate the virtual environment:","text":"<pre><code>source ./setup_shell.sh\n</code></pre>"},{"location":"setup/install/#3-setting-up-vendors","title":"3. Setting up vendors","text":"<p>RAI is vendor-agnostic. Use the configuration in <code>config.toml</code> to set up your vendor of choice for RAI modules. Vendor choices for RAI and our recommendations are summarized in Vendors Overview.</p> <p>Best-performing AI models</p> <p>We strongly recommend you to use of best-performing AI models to get the most out of RAI!</p> <p>Pick your local solution or service provider and follow one of these guides:</p> <ul> <li>Ollama</li> <li>OpenAI</li> <li>AWS Bedrock</li> </ul>"},{"location":"setup/setup_docker/","title":"Setup RAI with docker","text":"<p>Docker images are experimental</p> <p>Docker images are experimental. For tested setup, see the local setup.</p>"},{"location":"setup/setup_docker/#1-build-the-docker-image","title":"1. Build the docker image","text":"<p>Choose the docker image based on your preferred ROS 2 version.</p>"},{"location":"setup/setup_docker/#11-humble","title":"1.1. Humble","text":"<pre><code>docker build -t rai:humble --build-arg ROS_DISTRO=humble -f docker/Dockerfile .\n</code></pre>"},{"location":"setup/setup_docker/#12-jazzy","title":"1.2. Jazzy","text":"<pre><code>docker build -t rai:jazzy --build-arg ROS_DISTRO=jazzy -f docker/Dockerfile .\n</code></pre>"},{"location":"setup/setup_docker/#2-run-the-docker-container","title":"2. Run the docker container","text":"<p>ROS 2 communication</p> <p>If you intend to run demos on the host machine, ensure the docker container can communicate with it. Test this by running the standard ROS 2 example with one node in docker and one on the host: link. If topics are not visible or cannot be subscribed to, try using rmw_cyclone_dds instead of the default rmw_fastrtps_cpp.</p>"},{"location":"setup/setup_docker/#21-humble","title":"2.1. Humble","text":"<pre><code>docker run --net=host --ipc=host --pid=host -it rai:humble\n</code></pre>"},{"location":"setup/setup_docker/#22-jazzy","title":"2.2. Jazzy","text":"<pre><code>docker run --net=host --ipc=host --pid=host -it rai:jazzy\n</code></pre>"},{"location":"setup/setup_docker/#3-run-the-tests-to-confirm-the-setup","title":"3. Run the tests to confirm the setup","text":"<pre><code>cd /rai\nsource setup_shell.sh\npoetry run pytest\n</code></pre>"},{"location":"setup/tracing/","title":"Tracing Configuration","text":"<p>RAI supports tracing capabilities to help monitor and analyze the performance of your LLM applications, at a minor performance cost. By default, tracing is off. This document outlines how to configure tracing for your RAI project.</p>"},{"location":"setup/tracing/#configuration","title":"Configuration","text":"<p>Tracing configuration is managed through the <code>config.toml</code> file. The relevant parameters for tracing are:</p>"},{"location":"setup/tracing/#project-name","title":"Project Name","text":"<p>The <code>project</code> field under the <code>[tracing]</code> section sets the name for your tracing project. This name will be used to identify your project in the tracing tools.</p> <p>Project name</p> <p>Project name is currently only used by LangSmith. Langfuse will upload traces to the default project.</p>"},{"location":"setup/tracing/#langfuse-open-source","title":"Langfuse (open-source)","text":"<p>Langfuse is an open-source observability &amp; analytics platform for LLM applications.</p> <p>To enable Langfuse tracing:</p> <ol> <li>Set <code>use_langfuse = true</code> in the <code>config.toml</code> file.</li> <li>Set the <code>LANGFUSE_PUBLIC_KEY</code> and <code>LANGFUSE_SECRET_KEY</code> environment variables with your Langfuse    credentials.</li> <li>Optionally, you can specify a custom Langfuse host by modifying the <code>host</code> field under    <code>[tracing.langfuse]</code>.</li> </ol>"},{"location":"setup/tracing/#langsmith-closed-source-paid-limited-free-tier","title":"LangSmith (closed-source, paid, limited free tier)","text":"<p>LangSmith is a platform for debugging, testing, and monitoring LangChain applications.</p> <p>To enable LangSmith tracing:</p> <ol> <li>Set <code>use_langsmith = true</code> in the <code>config.toml</code> file.</li> <li>Set the <code>LANGCHAIN_API_KEY</code> environment variable with your LangSmith API key.</li> <li> <p>Optionally, you can specify a custom LangSmith host by modifying the <code>host</code> field under    <code>[tracing.langsmith]</code>.</p> <p>For deployment details please refer to LangFuse documentation</p> </li> </ol>"},{"location":"setup/tracing/#usage","title":"Usage","text":"<p>To enable tracing in your RAI application, you need to import the <code>rai.get_tracing_callbacks</code> function and add it to the configuration when invoking your agent or model. Here's how to do it:</p> <ol> <li> <p>First, import the <code>get_tracing_callbacks()</code> function:</p> <pre><code>from rai import get_tracing_callbacks\n</code></pre> </li> <li> <p>Then, add it to the configuration when invoking your agent or model:</p> <pre><code>response = agent.invoke(\n    input_dict,\n    config={\"callbacks\": get_tracing_callbacks()}\n)\n</code></pre> </li> </ol> <p>By adding the get_tracing_callbacks() to the config parameter, you enable tracing for that specific invocation. The get_tracing_callbacks() function returns a list of callback handlers based on your configuration in config.toml.</p>"},{"location":"setup/tracing/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with tracing:</p> <ol> <li>Ensure all required environment variables are set correctly.</li> <li>Check whether tracing is on by checking whether <code>use_langsmith</code> or <code>use_langfuse</code> flag is set to    <code>true</code> in <code>config.toml</code>.</li> <li>Verify that you have the necessary permissions and valid API keys for the tracing services you're    using.</li> <li>Look for any error messages in your application logs related to tracing initialization.</li> </ol> <p>For more detailed information on using these tracing tools, refer to their respective documentation:</p> <ul> <li>LangSmith Documentation</li> <li>Langfuse Documentation</li> </ul>"},{"location":"setup/vendors/","title":"Vendor setup","text":"<p>RAI supports multiple vendors for AI models and tracing tools, both open-source and commercial APIs. To setu[ it is recommended to use the RAI Configurator.</p> <p>Alternatively vendors can be configured manually in <code>config.toml</code> file.</p>"},{"location":"setup/vendors/#vendors-overview","title":"Vendors Overview","text":"<p>The table summarizes vendor alternative for core AI service and optional RAI modules:</p> Module Open source Alternative Why to consider alternative? More information LLM service Ollama OpenAI, Bedrock Overall performance of the LLM models, supported modalities and features LangChain models Optional: Tracing tool Langfuse LangSmith Better integration with LangChain Comparison Optional: Text to speech KokoroTTS, OpenTTS ElevenLabs Arguably, significantly better voice synthesis <li> KokoroTTS </li><li> OpenTTS GitHub </li><li> RAI voice interface </li> Optional: Speech to text Whisper OpenAI Whisper (hosted) When suitable local GPU is not an option <li> Whisper GitHub </li><li> RAI voice interface </li> <p>Best-performing AI models</p> <p>Our recommendation, if your environment allows it, is to go with OpenAI GPT4o model, ElevenLabs for TTS, locally-hosted Whisper, and Langsmith.</p>"},{"location":"setup/vendors/#llm-model-configuration-in-rai","title":"LLM Model Configuration in RAI","text":"<p>In RAI you can configure 2 models: <code>simple model</code> and <code>complex model</code>:</p> <ul> <li><code>complex model</code> should be used for sophisticated tasks like multi-step reasoning.</li> <li><code>simple model</code> is more suitable for simpler tasks for example image description.</li> </ul> <pre><code>from rai import get_llm_model\n\ncomplex_llm = get_llm_model(model_type=\"complex\")\nsimple_llm = get_llm_model(model_type=\"simple\")\n</code></pre>"},{"location":"setup/vendors/#vendors-installation","title":"Vendors Installation","text":""},{"location":"setup/vendors/#ollama","title":"Ollama","text":"<p>Ollama can be used to host models locally.</p> <ol> <li>Install <code>Ollama</code> see: https://ollama.com/download</li> <li>Start Ollama server: <code>ollama serve</code></li> <li>Choose LLM model and endpoint type. Ollama server deliveres 2 endpoints:<ul> <li>Ollama endpoint: RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>ollama</code> vendor</li> <li>OpenAI endpoint: RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>openai</code> vendor -&gt; <code>Use OpenAI compatible API</code>   Both endpoints should work interchangeably and decision is only dedicated by user's convenience.</li> </ul> </li> </ol>"},{"location":"setup/vendors/#openai","title":"OpenAI","text":"<ol> <li>Setup your OpenAI account, generate    and set the API key:    <code>bash export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Use RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>ollama</code> vendor</li> </ol>"},{"location":"setup/vendors/#aws-bedrock","title":"AWS Bedrock","text":"<ol> <li> <p>Set AWS Access Keys keys to your AWS account.</p> <pre><code>export AWS_ACCESS_KEY_ID=\"...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport AWS_SESSION_TOKEN=\"...\"\n</code></pre> </li> <li> <p>Use RAI Configurator -&gt; <code>Model Selection</code> -&gt; <code>bedrock</code> vendor</p> </li> </ol>"},{"location":"setup/vendors/#complex-llm-model-configuration","title":"Complex LLM Model Configuration","text":"<p>For custom setups please use LangChain API.</p> <pre><code>from langchain_openai.chat_models import ChatOpenAI\nfrom langchain_aws.chat_models import ChatBedrock\nfrom langchain_community.chat_models import ChatOllama\n\nllm1 = ChatOpenAI(model=\"gpt-4o\")\nllm2 = ChatOllama(model='llava')\nllm = ChatBedrock(model=\"anthropic.claude-3-opus-20240229-v1:0\")\n</code></pre>"},{"location":"setup/vendors/#text-to-speech","title":"Text To Speech","text":"<p>For configuration use <code>Text To Speech</code> tab in RAI Configurator.</p> <p>Usage examples can be found in Voice Interface Tutorial</p>"},{"location":"setup/vendors/#speech-to-text","title":"Speech To Text","text":"<p>For configuration use <code>Speech Recognition</code> tab in RAI Configurator.</p> <p>Usage examples can be found in Voice Interface Tutorial</p>"},{"location":"simulation_and_benchmarking/overview/","title":"Simulation and Benchmarking Overview","text":"<p>RAI provides a comprehensive framework for simulation and benchmarking that consists of two main components:</p>"},{"location":"simulation_and_benchmarking/overview/#rai-sim","title":"RAI Sim","text":"<p>RAI Sim provides a simulator-agnostic interface that allows RAI to work with any simulation environment. It defines a standard interface (<code>SimulationBridge</code>) that abstracts the details of different simulators, enabling:</p> <ul> <li>Consistent behavior across different simulation environments</li> <li>Easy integration with new simulators</li> <li>Seamless switching between simulation backends</li> </ul> <p>The package also provides simulator bridges for concrete simulators, currently supporting only O3DE. For detailed information about the simulation interface, see RAI Sim Documentation.</p>"},{"location":"simulation_and_benchmarking/overview/#rai-bench","title":"RAI Bench","text":"<p>RAI Bench provides benchmarks with ready-to-use tasks and a framework to create your own tasks. It enables:</p> <ul> <li>Define and execute tasks</li> <li>Measure and evaluate performance</li> <li>Collect and analyze results</li> </ul> <p>For detailed information about the benchmarking framework, see RAI Bench Documentation.</p>"},{"location":"simulation_and_benchmarking/overview/#integration","title":"Integration","text":"<p>RAI Sim and RAI Bench work together to provide benchmarks which utilize simulations for evaluation:</p> <ol> <li>Simulation Interface: RAI Sim provides the foundation with its simulator-agnostic interface</li> <li>Task Definition: RAI Bench defines tasks that can be executed in any supported simulator</li> <li>Execution: Tasks are executed through the simulation interface</li> <li>Evaluation: Results are collected and analyzed using the benchmarking framework</li> </ol> <p>This architecture allows for:</p> <ul> <li>Flexible task definition independent of the simulator</li> <li>Consistent evaluation across different simulation environments</li> <li>Easy addition of new simulators and tasks</li> <li>Comprehensive performance analysis</li> </ul>"},{"location":"simulation_and_benchmarking/overview/#use-cases","title":"Use Cases","text":"<p>The combined framework supports various use cases:</p> <ol> <li>Task Evaluation: Testing and comparing different approaches to the same task</li> <li>Performance Analysis: Measuring and analyzing system performance</li> <li>Development Testing: Validating new features in simulation</li> <li>Research: Conducting experiments in controlled environments</li> </ol> <p>For specific implementation details and examples, refer to the respective documentation files.</p>"},{"location":"simulation_and_benchmarking/rai_bench/","title":"RAI Bench","text":"<p>RAI Bench is a comprehensive package that both provides benchmarks with ready-to-use tasks and offers a framework for creating new tasks. It's designed to evaluate the performance of AI agents in various environments.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#available-benchmarks","title":"Available Benchmarks","text":"<ul> <li>Manipulation O3DE Benchmark</li> <li>Tool Calling Agent Benchmark</li> <li>VLM Benchmark</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#manipulation-o3de-benchmark","title":"Manipulation O3DE Benchmark","text":"<p>Evaluates agent performance in robotic arm manipulation tasks within the O3DE simulation environment. The benchmark evaluates how well agents can process sensor data and use tools to manipulate objects in the environment.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#framework-components","title":"Framework Components","text":"<p>Manipulation O3DE Benchmark provides a framework for creating custom tasks and scenarios with these core components:</p> <p></p>"},{"location":"simulation_and_benchmarking/rai_bench/#task","title":"Task","text":"<p>The <code>Task</code> class is an abstract base class that defines the interface for tasks used in this benchmark. Each concrete Task must implement:</p> <ul> <li>prompts that will be passed to the agent</li> <li>validation of simulation configurations</li> <li>calculating results based on scene state</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#scenario","title":"Scenario","text":"<p>A <code>Scenario</code> represents a specific test case combining:</p> <ul> <li>A task to be executed</li> <li>A simulation configuration</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#manipulationo3debenchmark","title":"ManipulationO3DEBenchmark","text":"<p>The <code>ManipulationO3DEBenchmark</code> class manages the execution of scenarios and collects results. It provides:</p> <ul> <li>Scenario execution management</li> <li>Performance metrics tracking</li> <li>Logs and results</li> <li>Robotic stack needed, provided as <code>LaunchDescription</code></li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#available-tasks","title":"Available Tasks","text":"<p>The benchmark includes several predefined manipulation tasks:</p> <ol> <li> <p>MoveObjectsToLeftTask - Move specified objects to the left side of the table</p> </li> <li> <p>PlaceObjectAtCoordTask - Place specified objects at specific coordinates</p> </li> <li> <p>PlaceCubesTask - Place specified cubes adjacent to each other</p> </li> <li> <p>BuildCubeTowerTask - Stack specified cubes to form a tower</p> </li> <li> <p>GroupObjectsTask - Group specified objects of specified types together</p> </li> </ol> <p>Tasks are parametrizable so you can configure which objects should be manipulated and how much precision is needed to complete a task.</p> <p>Tasks are scored on a scale from 0.0 to 1.0, where:</p> <ul> <li>0.0 indicates no improvement or worse placement than the starting one</li> <li>1.0 indicates perfect completion</li> </ul> <p>The score is typically calculated as:</p> <pre><code>score = (correctly_placed_now - correctly_placed_initially) / initially_incorrect\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#available-scene-configs-and-scenarios","title":"Available Scene Configs and Scenarios","text":"<p>You can find predefined scene configs in <code>rai_bench/manipulation_o3de/predefined/configs/</code>.</p> <p>Predefined scenarios can be imported, for example, choosing tasks by difficulty:</p> <pre><code>from rai_bench.manipulation_o3de import get_scenarios\n\nget_scenarios(levels=[\"easy\", \"medium\"])\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#tool-calling-agent-benchmark","title":"Tool Calling Agent Benchmark","text":"<p>Evaluates agent performance independently from any simulation, based only on tool calls that the agent makes. To make it independent from simulations, this benchmark introduces tool mocks which can be adjusted for different tasks. This makes the benchmark more universal and a lot faster.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#framework-components_1","title":"Framework Components","text":""},{"location":"simulation_and_benchmarking/rai_bench/#subtask","title":"SubTask","text":"<p>The <code>SubTask</code> class is used to validate just one tool call. Following classes are available:</p> <ul> <li><code>CheckArgsToolCallSubTask</code> - verify if a certain tool was called with expected arguments</li> <li><code>CheckTopicFieldsToolCallSubTask</code> - verify if a message published to ROS2 topic was of proper type and included expected fields</li> <li><code>CheckServiceFieldsToolCallSubTask</code> - verify if a message published to ROS2 service was of proper type and included expected fields</li> <li><code>CheckActionFieldsToolCallSubTask</code> - verify if a message published to ROS2 action was of proper type and included expected fields</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#validator","title":"Validator","text":"<p>The <code>Validator</code> class can combine single or multiple subtasks to create a single validation step. Following validators are available:</p> <ul> <li>OrderedCallsValidator - requires a strict order of subtasks. The next subtask will be validated only when the previous one was completed. Validator passes when all subtasks pass.</li> <li>NotOrderedCallsValidator - doesn't enforce order of subtasks. Every subtask will be validated against every tool call. Validator passes when all subtasks pass.</li> <li>OneFromManyValidator - passes when any one of the given subtasks passes.</li> </ul>"},{"location":"simulation_and_benchmarking/rai_bench/#task_1","title":"Task","text":"<p>A Task represents a specific prompts and set of tools available. A list of validators is assigned to validate the performance.</p> Task class definition <p>As you can see, the framework is very flexible. Any SubTask can be combined into any Validator that can be later assigned to any Task.</p> <p>Every Task needs to define it's prompt and system prompt, what tools agent will have available, how many tool calls are required to complete it and how many optional tool calls are possible.</p> <p>Optional tool calls mean that a certain tool calls is not obligatory to pass the Task, but shoudn't be considered an error, example: <code>GetROS2RGBCameraTask</code> which has prompt: <code>Get RGB camera image.</code> requires making one tool call with <code>get_ros2_image</code> tool. But listing topics before doing it is a valid approach, so in this case opitonal tool calls is <code>1</code>.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task","title":"<code>rai_bench.tool_calling_agent.interfaces.Task</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>class Task(ABC):\n    complexity: Literal[\"easy\", \"medium\", \"hard\"]\n    type: str\n    recursion_limit: int = DEFAULT_RECURSION_LIMIT\n\n    def __init__(\n        self,\n        validators: List[Validator],\n        task_args: TaskArgs,\n        logger: loggers_type | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Abstract base class representing a complete task to be validated.\n\n        A Task consists of multiple Validators, where each Validator can be treated as a single\n        step that is scored atomically. Each Task has a consistent prompt and available tools,\n        with validation methods that can be parameterized.\n\n        Attributes\n        ----------\n        complexity : Literal[\"easy\", \"medium\", \"hard\"]\n            difficulty level of the task\n        type : str\n            type identifier for the task\n        recursion_limit : int, optional\n            maximum recursion depth allowed, by default DEFAULT_RECURSION_LIMIT\n\n        Parameters\n        ----------\n        validators : List[Validator]\n            List of validators that will be applied in sequence.\n        task_args : TaskArgs\n            Configuration parameters for the task specified by user\n        logger : logging.Logger\n            Logger for recording task validation results and errors.\n        \"\"\"\n        if logger:\n            self.logger = logger\n        else:\n            self.logger = logging.getLogger(__name__)\n        self.validators = validators\n        self.extra_tool_calls = task_args.extra_tool_calls\n        self.prompt_detail = task_args.prompt_detail\n        self.n_shots = task_args.examples_in_system_prompt\n\n    def set_logger(self, logger: loggers_type):\n        self.logger = logger\n        for validator in self.validators:\n            validator.logger = logger\n\n    def get_tool_calls_from_invoke(self, response: dict[str, Any]) -&gt; list[ToolCall]:\n        \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n        tool_calls: List[ToolCall] = []\n        for msg in response[\"messages\"]:\n            if isinstance(msg, AIMessage):\n                tool_calls.extend(msg.tool_calls)\n        return tool_calls\n\n    def get_tool_calls_from_messages(\n        self, messages: List[BaseMessage]\n    ) -&gt; list[ToolCall]:\n        \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n        tool_calls: List[ToolCall] = []\n        for msg in messages:\n            if isinstance(msg, AIMessage):\n                tool_calls.extend(msg.tool_calls)\n        return tool_calls\n\n    def dump_validators(self) -&gt; List[ValidatorResult]:\n        return [val.dump_results() for val in self.validators]\n\n    @property\n    @abstractmethod\n    def available_tools(self) -&gt; List[BaseTool]:\n        \"\"\"List of tool available for the agent\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def optional_tool_calls_number(self) -&gt; int:\n        \"\"\"Optional tool calls means calls that are not considered error.\n        For example listing topics at the beginning.\"\"\"\n        pass\n\n    @property\n    def max_tool_calls_number(self) -&gt; int:\n        \"\"\"maxiumum number of call to still pass task.\n        Includes extra tool calls params.\n        and optional tool calls number which depends on task.\n        \"\"\"\n        return (\n            self.required_calls\n            + self.optional_tool_calls_number\n            + self.extra_tool_calls\n        )\n\n    @property\n    def additional_calls(self) -&gt; int:\n        \"\"\"number of additional calls that can be done to still pass task.\n        Includes extra tool calls params.\n        and optional tool calls number which depends on task.\n        \"\"\"\n        return self.optional_tool_calls_number + self.extra_tool_calls\n\n    @property\n    def required_calls(self) -&gt; int:\n        \"\"\"Minimal number of calls required to complete task\"\"\"\n        total = 0\n        for val in self.validators:\n            total += len(val.subtasks)\n        return total\n\n    @abstractmethod\n    def get_system_prompt(self) -&gt; str:\n        \"\"\"Get the system prompt that will be passed to agent\n\n        Returns\n        -------\n        str\n            System prompt\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_base_prompt(self) -&gt; str:\n        \"\"\"\n        Get the base task instruciton,\n        it will be used to identify task in results processing\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_prompt(self) -&gt; str:\n        \"\"\"Get the task instruction - the prompt that will be passed to agent.\n\n        Returns\n        -------\n        str\n            Prompt\n        \"\"\"\n        pass\n\n    def validate(self, tool_calls: List[ToolCall]):\n        \"\"\"Validate a list of tool calls against all validators in sequence\"\"\"\n        self.logger.debug(\n            f\"required_calls: {self.required_calls}, extra_calls {self.extra_tool_calls}\"\n        )\n        remaining_tool_calls = tool_calls[: self.max_tool_calls_number].copy()\n        self.logger.debug(f\"Tool calls to validate: {remaining_tool_calls}\")\n\n        done_properly = 0\n        for validator in self.validators:\n            if_success, remaining_tool_calls = validator.validate(\n                tool_calls=remaining_tool_calls\n            )\n\n            if if_success:\n                done_properly += 1\n\n        return done_properly / len(self.validators)\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.additional_calls","title":"<code>additional_calls</code>  <code>property</code>","text":"<p>number of additional calls that can be done to still pass task. Includes extra tool calls params. and optional tool calls number which depends on task.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.available_tools","title":"<code>available_tools</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>List of tool available for the agent</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.max_tool_calls_number","title":"<code>max_tool_calls_number</code>  <code>property</code>","text":"<p>maxiumum number of call to still pass task. Includes extra tool calls params. and optional tool calls number which depends on task.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.optional_tool_calls_number","title":"<code>optional_tool_calls_number</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Optional tool calls means calls that are not considered error. For example listing topics at the beginning.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.required_calls","title":"<code>required_calls</code>  <code>property</code>","text":"<p>Minimal number of calls required to complete task</p>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.__init__","title":"<code>__init__(validators, task_args, logger=None)</code>","text":"<p>Abstract base class representing a complete task to be validated.</p> <p>A Task consists of multiple Validators, where each Validator can be treated as a single step that is scored atomically. Each Task has a consistent prompt and available tools, with validation methods that can be parameterized.</p> <p>Attributes:</p> Name Type Description <code>complexity</code> <code>Literal['easy', 'medium', 'hard']</code> <p>difficulty level of the task</p> <code>type</code> <code>str</code> <p>type identifier for the task</p> <code>recursion_limit</code> <code>(int, optional)</code> <p>maximum recursion depth allowed, by default DEFAULT_RECURSION_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>validators</code> <code>List[Validator]</code> <p>List of validators that will be applied in sequence.</p> required <code>task_args</code> <code>TaskArgs</code> <p>Configuration parameters for the task specified by user</p> required <code>logger</code> <code>Logger</code> <p>Logger for recording task validation results and errors.</p> <code>None</code> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def __init__(\n    self,\n    validators: List[Validator],\n    task_args: TaskArgs,\n    logger: loggers_type | None = None,\n) -&gt; None:\n    \"\"\"\n    Abstract base class representing a complete task to be validated.\n\n    A Task consists of multiple Validators, where each Validator can be treated as a single\n    step that is scored atomically. Each Task has a consistent prompt and available tools,\n    with validation methods that can be parameterized.\n\n    Attributes\n    ----------\n    complexity : Literal[\"easy\", \"medium\", \"hard\"]\n        difficulty level of the task\n    type : str\n        type identifier for the task\n    recursion_limit : int, optional\n        maximum recursion depth allowed, by default DEFAULT_RECURSION_LIMIT\n\n    Parameters\n    ----------\n    validators : List[Validator]\n        List of validators that will be applied in sequence.\n    task_args : TaskArgs\n        Configuration parameters for the task specified by user\n    logger : logging.Logger\n        Logger for recording task validation results and errors.\n    \"\"\"\n    if logger:\n        self.logger = logger\n    else:\n        self.logger = logging.getLogger(__name__)\n    self.validators = validators\n    self.extra_tool_calls = task_args.extra_tool_calls\n    self.prompt_detail = task_args.prompt_detail\n    self.n_shots = task_args.examples_in_system_prompt\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_base_prompt","title":"<code>get_base_prompt()</code>  <code>abstractmethod</code>","text":"<p>Get the base task instruciton, it will be used to identify task in results processing</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>@abstractmethod\ndef get_base_prompt(self) -&gt; str:\n    \"\"\"\n    Get the base task instruciton,\n    it will be used to identify task in results processing\n    \"\"\"\n    pass\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_prompt","title":"<code>get_prompt()</code>  <code>abstractmethod</code>","text":"<p>Get the task instruction - the prompt that will be passed to agent.</p> <p>Returns:</p> Type Description <code>str</code> <p>Prompt</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>@abstractmethod\ndef get_prompt(self) -&gt; str:\n    \"\"\"Get the task instruction - the prompt that will be passed to agent.\n\n    Returns\n    -------\n    str\n        Prompt\n    \"\"\"\n    pass\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_system_prompt","title":"<code>get_system_prompt()</code>  <code>abstractmethod</code>","text":"<p>Get the system prompt that will be passed to agent</p> <p>Returns:</p> Type Description <code>str</code> <p>System prompt</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>@abstractmethod\ndef get_system_prompt(self) -&gt; str:\n    \"\"\"Get the system prompt that will be passed to agent\n\n    Returns\n    -------\n    str\n        System prompt\n    \"\"\"\n    pass\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_tool_calls_from_invoke","title":"<code>get_tool_calls_from_invoke(response)</code>","text":"<p>Extracts all tool calls from the response, flattened across all AI messages.</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def get_tool_calls_from_invoke(self, response: dict[str, Any]) -&gt; list[ToolCall]:\n    \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n    tool_calls: List[ToolCall] = []\n    for msg in response[\"messages\"]:\n        if isinstance(msg, AIMessage):\n            tool_calls.extend(msg.tool_calls)\n    return tool_calls\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.get_tool_calls_from_messages","title":"<code>get_tool_calls_from_messages(messages)</code>","text":"<p>Extracts all tool calls from the response, flattened across all AI messages.</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def get_tool_calls_from_messages(\n    self, messages: List[BaseMessage]\n) -&gt; list[ToolCall]:\n    \"\"\"Extracts all tool calls from the response, flattened across all AI messages.\"\"\"\n    tool_calls: List[ToolCall] = []\n    for msg in messages:\n        if isinstance(msg, AIMessage):\n            tool_calls.extend(msg.tool_calls)\n    return tool_calls\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#rai_bench.tool_calling_agent.interfaces.Task.validate","title":"<code>validate(tool_calls)</code>","text":"<p>Validate a list of tool calls against all validators in sequence</p> Source code in <code>rai_bench/tool_calling_agent/interfaces.py</code> <pre><code>def validate(self, tool_calls: List[ToolCall]):\n    \"\"\"Validate a list of tool calls against all validators in sequence\"\"\"\n    self.logger.debug(\n        f\"required_calls: {self.required_calls}, extra_calls {self.extra_tool_calls}\"\n    )\n    remaining_tool_calls = tool_calls[: self.max_tool_calls_number].copy()\n    self.logger.debug(f\"Tool calls to validate: {remaining_tool_calls}\")\n\n    done_properly = 0\n    for validator in self.validators:\n        if_success, remaining_tool_calls = validator.validate(\n            tool_calls=remaining_tool_calls\n        )\n\n        if if_success:\n            done_properly += 1\n\n    return done_properly / len(self.validators)\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_bench/#toolcallingagentbenchmark","title":"ToolCallingAgentBenchmark","text":"<p>The ToolCallingAgentBenchmark class manages the execution of tasks and collects results.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#available-tasks_1","title":"Available Tasks","text":"<p>There are predefined Tasks available which are grouped by categories:</p> <ul> <li>Basic - require retrieving info from certain topics</li> <li>Manipulation</li> <li>Custom Interfaces - requires using messages with custom interfaces</li> </ul> <p>Every Task has assigned the <code>complexity</code> which reflects the difficulty.</p> <p>When creating a Task, you can define few params:</p> <pre><code>class TaskArgs(BaseModel):\n    \"\"\"Holds the configurations specified by user\"\"\"\n\n    extra_tool_calls: int = 0\n    prompt_detail: Literal[\"brief\", \"descriptive\"] = \"brief\"\n    examples_in_system_prompt: Literal[0, 2, 5] = 0\n</code></pre> <ul> <li> <p>examples_in_system_prompt - How many examples there are in system prompts, example:</p> <ul> <li><code>0</code>: <code>You are a ROS 2 expert that want to solve tasks. You have access to various tools that allow you to query the ROS 2 system. Be proactive and use the tools to answer questions.</code></li> <li><code>2</code>: <code>You are a ROS 2 expert that want to solve tasks. You have access to various tools that allow you to query the ROS 2 system. Be proactive and use the tools to answer questions. Example of tool calls: get_ros2_message_interface, args: {'msg_type': 'geometry_msgs/msg/Twist'} publish_ros2_message, args: {'topic': '/cmd_vel', 'message_type': 'geometry_msgs/msg/Twist', 'message': {linear: {x: 0.5, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 1.0}}}</code></li> </ul> </li> <li> <p>prompt_detail - How descriptive should the Task prompt be, example:</p> <ul> <li><code>brief</code>: \"Get all camera images\"</li> <li> <p><code>descriptive</code>: \"Get all camera images from all available camera sources in the system.     This includes both RGB color images and depth images.     You can discover what camera topics are available and capture images from each.\"</p> <p>Descriptive prompts provides guidance and tips.</p> </li> </ul> </li> <li> <p>extra_tool_calls - How many extra tool calls an agent can make and still pass the Task, example:</p> <ul> <li><code>GetROS2RGBCameraTask</code> has 1 required tool call and 1 optional. When <code>extra_tool_calls</code> set to 5, agent can correct himself couple times and still pass even with 7 tool calls. There can be 2 types of invalid tool calls, first when the tool is used incorrectly and agent receives an error - this allows him to correct himself easier. Second type is when tool is called properly but it is not the tool that should be called or it is called with wrong params. In this case agent won't get any error so it will be harder for him to correct, but BOTH of these cases are counted as <code>extra tool call</code>.</li> </ul> </li> </ul> <p>If you want to know details about every task, visit <code>rai_bench/tool_calling_agent/tasks</code></p>"},{"location":"simulation_and_benchmarking/rai_bench/#vlm-benchmark","title":"VLM Benchmark","text":"<p>The VLM Benchmark is a benchmark for VLM models. It includes a set of tasks containing questions related to images and evaluates the performance of the agent that returns the answer in the structured format.</p>"},{"location":"simulation_and_benchmarking/rai_bench/#running","title":"Running","text":"<p>To run the benchmark:</p> <pre><code>cd rai\nsource setup_shell.sh\npython src/rai_bench/rai_bench/examples/vlm_benchmark.py --model-name gemma3:4b --vendor ollama\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_sim/","title":"RAI Sim","text":"<p>RAI Sim is a package that provides an interface for connecting with various simulation environments. It is designed to be simulator-agnostic, allowing RAI to work with any simulation environment that implements the required interface.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#core-components","title":"Core Components","text":""},{"location":"simulation_and_benchmarking/rai_sim/#simulationbridge","title":"SimulationBridge","text":"<p>The <code>SimulationBridge</code> is an abstract base class that defines the interface for communicating with different simulation environments. It provides the following key functionalities:</p> <ul> <li>Scene setup and management</li> <li>Entity spawning and despawning</li> <li>Object pose retrieval</li> <li>Scene state monitoring</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#sceneconfig","title":"SceneConfig","text":"<p>The <code>SceneConfig</code> is a configuration class that specifies the entities to be spawned in the simulation.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#simulationconfig","title":"SimulationConfig","text":"<p>The <code>SimulationConfig</code> is an abstract configuration class. Each simulation bridge can extend this with additional parameters specific to its implementation.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#scenestate","title":"SceneState","text":"<p>The <code>SceneState</code> class maintains information about the current state of the simulation scene, including:</p> <ul> <li>List of currently spawned entities</li> <li>Current poses of all entities</li> <li>Entity tracking and management</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#implementation-details","title":"Implementation Details","text":""},{"location":"simulation_and_benchmarking/rai_sim/#entity-management","title":"Entity Management","text":"<p>The package provides two main entity classes:</p> <ul> <li><code>Entity</code>: Represents an entity that can be spawned in the simulation</li> <li><code>SpawnedEntity</code>: Represents an entity that has been successfully spawned</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#tools","title":"Tools","text":"<p>RAI Sim includes utility tools for working with simulations:</p> <ul> <li><code>GetObjectPositionsGroundTruthTool</code>: Retrieves accurate positional data for objects in the simulation</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#usage","title":"Usage","text":"<p>To use RAI Sim with a specific simulation environment:</p> <ol> <li>Create a custom <code>SimulationBridge</code> implementation for your simulator</li> <li>Extend <code>SimulationConfig</code> with simulator-specific parameters</li> <li>Implement the required abstract methods:<ul> <li><code>init_simulation</code></li> <li><code>setup_scene</code></li> <li><code>_spawn_entity</code></li> <li><code>_despawn_entity</code></li> <li><code>get_object_pose</code></li> <li><code>get_scene_state</code></li> </ul> </li> </ol>"},{"location":"simulation_and_benchmarking/rai_sim/#configuration","title":"Configuration","text":"<p>Simulation configurations are typically loaded from YAML files with the following structure:</p> <pre><code>frame_id: &lt;reference_frame&gt;\nentities:\n    - name: &lt;unique_entity_name&gt;\n      prefab_name: &lt;resource_name&gt;\n      pose:\n          translation:\n              x: &lt;x_coordinate&gt;\n              y: &lt;y_coordinate&gt;\n              z: &lt;z_coordinate&gt;\n          rotation:\n              x: &lt;x_rotation&gt;\n              y: &lt;y_rotation&gt;\n              z: &lt;z_rotation&gt;\n              w: &lt;w_rotation&gt;\n</code></pre>"},{"location":"simulation_and_benchmarking/rai_sim/#error-handling","title":"Error Handling","text":"<p>The package includes comprehensive error handling for:</p> <ul> <li>Duplicate entity names</li> <li>Failed entity spawning/despawning</li> <li>Invalid configurations</li> <li>Simulation process management</li> </ul>"},{"location":"simulation_and_benchmarking/rai_sim/#integration-with-rai-bench","title":"Integration with RAI Bench","text":"<p>RAI Sim serves as the foundation for RAI Bench by providing:</p> <ul> <li>A consistent interface for all simulation environments</li> <li>Entity management and tracking</li> <li>Scene state monitoring</li> <li>Configuration management</li> </ul> <p>This allows RAI Bench to focus on task definition and evaluation while remaining simulator-agnostic.</p>"},{"location":"simulation_and_benchmarking/rai_sim/#launchmanager","title":"LaunchManager","text":"<p>RAI Sim also provides a ROS2LaunchManager class that manages the start and shutdown of ROS 2<code>LaunchDescription</code></p> ROS2LaunchManager class definition"},{"location":"simulation_and_benchmarking/rai_sim/#rai_sim.launch_manager.ROS2LaunchManager","title":"<code>rai_sim.launch_manager.ROS2LaunchManager</code>","text":"Source code in <code>rai_sim/launch_manager.py</code> <pre><code>class ROS2LaunchManager:\n    def __init__(self) -&gt; None:\n        self._stop_event: Optional[Event] = None\n        self._process: Optional[multiprocessing.Process] = None\n\n    def start(self, launch_description: LaunchDescription) -&gt; None:\n        self._stop_event = multiprocessing.Event()\n        self._process = multiprocessing.Process(\n            target=self._run_process,\n            args=(self._stop_event, launch_description),\n            daemon=True,\n        )\n        self._process.start()\n\n    def shutdown(self) -&gt; None:\n        if self._stop_event:\n            self._stop_event.set()\n        if self._process:\n            self._process.join()\n\n    def _run_process(\n        self, stop_event: Event, launch_description: LaunchDescription\n    ) -&gt; None:\n        loop = asyncio.get_event_loop()\n        asyncio.set_event_loop(loop)\n        launch_service = LaunchService()\n        launch_service.include_launch_description(launch_description)\n        # launch description launched\n        launch_task = loop.create_task(launch_service.run_async())\n        # when stop event set\n        loop.run_until_complete(loop.run_in_executor(None, stop_event.wait))\n        if not launch_task.done():\n            # XXX (jmatejcz) the shutdown function sends shutdown signal to all\n            # nodes launch with launch description which should do the trick\n            # but some nodes are stubborn and there is a possibility\n            # that they don't close. If this will happen sending PKILL for all\n            # ros nodes will be needed\n            shutdown_task = loop.create_task(\n                launch_service.shutdown(),\n            )\n            # shutdown task should complete when all nodes are closed\n            # but wait also for launch task to close just to be sure\n            loop.run_until_complete(asyncio.gather(shutdown_task, launch_task))\n</code></pre>"},{"location":"simulation_and_benchmarking/simulators/","title":"Simulators","text":"<p>TBD</p>"},{"location":"speech_to_speech/overview/","title":"Speech To Speech","text":""},{"location":"speech_to_speech/overview/#introduction","title":"Introduction","text":"<p><code>rai s2s</code> provides tools and components for voice interaction with the system. This package contains plug-and-play Agents which can be easily integrated with Agents provided by <code>rai core</code>, as well as custom ones. It also provides integration with host sound system, which can be used for low level sound manipulation.</p>"},{"location":"speech_to_speech/overview/#core-components","title":"Core Components","text":"Component Description Agents Agents in <code>rai s2s</code> provide functionality for voice interaction with the rest of the system. Models <code>rai s2s</code> provides a models which can be optionally installed and utilized by the Agents. Connector The <code>sounddevice</code> connector allows for interfacing directly with sound devices for asynchronous sound IO."},{"location":"speech_to_speech/overview/#best-practices","title":"Best Practices","text":"<p>When utilizing S2S features:</p> <ol> <li>Deployment of <code>SpeechToSpeechAgent</code> is meant for local setup, while the <code>SpeechRecognition</code> and <code>TextToSpeech</code> Agents are meant to be ran on separate hosts.</li> <li>Note that <code>sounddevice</code> python API has notable issues in multi-threaded environment - this can lead to issues when developing Agents using the <code>SoundDeviceConnector</code></li> </ol>"},{"location":"speech_to_speech/sounddevice/","title":"Sound Device Connector","text":"<p>The <code>SoundDeviceConnector</code> provides a Human-Robot Interface (HRI) for audio streaming, playback, and recording using sound devices. It is designed for seamless integration with RAI agents and tools requiring audio input/output, and conforms to the generic <code>HRIConnector</code> interface.</p> Connector Description Example Usage <code>SoundDeviceConnector</code> Audio streaming, playback, and recording via sounddevice. Implements HRIConnector for audio. <code>connector = SoundDeviceConnector(...)</code>"},{"location":"speech_to_speech/sounddevice/#key-features","title":"Key Features","text":"<ul> <li>Audio playback (write) and recording (read) with flexible device configuration</li> <li>Asynchronous (streaming) and synchronous (service call) audio operations</li> <li>Thread-safe device management and clean shutdown</li> <li>Unified message type (<code>SoundDeviceMessage</code>) for audio and control</li> <li>Full support for the HRIConnector interface</li> </ul>"},{"location":"speech_to_speech/sounddevice/#initialization","title":"Initialization","text":"<p>To use the connector, specify the target (output) and source (input) devices with their configurations:</p> <pre><code>from rai_s2s.sound_device import SoundDeviceConfig, SoundDeviceConnector\n\n# Example device configurations\noutput_config = SoundDeviceConfig(device_name=\"Speaker\", channels=1)\ninput_config = SoundDeviceConfig(device_name=\"Microphone\", channels=1)\n\nconnector = SoundDeviceConnector(\n    targets=[(\"speaker\", output_config)],\n    sources=[(\"mic\", input_config)],\n)\n</code></pre> <p>Tip</p> <p>If you're experiencing audio issues and device_name is set to 'default', try specifying the exact device name instead, as this often resolves the problem.</p>"},{"location":"speech_to_speech/sounddevice/#message-type-sounddevicemessage","title":"Message Type: <code>SoundDeviceMessage</code>","text":"<pre><code>from rai_s2s.sound_device import SoundDeviceMessage\n\nmsg = SoundDeviceMessage(\n    audios=[audio_data],   # List of audio data (bytes or numpy arrays)\n    read=False,            # Set True for recording\n    stop=False,            # Set True to stop playback/recording\n    duration=2.0           # Recording duration (seconds), if applicable\n)\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#example-usage","title":"Example Usage","text":""},{"location":"speech_to_speech/sounddevice/#audio-playback-synchronous","title":"Audio Playback (Synchronous)","text":"<pre><code># Play audio synchronously\nmsg = SoundDeviceMessage(audios=[audio_data])\nconnector.send_message(msg, target=\"speaker\")\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#audio-recording-synchronous","title":"Audio Recording (Synchronous)","text":"<pre><code># Record audio synchronously (blocking)\nmsg = SoundDeviceMessage(read=True)\nrecorded_msg = connector.service_call(msg, target=\"mic\", duration=2.0)\nrecorded_audio = recorded_msg.audios[0]\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#audio-streaming-asynchronous","title":"Audio Streaming (Asynchronous)","text":"<pre><code># Start asynchronous audio recording\nmsg = SoundDeviceMessage(read=True)\ndef on_feedback(audio_chunk):\n    print(\"Received chunk\", audio_chunk)\ndef on_done(final_audio):\n    print(\"Recording finished\")\naction_handle = connector.start_action(\n    action_data=msg,\n    target=\"mic\",\n    on_feedback=on_feedback,\n    on_done=on_done\n)\n\n# Stop the stream when done\nconnector.terminate_action(action_handle)\n</code></pre>"},{"location":"speech_to_speech/sounddevice/#device-management","title":"Device Management","text":"<ul> <li>Configure devices at initialization or using <code>configure_device(target, config)</code>.</li> <li>Retrieve audio parameters using <code>get_audio_params(target)</code>.</li> <li>All devices are managed in a thread-safe way and are properly closed on <code>shutdown()</code>.</li> </ul>"},{"location":"speech_to_speech/sounddevice/#error-handling","title":"Error Handling","text":"<ul> <li>All methods raise <code>SoundDeviceError</code> on invalid operations (e.g., unsupported message types, missing audio data).</li> <li>Use <code>send_message</code> with <code>stop=True</code> to stop playback or recording.</li> <li><code>receive_message</code> is not supported (use actions or service calls for recording).</li> </ul>"},{"location":"speech_to_speech/sounddevice/#see-also","title":"See Also","text":"<ul> <li>Connectors Overview</li> <li>Agents</li> </ul>"},{"location":"speech_to_speech/agents/asr/","title":"SpeechRecognitionAgent","text":""},{"location":"speech_to_speech/agents/asr/#overview","title":"Overview","text":"<p>The <code>SpeechRecognitionAgent</code> in the RAI framework is a specialized agent that performs voice activity detection (VAD), audio recording, and transcription. It integrates tightly with audio input sources and ROS2 messaging, allowing it to serve as a real-time voice interface for robotic systems.</p> <p>This agent manages multiple pipelines for detecting when to start and stop recording, performs transcription using configurable models, and broadcasts messages to relevant ROS2 topics.</p>"},{"location":"speech_to_speech/agents/asr/#class-definition","title":"Class Definition","text":"SpeechRecognitionAgent class definition"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent","title":"<code>rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>Agent responsible for voice recognition, transcription, and processing voice activity.</p> <p>Parameters:</p> Name Type Description Default <code>microphone_config</code> <code>SoundDeviceConfig</code> <p>Configuration for the microphone device used for audio input.</p> required <code>ros2_name</code> <code>str</code> <p>Name of the ROS2 node.</p> required <code>transcription_model</code> <code>BaseTranscriptionModel</code> <p>Model used for transcribing audio input to text.</p> required <code>vad</code> <code>BaseVoiceDetectionModel</code> <p>Voice activity detection model used to determine when speech is present.</p> required <code>grace_period</code> <code>float</code> <p>Time in seconds to wait before stopping recording after speech ends, by default 1.0.</p> <code>1.0</code> <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance for logging messages, by default None.</p> <code>None</code> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>class SpeechRecognitionAgent(BaseAgent):\n    \"\"\"\n    Agent responsible for voice recognition, transcription, and processing voice activity.\n\n    Parameters\n    ----------\n    microphone_config : SoundDeviceConfig\n        Configuration for the microphone device used for audio input.\n    ros2_name : str\n        Name of the ROS2 node.\n    transcription_model : BaseTranscriptionModel\n        Model used for transcribing audio input to text.\n    vad : BaseVoiceDetectionModel\n        Voice activity detection model used to determine when speech is present.\n    grace_period : float, optional\n        Time in seconds to wait before stopping recording after speech ends, by default 1.0.\n    logger : Optional[logging.Logger], optional\n        Logger instance for logging messages, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        microphone_config: SoundDeviceConfig,\n        ros2_name: str,\n        transcription_model: BaseTranscriptionModel,\n        vad: BaseVoiceDetectionModel,\n        grace_period: float = 1.0,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if logger is None:\n            self.logger = logging.getLogger(__name__)\n        else:\n            self.logger = logger\n        self.microphone = SoundDeviceConnector(\n            targets=[], sources=[(\"microphone\", microphone_config)]\n        )\n        self.ros2_hri_connector = ROS2HRIConnector(ros2_name)\n        self.ros2_connector = ROS2Connector(ros2_name + \"ari\")\n        super().__init__()\n        self.should_record_pipeline: List[BaseVoiceDetectionModel] = []\n        self.should_stop_pipeline: List[BaseVoiceDetectionModel] = []\n\n        self.transcription_model = transcription_model\n        self.transcription_lock = Lock()\n\n        self.vad: BaseVoiceDetectionModel = vad\n\n        self.grace_period = grace_period\n        self.grace_period_start = 0\n\n        self.recording_started = False\n        self.ran_setup = False\n\n        self.sample_buffer = []\n        self.sample_buffer_lock = Lock()\n        self.active_thread = \"\"\n        self.transcription_threads: dict[str, ThreadData] = {}\n        self.transcription_buffers: dict[str, list[NDArray]] = {}\n        self.is_playing = True\n\n    @classmethod\n    def from_config(cls, cfg_path: Optional[str] = None) -&gt; Self:\n        cfg = load_config(cfg_path)\n        microphone_configuration = SoundDeviceConfig(\n            stream=True,\n            channels=1,\n            device_name=cfg.microphone.device_name,\n            block_size=1280,\n            consumer_sampling_rate=16000,\n            dtype=\"int16\",\n            device_number=None,\n            is_input=True,\n            is_output=False,\n        )\n        match cfg.transcribe.model_type:\n            case \"LocalWhisper (Free)\":\n                from rai_s2s.asr.models import LocalWhisper\n\n                model = LocalWhisper(\n                    cfg.transcribe.model_name, 16000, language=cfg.transcribe.language\n                )\n            case \"FasterWhisper (Free)\":\n                from rai_s2s.asr.models import FasterWhisper\n\n                model = FasterWhisper(\n                    cfg.transcribe.model_name, 16000, language=cfg.transcribe.language\n                )\n            case \"OpenAI (Cloud)\":\n                from rai_s2s.asr.models import OpenAIWhisper\n\n                model = OpenAIWhisper(\n                    cfg.transcribe.model_name, 16000, language=cfg.transcribe.language\n                )\n            case _:\n                raise ValueError(f\"Unknown model name f{cfg.transcribe.model_name}\")\n\n        match cfg.voice_activity_detection.model_name:\n            case \"SileroVAD\":\n                from rai_s2s.asr.models import SileroVAD\n\n                vad = SileroVAD(16000, cfg.voice_activity_detection.threshold)\n\n        agent = cls(microphone_configuration, \"rai_auto_asr_agent\", model, vad)\n        if cfg.wakeword.is_used:\n            match cfg.wakeword.model_type:\n                case \"OpenWakeWord\":\n                    from rai_s2s.asr.models import OpenWakeWord\n\n                    agent.add_detection_model(\n                        OpenWakeWord(cfg.wakeword.model_name, cfg.wakeword.threshold)\n                    )\n        return agent\n\n    def __call__(self):\n        self.run()\n\n    def add_detection_model(\n        self, model: BaseVoiceDetectionModel, pipeline: str = \"record\"\n    ):\n        \"\"\"\n        Add a voice detection model to the specified processing pipeline.\n\n        Parameters\n        ----------\n        model : BaseVoiceDetectionModel\n            The voice detection model to be added.\n        pipeline : str, optional\n            The pipeline where the model should be added, either 'record' or 'stop'.\n            Default is 'record'.\n\n        Raises\n        ------\n        ValueError\n            If the specified pipeline is not 'record' or 'stop'.\n        \"\"\"\n\n        if pipeline == \"record\":\n            self.should_record_pipeline.append(model)\n        elif pipeline == \"stop\":\n            self.should_stop_pipeline.append(model)\n        else:\n            raise ValueError(\"Pipeline should be either 'record' or 'stop'\")\n\n    def run(self):\n        \"\"\"\n        Start the voice recognition agent, initializing the microphone and handling incoming audio samples.\n        \"\"\"\n        self.running = True\n        msg = SoundDeviceMessage(read=True)\n        self.listener_handle = self.microphone.start_action(\n            action_data=msg,\n            target=\"microphone\",\n            on_feedback=self._on_new_sample,\n            on_done=lambda: None,\n        )\n        self.logger.info(\"Started Voice Agent\")\n\n    def stop(self):\n        \"\"\"\n        Clean exit the voice recognition agent, ensuring all transcription threads finish before termination.\n        \"\"\"\n        self.logger.info(\"Stopping Voice Agent\")\n        self.running = False\n        self.microphone.terminate_action(self.listener_handle)\n        self.ros2_hri_connector.shutdown()\n        self.ros2_connector.shutdown()\n        while not all(\n            [thread[\"joined\"] for thread in self.transcription_threads.values()]\n        ):\n            for thread_id in self.transcription_threads:\n                if self.transcription_threads[thread_id][\"event\"].is_set():\n                    self.transcription_threads[thread_id][\"thread\"].join()\n                    self.transcription_threads[thread_id][\"joined\"] = True\n                else:\n                    self.logger.info(\n                        f\"Waiting for transcription of {thread_id} to finish...\"\n                    )\n        self.logger.info(\"Voice agent stopped\")\n\n    def _on_new_sample(self, indata: np.ndarray, status_flags: dict[str, Any]):\n        sample_time = time.time()\n        with self.sample_buffer_lock:\n            self.sample_buffer.append(indata)\n            if not self.recording_started and len(self.sample_buffer) &gt; 5:\n                self.sample_buffer = self.sample_buffer[-5:]\n\n        # attempt to join finished threads:\n        for thread_id in self.transcription_threads:\n            if self.transcription_threads[thread_id][\"event\"].is_set():\n                self.transcription_threads[thread_id][\"thread\"].join()\n                self.transcription_threads[thread_id][\"joined\"] = True\n\n        voice_detected, output_parameters = self.vad(indata, {})\n        self.logger.debug(f\"Voice detected: {voice_detected}: {output_parameters}\")\n        should_record = False\n        if voice_detected and not self.recording_started:\n            should_record = self._should_record(indata, output_parameters)\n\n        if should_record:\n            self.logger.info(\"starting recording...\")\n            self.recording_started = True\n            thread_id = str(uuid4())[0:8]\n            transcription_thread = Thread(\n                target=self._transcription_thread,\n                args=[thread_id],\n            )\n            transcription_finished = Event()\n            self.active_thread = thread_id\n            self.transcription_threads[thread_id] = {\n                \"thread\": transcription_thread,\n                \"event\": transcription_finished,\n                \"transcription\": \"\",\n                \"joined\": False,\n            }\n\n        if voice_detected:\n            self.logger.debug(\"Voice detected... resetting grace period\")\n            self.grace_period_start = sample_time\n            self._send_ros2_message(\"pause\", \"/voice_commands\")\n            self.is_playing = False\n        if (\n            self.recording_started\n            and sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self.logger.info(\n                \"Grace period ended... stopping recording, starting transcription\"\n            )\n            self.recording_started = False\n            self.grace_period_start = 0\n            with self.sample_buffer_lock:\n                self.transcription_buffers[self.active_thread] = self.sample_buffer\n                self.sample_buffer = []\n            self.transcription_threads[self.active_thread][\"thread\"].start()\n            self.active_thread = \"\"\n            self._send_ros2_message(\"stop\", \"/voice_commands\")\n            self.is_playing = False\n        elif not self.is_playing and (\n            sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self._send_ros2_message(\"play\", \"/voice_commands\")\n            self.is_playing = True\n\n    def _should_record(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; bool:\n        if len(self.should_record_pipeline) == 0:\n            return True\n        for model in self.should_record_pipeline:\n            detected, output = model(audio_data, input_parameters)\n            self.logger.debug(f\"detected {detected}, output {output}\")\n            if detected:\n                model.reset()\n                return True\n        return False\n\n    def _transcription_thread(self, identifier: str):\n        self.logger.info(f\"transcription thread {identifier} started\")\n        audio_data = np.concatenate(self.transcription_buffers[identifier])\n\n        # NOTE: this is only necessary for the local model, but it seems to cause no relevant performance drops in case of cloud models\n        with self.transcription_lock:\n            transcription = self.transcription_model.transcribe(audio_data)\n        self._send_ros2_message(transcription, \"/from_human\")\n        self.transcription_threads[identifier][\"transcription\"] = transcription\n        self.transcription_threads[identifier][\"event\"].set()\n\n    def _send_ros2_message(self, data: str, topic: str):\n        self.logger.debug(f\"Sending message to {topic}: {data}\")\n        if topic == \"/voice_commands\":\n            msg = ROS2Message(payload={\"data\": data})\n            try:\n                self.ros2_connector.send_message(\n                    msg, topic, msg_type=\"std_msgs/msg/String\"\n                )\n            except Exception as e:\n                self.logger.error(f\"Error sending message to {topic}: {e}\")\n        else:\n            msg = ROS2HRIMessage(\n                text=data,\n                message_author=\"human\",\n                communication_id=ROS2HRIMessage.generate_conversation_id(),\n            )\n            self.ros2_hri_connector.send_message(msg, topic)\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent.add_detection_model","title":"<code>add_detection_model(model, pipeline='record')</code>","text":"<p>Add a voice detection model to the specified processing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseVoiceDetectionModel</code> <p>The voice detection model to be added.</p> required <code>pipeline</code> <code>str</code> <p>The pipeline where the model should be added, either 'record' or 'stop'. Default is 'record'.</p> <code>'record'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified pipeline is not 'record' or 'stop'.</p> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>def add_detection_model(\n    self, model: BaseVoiceDetectionModel, pipeline: str = \"record\"\n):\n    \"\"\"\n    Add a voice detection model to the specified processing pipeline.\n\n    Parameters\n    ----------\n    model : BaseVoiceDetectionModel\n        The voice detection model to be added.\n    pipeline : str, optional\n        The pipeline where the model should be added, either 'record' or 'stop'.\n        Default is 'record'.\n\n    Raises\n    ------\n    ValueError\n        If the specified pipeline is not 'record' or 'stop'.\n    \"\"\"\n\n    if pipeline == \"record\":\n        self.should_record_pipeline.append(model)\n    elif pipeline == \"stop\":\n        self.should_stop_pipeline.append(model)\n    else:\n        raise ValueError(\"Pipeline should be either 'record' or 'stop'\")\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent.run","title":"<code>run()</code>","text":"<p>Start the voice recognition agent, initializing the microphone and handling incoming audio samples.</p> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start the voice recognition agent, initializing the microphone and handling incoming audio samples.\n    \"\"\"\n    self.running = True\n    msg = SoundDeviceMessage(read=True)\n    self.listener_handle = self.microphone.start_action(\n        action_data=msg,\n        target=\"microphone\",\n        on_feedback=self._on_new_sample,\n        on_done=lambda: None,\n    )\n    self.logger.info(\"Started Voice Agent\")\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#rai_s2s.asr.agents.asr_agent.SpeechRecognitionAgent.stop","title":"<code>stop()</code>","text":"<p>Clean exit the voice recognition agent, ensuring all transcription threads finish before termination.</p> Source code in <code>rai_s2s/asr/agents/asr_agent.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Clean exit the voice recognition agent, ensuring all transcription threads finish before termination.\n    \"\"\"\n    self.logger.info(\"Stopping Voice Agent\")\n    self.running = False\n    self.microphone.terminate_action(self.listener_handle)\n    self.ros2_hri_connector.shutdown()\n    self.ros2_connector.shutdown()\n    while not all(\n        [thread[\"joined\"] for thread in self.transcription_threads.values()]\n    ):\n        for thread_id in self.transcription_threads:\n            if self.transcription_threads[thread_id][\"event\"].is_set():\n                self.transcription_threads[thread_id][\"thread\"].join()\n                self.transcription_threads[thread_id][\"joined\"] = True\n            else:\n                self.logger.info(\n                    f\"Waiting for transcription of {thread_id} to finish...\"\n                )\n    self.logger.info(\"Voice agent stopped\")\n</code></pre>"},{"location":"speech_to_speech/agents/asr/#purpose","title":"Purpose","text":"<p>The <code>SpeechRecognitionAgent</code> class enables real-time voice processing with the following responsibilities:</p> <ul> <li>Detecting speech through VAD</li> <li>Managing recording state and grace periods</li> <li>Buffering and threading transcription processes</li> <li>Publishing transcriptions and control messages to ROS2 topics</li> <li>Supporting multiple VAD and transcription model types</li> </ul>"},{"location":"speech_to_speech/agents/asr/#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Description <code>microphone_config</code> <code>SoundDeviceConfig</code> Configuration for the microphone input. <code>ros2_name</code> <code>str</code> Name of the ROS2 node. <code>transcription_model</code> <code>BaseTranscriptionModel</code> Model instance for transcribing speech. <code>vad</code> <code>BaseVoiceDetectionModel</code> Model for detecting voice activity. <code>grace_period</code> <code>float</code> Time (in seconds) to continue buffering after speech ends. Defaults to <code>1.0</code>. <code>logger</code> <code>Optional[logging.Logger]</code> Logger instance. If <code>None</code>, defaults to module logger."},{"location":"speech_to_speech/agents/asr/#key-methods","title":"Key Methods","text":""},{"location":"speech_to_speech/agents/asr/#from_config","title":"<code>from_config()</code>","text":"<p>Creates a <code>SpeechRecognitionAgent</code> instance from a YAML config file. Dynamically loads the required transcription and VAD models.</p>"},{"location":"speech_to_speech/agents/asr/#run","title":"<code>run()</code>","text":"<p>Starts the microphone stream and handles incoming audio samples.</p>"},{"location":"speech_to_speech/agents/asr/#stop","title":"<code>stop()</code>","text":"<p>Stops the agent gracefully, joins all running transcription threads, and shuts down ROS2 connectors.</p>"},{"location":"speech_to_speech/agents/asr/#add_detection_modelmodel-pipelinerecord","title":"<code>add_detection_model(model, pipeline=\"record\")</code>","text":"<p>Adds a custom VAD model to a processing pipeline.</p> <ul> <li><code>pipeline</code> can be either <code>'record'</code> or <code>'stop'</code></li> </ul> <p><code>'stop'</code> pipeline</p> <p>The <code>'stop'</code> pipeline is present for forward compatibility. It currently doesn't affect Agent's functioning.</p>"},{"location":"speech_to_speech/agents/asr/#best-practices","title":"Best Practices","text":"<ol> <li>Graceful Shutdown: Always call <code>stop()</code> to ensure transcription threads complete.</li> <li>Model Compatibility: Ensure all transcription and VAD models are compatible with the sample rate (typically 16 kHz).</li> <li>Thread Safety: Use provided locks for shared state, especially around the transcription model.</li> <li>Logging: Utilize <code>self.logger</code> for debug and info logs to aid in tracing activity.</li> <li>Config-driven Design: Use <code>from_config()</code> to ensure modular and portable deployment.</li> </ol>"},{"location":"speech_to_speech/agents/asr/#architecture","title":"Architecture","text":"<p>The <code>SpeechRecognitionAgent</code> typically interacts with the following components:</p> <ul> <li>SoundDeviceConnector: Interfaces with microphone audio input.</li> <li>BaseVoiceDetectionModel: Determines whether speech is present.</li> <li>BaseTranscriptionModel: Converts speech audio into text.</li> <li>ROS2Connector / ROS2HRIConnector: Publishes transcription and control messages to ROS2 topics.</li> <li>Config Loader: Dynamically creates agent from structured config files.</li> </ul>"},{"location":"speech_to_speech/agents/asr/#see-also","title":"See Also","text":"<ul> <li>BaseAgent: Abstract agent class providing lifecycle and logging support.</li> <li>ROS2 Connectors: Communication layer for ROS2 topics.</li> <li>Models: For available voice based models and instructions for creating new ones.</li> <li>TextToSpeech: For TextToSpeechAgent meant for distributed deployment.</li> </ul>"},{"location":"speech_to_speech/agents/overview/","title":"S2S Agents","text":""},{"location":"speech_to_speech/agents/overview/#overview","title":"Overview","text":"<p>Agents in RAI are modular components that encapsulate specific functionalities and behaviors. They follow a consistent interface defined by the <code>BaseAgent</code> class and can be combined to create complex robotic systems. The Speech to Speech Agents are used for voice-based interaction, and communicate with other agents.</p>"},{"location":"speech_to_speech/agents/overview/#speechtospeechagent","title":"SpeechToSpeechAgent","text":"<p><code>SpeechToSpeechAgent</code> is the abstract base class for locally deployable S2S Agents. It provides functionality to manage sound device integration, as well as defines the communication schema for integration with the rest of the system.</p>"},{"location":"speech_to_speech/agents/overview/#class-definition","title":"Class Definition","text":"SpeechToSpeechAgent class definition"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent","title":"<code>rai_s2s.s2s.agents.SpeechToSpeechAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>class SpeechToSpeechAgent(BaseAgent):\n    def __init__(\n        self,\n        from_human_topic: str,\n        to_human_topic: str,\n        *,\n        microphone_config: SoundDeviceConfig,\n        speaker_config: SoundDeviceConfig,\n        transcription_model: BaseTranscriptionModel,\n        vad: BaseVoiceDetectionModel,\n        tts: TTSModel,\n        grace_period: float = 1.0,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        super().__init__()\n        if logger is not None:\n            self.logger = logger\n        self.sound_connector = SoundDeviceConnector(\n            targets=[(\"speaker\", speaker_config)],\n            sources=[(\"microphone\", microphone_config)],\n        )\n\n        self.from_human_topic = from_human_topic\n        self.to_human_topic = to_human_topic\n\n        sample_rate, _, out_channels = self.sound_connector.get_audio_params(\"speaker\")\n        tts.sample_rate = sample_rate\n        tts.channels = out_channels\n\n        self.playback_data = PlayData()\n\n        self.should_record_pipeline: List[BaseVoiceDetectionModel] = []\n        self.should_stop_pipeline: List[BaseVoiceDetectionModel] = []\n\n        self.current_transcription_id = str(uuid4())[0:8]\n        self.current_speech_id = None\n        self.text_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n\n        self.terminate_agent = Event()\n\n        self.audio_generating_thread = Thread(target=self._audio_gen_thread)\n        self.audio_generating_thread.start()\n        self.audio_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n        self.remembered_speech_ids: list[str] = []\n\n        self.tts_model = tts\n\n        self.transcription_model = transcription_model\n\n        self.vad: BaseVoiceDetectionModel = vad\n        self.grace_period = grace_period\n        self.grace_period_start = 0\n\n        self.sample_buffer = []\n        self.sample_buffer_lock = Lock()\n        self.transcription_lock = Lock()\n        self.active_thread = \"\"\n        self.transcription_threads: dict[str, ThreadData] = {}\n        self.transcription_buffers: dict[str, list[NDArray]] = {}\n        self.is_playing = False\n\n        self.recording_started = False\n        # self.ran_setup = False\n\n        self.hri_connector: HRIConnector = self._setup_hri_connector()\n\n        self.microphone_samples: Optional[np.ndarray] = None\n        self.save_flag = False\n\n    @abstractmethod\n    def _setup_hri_connector(self) -&gt; HRIConnector: ...\n\n    def _audio_gen_thread(self):\n        while not self.terminate_agent.wait(timeout=0.01):\n            if self.current_transcription_id in self.text_queues:\n                try:\n                    data = self.text_queues[self.current_transcription_id].get(\n                        block=False\n                    )\n                except Empty:\n                    continue\n                audio = self.tts_model.get_speech(data)\n                try:\n                    self.audio_queues[self.current_transcription_id].put(audio)\n                except KeyError as e:\n                    self.logger.error(\n                        f\"Could not find queue for {self.current_transcription_id}: queuse: {self.audio_queues.keys()}\"\n                    )\n                    raise e\n\n    def run(self):\n        \"\"\"\n        Start the text-to-speech agent, initializing playback and launching the transcription thread.\n        \"\"\"\n        self.running = True\n        self.logger.info(\"Starting SpeechToSpeechAgent...\")\n\n        msg = SoundDeviceMessage(read=False)\n        self.player_handle = self.sound_connector.start_action(\n            action_data=msg,\n            target=\"speaker\",\n            on_feedback=self._speaker_callback,\n            on_done=lambda: None,\n        )\n        msg = SoundDeviceMessage(read=True)\n        self.listener_handle = self.sound_connector.start_action(\n            action_data=msg,\n            target=\"microphone\",\n            on_feedback=self._on_microphone_sample,\n            on_done=lambda: None,\n        )\n        self.logger.info(\"SpeechToSpeechAgent Started!\")\n\n    def _speaker_callback(self, outdata, frames, time, status_dict):\n        set_flags = [flag for flag, status in status_dict.items() if status]\n\n        if set_flags:\n            self.logger.warning(\"Flags set:\" + \", \".join(set_flags))\n        if self.playback_data.playing:\n            if self.playback_data.current_segment is None:\n                try:\n                    self.playback_data.current_segment = self.audio_queues[\n                        self.current_transcription_id\n                    ].get(block=False)\n                    self.playback_data.data = np.array(\n                        self.playback_data.current_segment.get_array_of_samples()  # type: ignore\n                    ).reshape(-1, self.playback_data.channels)\n                except Empty:\n                    pass\n                except KeyError:\n                    pass\n            if self.playback_data.data is not None:\n                current_frame = self.playback_data.current_frame\n                chunksize = min(len(self.playback_data.data) - current_frame, frames)\n                outdata[:chunksize] = self.playback_data.data[\n                    current_frame : current_frame + chunksize\n                ]\n                if chunksize &lt; frames:\n                    outdata[chunksize:] = 0\n                    self.playback_data.current_frame = 0\n                    self.playback_data.current_segment = None\n                    self.playback_data.data = None\n                else:\n                    self.playback_data.current_frame += chunksize\n\n    def _on_microphone_sample(self, indata: np.ndarray, status_flags: dict[str, Any]):\n        sample_time = time.time()\n        with self.sample_buffer_lock:\n            self.sample_buffer.append(indata)\n            if not self.recording_started and len(self.sample_buffer) &gt; 5:\n                self.sample_buffer = self.sample_buffer[-5:]\n\n        # attempt to join finished threads:\n        for thread_id in self.transcription_threads:\n            if self.transcription_threads[thread_id][\"event\"].is_set():\n                self.transcription_threads[thread_id][\"thread\"].join()\n                self.transcription_threads[thread_id][\"joined\"] = True\n\n        voice_detected, output_parameters = self.vad(indata, {})\n        self.logger.debug(f\"Voice detected: {voice_detected}: {output_parameters}\")\n        should_record = False\n        if voice_detected and not self.recording_started:\n            should_record = self._should_record(indata, output_parameters)\n\n        if should_record:\n            self.logger.info(\"starting recording...\")\n            self.recording_started = True\n            thread_id = str(uuid4())[0:8]\n            transcription_thread = Thread(\n                target=self._transcription_thread,\n                args=[thread_id],\n            )\n            transcription_finished = Event()\n            self.active_thread = thread_id\n            self.transcription_threads[thread_id] = {\n                \"thread\": transcription_thread,\n                \"event\": transcription_finished,\n                \"transcription\": \"\",\n                \"joined\": False,\n            }\n\n        if voice_detected:\n            self.logger.debug(\"Voice detected... resetting grace period\")\n            self.grace_period_start = sample_time\n            self.set_playback_state(\"pause\")\n            self.is_playing = False\n        if (\n            self.recording_started\n            and sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self.logger.info(\n                \"Grace period ended... stopping recording, starting transcription\"\n            )\n            self.recording_started = False\n            self.grace_period_start = 0\n            with self.sample_buffer_lock:\n                self.transcription_buffers[self.active_thread] = self.sample_buffer\n                self.sample_buffer = []\n            self.transcription_threads[self.active_thread][\"thread\"].start()\n            self.active_thread = \"\"\n            self.set_playback_state(\"stop\")\n            self.is_playing = False\n        elif not self.is_playing and (\n            sample_time - self.grace_period_start &gt; self.grace_period\n        ):\n            self.set_playback_state(\"play\")\n            self.is_playing = True\n\n    def _should_record(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; bool:\n        if len(self.should_record_pipeline) == 0:\n            return True\n        for model in self.should_record_pipeline:\n            detected, output = model(audio_data, input_parameters)\n            self.logger.debug(f\"detected {detected}, output {output}\")\n            if detected:\n                model.reset()\n                return True\n        return False\n\n    def _transcription_thread(self, identifier: str):\n        self.logger.info(f\"transcription thread {identifier} started\")\n        audio_data = np.concatenate(self.transcription_buffers[identifier])\n        with (\n            self.transcription_lock\n        ):  # this is only necessary for the local model... TODO: fix this somehow\n            transcription = self.transcription_model.transcribe(audio_data)\n        self._send_from_human_message(transcription)\n        self.transcription_threads[identifier][\"transcription\"] = transcription\n        self.transcription_threads[identifier][\"event\"].set()\n\n    @abstractmethod\n    def _send_from_human_message(self, data: str): ...\n\n    def _on_to_human_message(self, message: HRIMessage):\n        self.logger.info(f\"Receieved message from human: {message.text}\")\n        self.logger.warning(\n            f\"Starting playback, current id: {self.current_transcription_id}\"\n        )\n        if (\n            self.current_speech_id is None\n            and message.communication_id is not None\n            and message.communication_id not in self.remembered_speech_ids\n        ):\n            self.current_speech_id = message.communication_id\n            self.remembered_speech_ids.append(self.current_speech_id)\n            if len(self.remembered_speech_ids) &gt; 64:\n                self.remembered_speech_ids.pop(0)\n        if self.current_speech_id == message.communication_id:\n            self.text_queues[self.current_transcription_id].put(message.text)\n        self.playback_data.playing = True\n\n    def add_detection_model(self, model: BaseVoiceDetectionModel):\n        \"\"\"\n        Add a voice detection model to check before recording starts.\n\n        Parameters\n        ----------\n        model : BaseVoiceDetectionModel\n            The voice detection model to be added.\n        \"\"\"\n\n        self.should_record_pipeline.append(model)\n\n    def set_playback_state(self, state: Literal[\"play\", \"pause\", \"stop\"]):\n        \"\"\"\n        Set the playback state of the system.\n\n        Parameters\n        ----------\n        state : {\"play\", \"pause\", \"stop\"}\n            The desired playback state:\n            - \"play\": Start or resume playback.\n            - \"pause\": Pause the current playback.\n            - \"stop\": Stop playback and reset playback-related data and queues.\n\n        Notes\n        -----\n        - When state is \"stop\", this method:\n          - Resets the `current_speech_id`.\n          - Generates a new `current_transcription_id`.\n          - Initializes new audio and text queues.\n          - Clears previous playback data.\n        - Logs actions and transitions for debugging and monitoring purposes.\n        \"\"\"\n        if state == \"play\":\n            self.playback_data.playing = True\n        elif state == \"pause\":\n            self.playback_data.playing = False\n        elif state == \"stop\":\n            self.current_speech_id = None\n            self.playback_data.playing = False\n            previous_id = self.current_transcription_id\n            self.logger.warning(f\"Stopping playback, previous id: {previous_id}\")\n            self.current_transcription_id = str(uuid4())[0:8]\n            self.audio_queues[self.current_transcription_id] = Queue()\n            self.text_queues[self.current_transcription_id] = Queue()\n            try:\n                del self.audio_queues[previous_id]\n                del self.text_queues[previous_id]\n            except KeyError:\n                pass\n            self.playback_data.data = None\n            self.playback_data.current_frame = 0\n            self.playback_data.current_segment = None\n\n        self.logger.debug(f\"Current status is: {self.playback_data.playing}\")\n\n    def stop(self):\n        \"\"\"\n        Clean exit the speech-to-speech agent, terminating playback and joining the transcription thread.\n        \"\"\"\n        self.sound_connector.shutdown()\n\n        self.logger.info(\"Stopping TextToSpeechAgent\")\n        self.terminate_agent.set()\n        if self.audio_generating_thread is not None:\n            self.audio_generating_thread.join()\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.add_detection_model","title":"<code>add_detection_model(model)</code>","text":"<p>Add a voice detection model to check before recording starts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseVoiceDetectionModel</code> <p>The voice detection model to be added.</p> required Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def add_detection_model(self, model: BaseVoiceDetectionModel):\n    \"\"\"\n    Add a voice detection model to check before recording starts.\n\n    Parameters\n    ----------\n    model : BaseVoiceDetectionModel\n        The voice detection model to be added.\n    \"\"\"\n\n    self.should_record_pipeline.append(model)\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.run","title":"<code>run()</code>","text":"<p>Start the text-to-speech agent, initializing playback and launching the transcription thread.</p> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start the text-to-speech agent, initializing playback and launching the transcription thread.\n    \"\"\"\n    self.running = True\n    self.logger.info(\"Starting SpeechToSpeechAgent...\")\n\n    msg = SoundDeviceMessage(read=False)\n    self.player_handle = self.sound_connector.start_action(\n        action_data=msg,\n        target=\"speaker\",\n        on_feedback=self._speaker_callback,\n        on_done=lambda: None,\n    )\n    msg = SoundDeviceMessage(read=True)\n    self.listener_handle = self.sound_connector.start_action(\n        action_data=msg,\n        target=\"microphone\",\n        on_feedback=self._on_microphone_sample,\n        on_done=lambda: None,\n    )\n    self.logger.info(\"SpeechToSpeechAgent Started!\")\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.set_playback_state","title":"<code>set_playback_state(state)</code>","text":"<p>Set the playback state of the system.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>(play, pause, stop)</code> <p>The desired playback state: - \"play\": Start or resume playback. - \"pause\": Pause the current playback. - \"stop\": Stop playback and reset playback-related data and queues.</p> <code>\"play\"</code> Notes <ul> <li>When state is \"stop\", this method:</li> <li>Resets the <code>current_speech_id</code>.</li> <li>Generates a new <code>current_transcription_id</code>.</li> <li>Initializes new audio and text queues.</li> <li>Clears previous playback data.</li> <li>Logs actions and transitions for debugging and monitoring purposes.</li> </ul> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def set_playback_state(self, state: Literal[\"play\", \"pause\", \"stop\"]):\n    \"\"\"\n    Set the playback state of the system.\n\n    Parameters\n    ----------\n    state : {\"play\", \"pause\", \"stop\"}\n        The desired playback state:\n        - \"play\": Start or resume playback.\n        - \"pause\": Pause the current playback.\n        - \"stop\": Stop playback and reset playback-related data and queues.\n\n    Notes\n    -----\n    - When state is \"stop\", this method:\n      - Resets the `current_speech_id`.\n      - Generates a new `current_transcription_id`.\n      - Initializes new audio and text queues.\n      - Clears previous playback data.\n    - Logs actions and transitions for debugging and monitoring purposes.\n    \"\"\"\n    if state == \"play\":\n        self.playback_data.playing = True\n    elif state == \"pause\":\n        self.playback_data.playing = False\n    elif state == \"stop\":\n        self.current_speech_id = None\n        self.playback_data.playing = False\n        previous_id = self.current_transcription_id\n        self.logger.warning(f\"Stopping playback, previous id: {previous_id}\")\n        self.current_transcription_id = str(uuid4())[0:8]\n        self.audio_queues[self.current_transcription_id] = Queue()\n        self.text_queues[self.current_transcription_id] = Queue()\n        try:\n            del self.audio_queues[previous_id]\n            del self.text_queues[previous_id]\n        except KeyError:\n            pass\n        self.playback_data.data = None\n        self.playback_data.current_frame = 0\n        self.playback_data.current_segment = None\n\n    self.logger.debug(f\"Current status is: {self.playback_data.playing}\")\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.SpeechToSpeechAgent.stop","title":"<code>stop()</code>","text":"<p>Clean exit the speech-to-speech agent, terminating playback and joining the transcription thread.</p> Source code in <code>rai_s2s/s2s/agents/s2s_agent.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Clean exit the speech-to-speech agent, terminating playback and joining the transcription thread.\n    \"\"\"\n    self.sound_connector.shutdown()\n\n    self.logger.info(\"Stopping TextToSpeechAgent\")\n    self.terminate_agent.set()\n    if self.audio_generating_thread is not None:\n        self.audio_generating_thread.join()\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#communication","title":"Communication","text":"<p>The Agent communicates through two communication channels provided during initialization - <code>from_human</code> and <code>to_human</code>. On the <code>from_human</code> channel text transcribed from human voice is published. On the <code>to_human</code> channel receives text to be played to the human through text-to-speech.</p>"},{"location":"speech_to_speech/agents/overview/#voice-interaction","title":"Voice interaction","text":"<p>The voice interaction is performed through two audio streams, with two devices. These devices can be different, but don't have to - and in case of most local deployments they will be the same. The list of available sounddevices for configuration can be obtained by running <code>python -c \"import sounddevice as sd; print(sd.query_devices())\"</code>. The configuration requires the user to specify the name of the sound device to be used for interfacing. This is the entire string from the index until the comma before the hostapi (typically <code>ALSA</code> on Ubuntu).</p> <p>The voice interaction works as follows: - The user speaks, which leads to the <code>VoiceActivityDetection</code> model activation. - [Optional] the recording pipeline (containing other models like OpenWakeWord) runs checks. - The recording starts. - The recording continues until the user stops talking (based on silence grace period). - The recording is transcribed and sent to the system. - The Agent receives text data to be played to the user. - The playback begins. - The playback can be interrupted by user speaking: - if there is additional recording pipeline the playback will pause while the user speaks (and continue, if the pipeline returns false). - otherwise the new recording will be send to the system, and transcription will stop the playback.</p>"},{"location":"speech_to_speech/agents/overview/#implementations","title":"Implementations","text":"<p>ROS based implementation is available in <code>ROS2S2SAgent</code>.</p> ROS2S2SAgent class definition"},{"location":"speech_to_speech/agents/overview/#rai_s2s.s2s.agents.ros2s2s_agent.ROS2S2SAgent","title":"<code>rai_s2s.s2s.agents.ros2s2s_agent.ROS2S2SAgent</code>","text":"<p>               Bases: <code>SpeechToSpeechAgent</code></p> Source code in <code>rai_s2s/s2s/agents/ros2s2s_agent.py</code> <pre><code>class ROS2S2SAgent(SpeechToSpeechAgent):\n    def __init__(\n        self,\n        from_human_topic: str,\n        to_human_topic: str,\n        *,\n        microphone_config: SoundDeviceConfig,\n        speaker_config: SoundDeviceConfig,\n        transcription_model: BaseTranscriptionModel,\n        vad: BaseVoiceDetectionModel,\n        tts: TTSModel,\n        grace_period: float = 1,\n        logger: Optional[logging.Logger] = None,\n        **kwargs,\n    ):\n        super().__init__(\n            from_human_topic,\n            to_human_topic,\n            microphone_config=microphone_config,\n            speaker_config=speaker_config,\n            transcription_model=transcription_model,\n            vad=vad,\n            tts=tts,\n            grace_period=grace_period,\n            logger=logger,\n            **kwargs,\n        )\n\n    def _setup_hri_connector(self):\n        hri_connector = ROS2HRIConnector()\n        hri_connector.register_callback(self.to_human_topic, self._on_to_human_message)\n        return hri_connector\n\n    def _send_from_human_message(self, data: str):\n        print(f\"Sending message to {self.from_human_topic}\")\n        self.hri_connector.send_message(\n            ROS2HRIMessage(text=data), self.from_human_topic\n        )\n\n    def _send_to_human_message(self, data: str):\n        print(f\"Sending message to {self.to_human_topic}\")\n        self.hri_connector.send_message(ROS2HRIMessage(text=data), self.to_human_topic)\n</code></pre>"},{"location":"speech_to_speech/agents/overview/#see-also","title":"See Also","text":"<ul> <li>Models: For available voice based models and instructions for creating new ones.</li> <li>AutomaticSpeechRecognition: For AutomaticSpeechRecognitionAgent meant for distributed deployment.</li> <li>TextToSpeech: For TextToSpeechAgent meant for distributed deployment.</li> </ul>"},{"location":"speech_to_speech/agents/tts/","title":"TextToSpeechAgent","text":""},{"location":"speech_to_speech/agents/tts/#overview","title":"Overview","text":"<p>The <code>TextToSpeechAgent</code> in the RAI framework is a modular agent responsible for converting incoming text into audio using a text-to-speech (TTS) model and playing it through a configured audio output device. It supports real-time playback control through ROS2 messages and handles asynchronous speech processing using threads and queues.</p>"},{"location":"speech_to_speech/agents/tts/#class-definition","title":"Class Definition","text":"TextToSpeechAgent class definition"},{"location":"speech_to_speech/agents/tts/#rai_s2s.tts.agents.TextToSpeechAgent","title":"<code>rai_s2s.tts.agents.TextToSpeechAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>Agent responsible for converting text to speech and handling audio playback.</p> <p>Parameters:</p> Name Type Description Default <code>speaker_config</code> <code>SoundDeviceConfig</code> <p>Configuration for the sound device used for playback.</p> required <code>ros2_name</code> <code>str</code> <p>Name of the ROS2 node.</p> required <code>tts</code> <code>TTSModel</code> <p>Text-to-speech model used for generating audio.</p> required <code>logger</code> <code>Optional[Logger]</code> <p>Logger instance for logging messages, by default None.</p> <code>None</code> <code>max_speech_history</code> <code>int</code> <p>Maximum amount of speech ids to remember, by default 64</p> <code>64</code> Source code in <code>rai_s2s/tts/agents/tts_agent.py</code> <pre><code>class TextToSpeechAgent(BaseAgent):\n    \"\"\"\n    Agent responsible for converting text to speech and handling audio playback.\n\n    Parameters\n    ----------\n    speaker_config : SoundDeviceConfig\n        Configuration for the sound device used for playback.\n    ros2_name : str\n        Name of the ROS2 node.\n    tts : TTSModel\n        Text-to-speech model used for generating audio.\n    logger : Optional[logging.Logger], optional\n        Logger instance for logging messages, by default None.\n    max_speech_history : int, optional\n        Maximum amount of speech ids to remember, by default 64\n    \"\"\"\n\n    def __init__(\n        self,\n        speaker_config: SoundDeviceConfig,\n        ros2_name: str,\n        tts: TTSModel,\n        logger: Optional[logging.Logger] = None,\n        max_speech_history=64,\n    ):\n        if logger is None:\n            self.logger = logging.getLogger(__name__)\n        else:\n            self.logger = logger\n\n        self.speaker = SoundDeviceConnector(\n            targets=[(\"speaker\", speaker_config)], sources=[]\n        )\n        sample_rate, _, out_channels = self.speaker.get_audio_params(\"speaker\")\n        tts.sample_rate = sample_rate\n        tts.channels = out_channels\n\n        self.node_base_name = ros2_name\n        self.model = tts\n        self.ros2_connector = self._setup_ros2_connector()\n        super().__init__()\n\n        self.current_transcription_id = str(uuid4())[0:8]\n        self.current_speech_id = None\n        self.text_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n        self.audio_queues: dict[str, Queue] = {self.current_transcription_id: Queue()}\n        self.remembered_speech_ids: list[str] = []\n\n        self.tog_play_event = Event()\n        self.stop_event = Event()\n        self.current_audio = None\n\n        self.terminate_agent = Event()\n        self.transcription_thread = None\n        self.running = False\n\n        self.playback_data = PlayData()\n\n    @classmethod\n    def from_config(cls, cfg_path: Optional[str] = None) -&gt; Self:\n        cfg = load_config(cfg_path)\n        config = SoundDeviceConfig(\n            stream=True,\n            is_output=True,\n            device_name=cfg.speaker.device_name,\n        )\n        match cfg.text_to_speech.model_type:\n            case \"ElevenLabs\":\n                from rai_s2s.tts.models import ElevenLabsTTS\n\n                if cfg.text_to_speech.voice != \"\":\n                    model = ElevenLabsTTS(voice=cfg.text_to_speech.voice)\n                else:\n                    raise ValueError(\"ElevenLabs [tts] vendor required voice to be set\")\n            case \"OpenTTS\":\n                from rai_s2s.tts.models import OpenTTS\n\n                if cfg.text_to_speech.voice != \"\":\n                    model = OpenTTS(voice=cfg.text_to_speech.voice)\n                else:\n                    model = OpenTTS()\n            case \"KokoroTTS\":\n                from rai_s2s.tts.models import KokoroTTS\n\n                if cfg.text_to_speech.voice != \"\":\n                    model = KokoroTTS(voice=cfg.text_to_speech.voice)\n                else:\n                    model = KokoroTTS()\n            case _:\n                raise ValueError(f\"Unknown model_type: {cfg.text_to_speech.model_type}\")\n        return cls(config, \"rai_auto_tts\", model)\n\n    def __call__(self):\n        self.run()\n\n    def run(self):\n        \"\"\"\n        Start the text-to-speech agent, initializing playback and launching the transcription thread.\n        \"\"\"\n        self.running = True\n        self.logger.info(\"TextToSpeechAgent started\")\n        self.transcription_thread = Thread(target=self._transcription_thread)\n        self.transcription_thread.start()\n\n        msg = SoundDeviceMessage(read=False)\n        self.speaker.start_action(\n            msg,\n            \"speaker\",\n            on_feedback=self._speaker_callback,\n            on_done=lambda: None,\n        )\n\n    def _speaker_callback(self, outdata, frames, time, status_dict):\n        set_flags = [flag for flag, status in status_dict.items() if status]\n\n        if set_flags:\n            self.logger.warning(\"Flags set:\" + \", \".join(set_flags))\n        if self.playback_data.playing:\n            if self.playback_data.current_segment is None:\n                try:\n                    self.playback_data.current_segment = self.audio_queues[\n                        self.current_transcription_id\n                    ].get(block=False)\n                    self.playback_data.data = np.array(\n                        self.playback_data.current_segment.get_array_of_samples()  # type: ignore\n                    ).reshape(-1, self.playback_data.channels)\n                except Empty:\n                    pass\n                except KeyError:\n                    pass\n            if self.playback_data.data is not None:\n                current_frame = self.playback_data.current_frame\n                chunksize = min(len(self.playback_data.data) - current_frame, frames)\n                outdata[:chunksize] = self.playback_data.data[\n                    current_frame : current_frame + chunksize\n                ]\n                if chunksize &lt; frames:\n                    outdata[chunksize:] = 0\n                    self.playback_data.current_frame = 0\n                    self.playback_data.current_segment = None\n                    self.playback_data.data = None\n                else:\n                    self.playback_data.current_frame += chunksize\n\n        if not self.playback_data.playing:\n            outdata[:] = np.zeros(outdata.size).reshape(outdata.shape)\n\n    def stop(self):\n        \"\"\"\n        Clean exit the text-to-speech agent, terminating playback and joining the transcription thread.\n        \"\"\"\n        self.logger.info(\"Stopping TextToSpeechAgent\")\n        self.terminate_agent.set()\n        if self.transcription_thread is not None:\n            self.transcription_thread.join()\n\n    def _transcription_thread(self):\n        while not self.terminate_agent.wait(timeout=0.01):\n            if self.current_transcription_id in self.text_queues:\n                try:\n                    data = self.text_queues[self.current_transcription_id].get(\n                        block=False\n                    )\n                except Empty:\n                    continue\n                audio = self.model.get_speech(data)\n                try:\n                    self.audio_queues[self.current_transcription_id].put(audio)\n                except KeyError as e:\n                    self.logger.error(\n                        f\"Could not find queue for {self.current_transcription_id}: queuse: {self.audio_queues.keys()}\"\n                    )\n                    raise e\n\n    def _setup_ros2_connector(self):\n        self.hri_ros2_connector = ROS2HRIConnector(\n            self.node_base_name  # , \"single_threaded\"\n        )\n        self.hri_ros2_connector.register_callback(\n            \"/to_human\", self._on_to_human_message\n        )\n        self.ros2_connector = ROS2Connector(\n            self.node_base_name  # , False, \"single_threaded\"\n        )\n        self.ros2_connector.register_callback(\n            \"/voice_commands\", self._on_command_message, msg_type=\"std_msgs/msg/String\"\n        )\n\n    def _on_to_human_message(self, msg: ROS2HRIMessage):\n        self.logger.debug(f\"Receieved message from human: {msg.text}\")\n        self.logger.warning(\n            f\"Starting playback, current id: {self.current_transcription_id}\"\n        )\n        if (\n            self.current_speech_id is None\n            and msg.communication_id is not None\n            and msg.communication_id not in self.remembered_speech_ids\n        ):\n            self.current_speech_id = msg.communication_id\n            self.remembered_speech_ids.append(self.current_speech_id)\n            if len(self.remembered_speech_ids) &gt; 64:\n                self.remembered_speech_ids.pop(0)\n        if self.current_speech_id == msg.communication_id:\n            self.text_queues[self.current_transcription_id].put(msg.text)\n        self.playback_data.playing = True\n\n    def _on_command_message(self, message: ROS2Message):\n        self.logger.info(f\"Receieved status message: {message}\")\n        if message.payload.data == \"tog_play\":\n            self.playback_data.playing = not self.playback_data.playing\n        elif message.payload.data == \"play\":\n            self.playback_data.playing = True\n        elif message.payload.data == \"pause\":\n            self.playback_data.playing = False\n        elif message.payload.data == \"stop\":\n            self.current_speech_id = None\n            self.playback_data.playing = False\n            previous_id = self.current_transcription_id\n            self.logger.warning(f\"Stopping playback, previous id: {previous_id}\")\n            self.current_transcription_id = str(uuid4())[0:8]\n            self.audio_queues[self.current_transcription_id] = Queue()\n            self.text_queues[self.current_transcription_id] = Queue()\n            try:\n                del self.audio_queues[previous_id]\n                del self.text_queues[previous_id]\n            except KeyError:\n                pass\n            self.playback_data.data = None\n            self.playback_data.current_frame = 0\n            self.playback_data.current_segment = None\n\n        self.logger.debug(f\"Current status is: {self.playback_data.playing}\")\n</code></pre>"},{"location":"speech_to_speech/agents/tts/#rai_s2s.tts.agents.TextToSpeechAgent.run","title":"<code>run()</code>","text":"<p>Start the text-to-speech agent, initializing playback and launching the transcription thread.</p> Source code in <code>rai_s2s/tts/agents/tts_agent.py</code> <pre><code>def run(self):\n    \"\"\"\n    Start the text-to-speech agent, initializing playback and launching the transcription thread.\n    \"\"\"\n    self.running = True\n    self.logger.info(\"TextToSpeechAgent started\")\n    self.transcription_thread = Thread(target=self._transcription_thread)\n    self.transcription_thread.start()\n\n    msg = SoundDeviceMessage(read=False)\n    self.speaker.start_action(\n        msg,\n        \"speaker\",\n        on_feedback=self._speaker_callback,\n        on_done=lambda: None,\n    )\n</code></pre>"},{"location":"speech_to_speech/agents/tts/#rai_s2s.tts.agents.TextToSpeechAgent.stop","title":"<code>stop()</code>","text":"<p>Clean exit the text-to-speech agent, terminating playback and joining the transcription thread.</p> Source code in <code>rai_s2s/tts/agents/tts_agent.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Clean exit the text-to-speech agent, terminating playback and joining the transcription thread.\n    \"\"\"\n    self.logger.info(\"Stopping TextToSpeechAgent\")\n    self.terminate_agent.set()\n    if self.transcription_thread is not None:\n        self.transcription_thread.join()\n</code></pre>"},{"location":"speech_to_speech/agents/tts/#purpose","title":"Purpose","text":"<p>The <code>TextToSpeechAgent</code> enables:</p> <ul> <li>Real-time conversion of text to speech</li> <li>Playback control (play/pause/stop) via ROS2 messages</li> <li>Dynamic loading of TTS models from configuration</li> <li>Robust audio handling using queues and event-driven logic</li> <li>Integration with human-robot interaction topics (HRI)</li> </ul>"},{"location":"speech_to_speech/agents/tts/#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Description <code>speaker_config</code> <code>SoundDeviceConfig</code> Configuration for the audio output (speaker). <code>ros2_name</code> <code>str</code> Name of the ROS2 node. <code>tts</code> <code>TTSModel</code> Text-to-speech model instance. <code>logger</code> <code>Optional[logging.Logger]</code> Logger instance, or default logger if <code>None</code>. <code>max_speech_history</code> <code>int</code> Number of speech message IDs to remember (default: 64)."},{"location":"speech_to_speech/agents/tts/#key-methods","title":"Key Methods","text":""},{"location":"speech_to_speech/agents/tts/#from_configcfg_path-optionalstr","title":"<code>from_config(cfg_path: Optional[str])</code>","text":"<p>Instantiates the agent from a configuration file, dynamically selecting the TTS model and setting up audio output.</p>"},{"location":"speech_to_speech/agents/tts/#run","title":"<code>run()</code>","text":"<p>Initializes the agent:</p> <ul> <li>Starts a thread to handle queued text-to-speech conversion</li> <li>Launches speaker playback via <code>SoundDeviceConnector</code></li> </ul>"},{"location":"speech_to_speech/agents/tts/#stop","title":"<code>stop()</code>","text":"<p>Gracefully stops the agent by setting the termination flag and joining the transcription thread.</p>"},{"location":"speech_to_speech/agents/tts/#communication","title":"Communication","text":"<p>The Agent uses the <code>ROS2HRIConnector</code> for connection through 2 ROS2 topics:</p> <ul> <li><code>/to_human</code>: Incoming text messages to convert. Uses <code>rai_interfaces/msg/HRIMessage</code>.</li> <li><code>/voice_commands</code>: Playback control with ROS2 <code>std_msgs/msg/String</code>. Valid values: <code>\"play\"</code>, <code>\"pause\"</code>, <code>\"stop\"</code></li> </ul>"},{"location":"speech_to_speech/agents/tts/#best-practices","title":"Best Practices","text":"<ol> <li>Queue Management: Properly track transcription IDs to avoid queue collisions or memory leaks.</li> <li>Playback Sync: Ensure audio queues are flushed on <code>stop</code> to avoid replaying outdated speech.</li> <li>Graceful Shutdown: Always call <code>stop()</code> to terminate threads cleanly.</li> <li>Model Configuration: Ensure model-specific settings (e.g., voice selection for ElevenLabs) are defined in config files.</li> </ol>"},{"location":"speech_to_speech/agents/tts/#architecture","title":"Architecture","text":"<p>The <code>TextToSpeechAgent</code> interacts with the following core components:</p> <ul> <li>TTSModel: Converts text into audio (e.g., ElevenLabsTTS, OpenTTS)</li> <li>SoundDeviceConnector: Sends synthesized audio to output hardware</li> <li>ROS2HRIConnector: Handles incoming HRI and command messages</li> <li>Queues and Threads: Enable asynchronous and buffered audio processing</li> </ul>"},{"location":"speech_to_speech/agents/tts/#see-also","title":"See Also","text":"<ul> <li>BaseAgent: Abstract base for all agents in RAI</li> <li>SoundDeviceConnector: For details on speaker configuration and streaming</li> <li>Text-to-Speech Models: Supported TTS engines and usage</li> <li>ROS2 HRI Messaging: Interfacing with <code>/to_human</code> and <code>/voice_commands</code></li> </ul>"},{"location":"speech_to_speech/models/overview/","title":"Models","text":""},{"location":"speech_to_speech/models/overview/#overview","title":"Overview","text":"<p>This package provides three primary types of models:</p> <ul> <li>Voice Activity Detection (VAD)</li> <li>Wake Word Detection</li> <li>Transcription</li> </ul> <p>These models are designed with simple and consistent interfaces to allow chaining and integration into audio processing pipelines.</p>"},{"location":"speech_to_speech/models/overview/#model-interfaces","title":"Model Interfaces","text":""},{"location":"speech_to_speech/models/overview/#vad-and-wake-word-detection-api","title":"VAD and Wake Word Detection API","text":"<p>All VAD and Wake Word detection models implement a common <code>detect</code> interface:</p> <pre><code>    def detect(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; Tuple[bool, dict[str, Any]]:\n</code></pre> <p>This design supports chaining multiple models together by passing the output dictionary (<code>input_parameters</code>) from one model into the next.</p>"},{"location":"speech_to_speech/models/overview/#transcription-api","title":"Transcription API","text":"<p>Transcription models implement the <code>transcribe</code> method:</p> <pre><code>    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n</code></pre> <p>This method takes raw audio data encoded as 2-byte integers and returns the corresponding text transcription.</p>"},{"location":"speech_to_speech/models/overview/#included-models","title":"Included Models","text":""},{"location":"speech_to_speech/models/overview/#silerovad","title":"SileroVAD","text":"<ul> <li>Open source model: GitHub</li> <li>No additional setup required</li> <li>Returns a confidence value indicating the presence of speech in the audio</li> </ul> SileroVAD"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.silero_vad.SileroVAD","title":"<code>rai_s2s.asr.models.silero_vad.SileroVAD</code>","text":"<p>               Bases: <code>BaseVoiceDetectionModel</code></p> <p>Voice Activity Detection (VAD) model using SileroVAD.</p> <p>This class loads the SileroVAD model from Torch Hub and detects speech presence in an audio signal. It supports two sampling rates: 8000 Hz and 16000 Hz.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_rate</code> <code>Literal[8000, 16000]</code> <p>The sampling rate of the input audio. Must be either 8000 or 16000. Default is 16000.</p> <code>16000</code> <code>threshold</code> <code>float</code> <p>Confidence threshold for voice detection. If the VAD confidence exceeds this threshold, the method returns <code>True</code> (indicating voice presence). Default is 0.5.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the VAD model, set to <code>\"silero_vad\"</code>.</p> <code>model</code> <code>Module</code> <p>The loaded SileroVAD model.</p> <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the input audio (either 8000 or 16000).</p> <code>window_size</code> <code>int</code> <p>The size of the processing window, determined by the sampling rate. - 512 samples for 16000 Hz - 256 samples for 8000 Hz</p> <code>threshold</code> <code>float</code> <p>Confidence threshold for determining voice activity.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported sampling rate is provided.</p> Source code in <code>rai_s2s/asr/models/silero_vad.py</code> <pre><code>class SileroVAD(BaseVoiceDetectionModel):\n    \"\"\"\n    Voice Activity Detection (VAD) model using SileroVAD.\n\n    This class loads the SileroVAD model from Torch Hub and detects speech presence in an audio signal.\n    It supports two sampling rates: 8000 Hz and 16000 Hz.\n\n    Parameters\n    ----------\n    sampling_rate : Literal[8000, 16000], optional\n        The sampling rate of the input audio. Must be either 8000 or 16000. Default is 16000.\n    threshold : float, optional\n        Confidence threshold for voice detection. If the VAD confidence exceeds this threshold,\n        the method returns `True` (indicating voice presence). Default is 0.5.\n\n    Attributes\n    ----------\n    model_name : str\n        Name of the VAD model, set to `\"silero_vad\"`.\n    model : torch.nn.Module\n        The loaded SileroVAD model.\n    sampling_rate : int\n        The sampling rate of the input audio (either 8000 or 16000).\n    window_size : int\n        The size of the processing window, determined by the sampling rate.\n        - 512 samples for 16000 Hz\n        - 256 samples for 8000 Hz\n    threshold : float\n        Confidence threshold for determining voice activity.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported sampling rate is provided.\n    \"\"\"\n\n    def __init__(self, sampling_rate: Literal[8000, 16000] = 16000, threshold=0.5):\n        super(SileroVAD, self).__init__()\n        self.model_name = \"silero_vad\"\n        self.model, _ = torch.hub.load(\n            repo_or_dir=\"snakers4/silero-vad\",\n            model=self.model_name,\n        )  # type: ignore\n        # NOTE: See silero vad implementation: https://github.com/snakers4/silero-vad/blob/9060f664f20eabb66328e4002a41479ff288f14c/src/silero_vad/utils_vad.py#L61\n        if sampling_rate == 16000:\n            self.sampling_rate = 16000\n            self.window_size = 512\n        elif sampling_rate == 8000:\n            self.sampling_rate = 8000\n            self.window_size = 256\n        else:\n            raise ValueError(\n                \"Only 8000 and 16000 sampling rates are supported\"\n            )  # TODO: consider if this should be a ValueError or something else\n        self.threshold = threshold\n\n    def _int2float(self, sound: NDArray[np.int16]):\n        converted_sound = sound.astype(\"float32\")\n        converted_sound *= 1 / 32768\n        converted_sound = converted_sound.squeeze()\n        return converted_sound\n\n    def detect(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; Tuple[bool, dict[str, Any]]:\n        \"\"\"\n        Detects voice activity in the given audio data.\n\n        This method processes a window of the most recent audio samples, computes a confidence score\n        using the SileroVAD model, and determines if the confidence exceeds the specified threshold.\n\n        Parameters\n        ----------\n        audio_data : NDArray\n            A NumPy array containing audio input data.\n        input_parameters : dict of str to Any\n            Additional parameters for detection.\n\n        Returns\n        -------\n        Tuple[bool, dict]\n            - A boolean indicating whether voice activity was detected (`True` if detected, `False` otherwise).\n            - A dictionary containing the computed VAD confidence score.\n        \"\"\"\n        vad_confidence = self.model(\n            torch.tensor(self._int2float(audio_data[-self.window_size :])),\n            self.sampling_rate,\n        ).item()\n        ret = input_parameters.copy()\n        ret.update({self.model_name: {\"vad_confidence\": vad_confidence}})\n\n        return vad_confidence &gt; self.threshold, ret\n\n    def reset(self):\n        \"\"\"\n        Resets the voice activity detection model.\n        \"\"\"\n        self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.silero_vad.SileroVAD.detect","title":"<code>detect(audio_data, input_parameters)</code>","text":"<p>Detects voice activity in the given audio data.</p> <p>This method processes a window of the most recent audio samples, computes a confidence score using the SileroVAD model, and determines if the confidence exceeds the specified threshold.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>NDArray</code> <p>A NumPy array containing audio input data.</p> required <code>input_parameters</code> <code>dict of str to Any</code> <p>Additional parameters for detection.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, dict]</code> <ul> <li>A boolean indicating whether voice activity was detected (<code>True</code> if detected, <code>False</code> otherwise).</li> <li>A dictionary containing the computed VAD confidence score.</li> </ul> Source code in <code>rai_s2s/asr/models/silero_vad.py</code> <pre><code>def detect(\n    self, audio_data: NDArray, input_parameters: dict[str, Any]\n) -&gt; Tuple[bool, dict[str, Any]]:\n    \"\"\"\n    Detects voice activity in the given audio data.\n\n    This method processes a window of the most recent audio samples, computes a confidence score\n    using the SileroVAD model, and determines if the confidence exceeds the specified threshold.\n\n    Parameters\n    ----------\n    audio_data : NDArray\n        A NumPy array containing audio input data.\n    input_parameters : dict of str to Any\n        Additional parameters for detection.\n\n    Returns\n    -------\n    Tuple[bool, dict]\n        - A boolean indicating whether voice activity was detected (`True` if detected, `False` otherwise).\n        - A dictionary containing the computed VAD confidence score.\n    \"\"\"\n    vad_confidence = self.model(\n        torch.tensor(self._int2float(audio_data[-self.window_size :])),\n        self.sampling_rate,\n    ).item()\n    ret = input_parameters.copy()\n    ret.update({self.model_name: {\"vad_confidence\": vad_confidence}})\n\n    return vad_confidence &gt; self.threshold, ret\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.silero_vad.SileroVAD.reset","title":"<code>reset()</code>","text":"<p>Resets the voice activity detection model.</p> Source code in <code>rai_s2s/asr/models/silero_vad.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the voice activity detection model.\n    \"\"\"\n    self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#openwakeword","title":"OpenWakeWord","text":"<ul> <li>Open source project: GitHub</li> <li>Supports predefined and custom wake words</li> <li>Returns <code>True</code> when the specified wake word is detected in the audio</li> </ul> OpenWakeWord"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord","title":"<code>rai_s2s.asr.models.open_wake_word.OpenWakeWord</code>","text":"<p>               Bases: <code>BaseVoiceDetectionModel</code></p> <p>A wake word detection model using the Open Wake Word framework.</p> <p>This class loads a specified wake word model and detects whether a wake word is present in the provided audio input.</p> <p>Parameters:</p> Name Type Description Default <code>wake_word_model_path</code> <code>str</code> <p>Path to the wake word model file or name of a standard one.</p> required <code>threshold</code> <code>float</code> <p>The confidence threshold for wake word detection. If a prediction surpasses this value, the model will trigger a wake word detection. Default is 0.1.</p> <code>0.1</code> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model, set to <code>\"open_wake_word\"</code>.</p> <code>model</code> <code>Model</code> <p>The Open Wake Word model instance used for inference.</p> <code>threshold</code> <code>float</code> <p>The confidence threshold for determining wake word detection.</p> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>class OpenWakeWord(BaseVoiceDetectionModel):\n    \"\"\"\n    A wake word detection model using the Open Wake Word framework.\n\n    This class loads a specified wake word model and detects whether a wake word is present\n    in the provided audio input.\n\n    Parameters\n    ----------\n    wake_word_model_path : str\n        Path to the wake word model file or name of a standard one.\n    threshold : float, optional\n        The confidence threshold for wake word detection. If a prediction surpasses this\n        value, the model will trigger a wake word detection. Default is 0.1.\n\n    Attributes\n    ----------\n    model_name : str\n        The name of the model, set to `\"open_wake_word\"`.\n    model : OWWModel\n        The Open Wake Word model instance used for inference.\n    threshold : float\n        The confidence threshold for determining wake word detection.\n    \"\"\"\n\n    def __init__(self, wake_word_model_path: str, threshold: float = 0.1):\n        \"\"\"\n        Initializes the OpenWakeWord detection model.\n\n        Parameters\n        ----------\n        wake_word_model_path : str\n            Path to the wake word model file.\n        threshold : float, optional\n            Confidence threshold for wake word detection. Default is 0.1.\n        \"\"\"\n        super(OpenWakeWord, self).__init__()\n        self.model_name = \"open_wake_word\"\n        download_models()\n        self.model = OWWModel(\n            wakeword_models=[\n                wake_word_model_path,\n            ],\n            inference_framework=\"onnx\",\n        )\n        self.threshold = threshold\n\n    def detect(\n        self, audio_data: NDArray, input_parameters: dict[str, Any]\n    ) -&gt; Tuple[bool, dict[str, Any]]:\n        \"\"\"\n        Detects whether a wake word is present in the given audio data.\n\n        This method runs inference on the provided audio data and determines whether\n        the detected confidence surpasses the threshold. If so, it resets the model\n        and returns `True`, indicating a wake word detection.\n\n        Parameters\n        ----------\n        audio_data : NDArray\n            A NumPy array representing the input audio data.\n        input_parameters : dict of str to Any\n            Additional input parameters to be included in the output.\n\n        Returns\n        -------\n        Tuple[bool, dict]\n            A tuple where the first value is a boolean indicating whether the wake word\n            was detected (`True` if detected, `False` otherwise). The second value is\n            a dictionary containing predictions and confidence values for them.\n\n        Raises\n        ------\n        Exception\n            If the predictions returned by the model are not in the expected dictionary format.\n        \"\"\"\n        predictions = self.model.predict(audio_data)\n        ret = input_parameters.copy()\n        ret.update({self.model_name: {\"predictions\": predictions}})\n        if not isinstance(predictions, dict):\n            raise Exception(\n                f\"Unexpected format from model predict {type(predictions)}:{predictions}\"\n            )\n        for _, value in predictions.items():  # type ignore\n            if value &gt; self.threshold:\n                self.model.reset()\n                return True, ret\n        return False, ret\n\n    def reset(self):\n        \"\"\"\n        Resets the wake word detection model.\n        \"\"\"\n        self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord.__init__","title":"<code>__init__(wake_word_model_path, threshold=0.1)</code>","text":"<p>Initializes the OpenWakeWord detection model.</p> <p>Parameters:</p> Name Type Description Default <code>wake_word_model_path</code> <code>str</code> <p>Path to the wake word model file.</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold for wake word detection. Default is 0.1.</p> <code>0.1</code> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>def __init__(self, wake_word_model_path: str, threshold: float = 0.1):\n    \"\"\"\n    Initializes the OpenWakeWord detection model.\n\n    Parameters\n    ----------\n    wake_word_model_path : str\n        Path to the wake word model file.\n    threshold : float, optional\n        Confidence threshold for wake word detection. Default is 0.1.\n    \"\"\"\n    super(OpenWakeWord, self).__init__()\n    self.model_name = \"open_wake_word\"\n    download_models()\n    self.model = OWWModel(\n        wakeword_models=[\n            wake_word_model_path,\n        ],\n        inference_framework=\"onnx\",\n    )\n    self.threshold = threshold\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord.detect","title":"<code>detect(audio_data, input_parameters)</code>","text":"<p>Detects whether a wake word is present in the given audio data.</p> <p>This method runs inference on the provided audio data and determines whether the detected confidence surpasses the threshold. If so, it resets the model and returns <code>True</code>, indicating a wake word detection.</p> <p>Parameters:</p> Name Type Description Default <code>audio_data</code> <code>NDArray</code> <p>A NumPy array representing the input audio data.</p> required <code>input_parameters</code> <code>dict of str to Any</code> <p>Additional input parameters to be included in the output.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, dict]</code> <p>A tuple where the first value is a boolean indicating whether the wake word was detected (<code>True</code> if detected, <code>False</code> otherwise). The second value is a dictionary containing predictions and confidence values for them.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the predictions returned by the model are not in the expected dictionary format.</p> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>def detect(\n    self, audio_data: NDArray, input_parameters: dict[str, Any]\n) -&gt; Tuple[bool, dict[str, Any]]:\n    \"\"\"\n    Detects whether a wake word is present in the given audio data.\n\n    This method runs inference on the provided audio data and determines whether\n    the detected confidence surpasses the threshold. If so, it resets the model\n    and returns `True`, indicating a wake word detection.\n\n    Parameters\n    ----------\n    audio_data : NDArray\n        A NumPy array representing the input audio data.\n    input_parameters : dict of str to Any\n        Additional input parameters to be included in the output.\n\n    Returns\n    -------\n    Tuple[bool, dict]\n        A tuple where the first value is a boolean indicating whether the wake word\n        was detected (`True` if detected, `False` otherwise). The second value is\n        a dictionary containing predictions and confidence values for them.\n\n    Raises\n    ------\n    Exception\n        If the predictions returned by the model are not in the expected dictionary format.\n    \"\"\"\n    predictions = self.model.predict(audio_data)\n    ret = input_parameters.copy()\n    ret.update({self.model_name: {\"predictions\": predictions}})\n    if not isinstance(predictions, dict):\n        raise Exception(\n            f\"Unexpected format from model predict {type(predictions)}:{predictions}\"\n        )\n    for _, value in predictions.items():  # type ignore\n        if value &gt; self.threshold:\n            self.model.reset()\n            return True, ret\n    return False, ret\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_wake_word.OpenWakeWord.reset","title":"<code>reset()</code>","text":"<p>Resets the wake word detection model.</p> Source code in <code>rai_s2s/asr/models/open_wake_word.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the wake word detection model.\n    \"\"\"\n    self.model.reset()\n</code></pre>"},{"location":"speech_to_speech/models/overview/#openaiwhisper","title":"OpenAIWhisper","text":"<ul> <li>Cloud-based transcription model: Documentation</li> <li>Requires setting the <code>OPEN_API_KEY</code> environment variable</li> <li>Offers language and model customization via the API</li> </ul> OpenAIWhisper"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.open_ai_whisper.OpenAIWhisper","title":"<code>rai_s2s.asr.models.open_ai_whisper.OpenAIWhisper</code>","text":"<p>               Bases: <code>BaseTranscriptionModel</code></p> Source code in <code>rai_s2s/asr/models/open_ai_whisper.py</code> <pre><code>class OpenAIWhisper(BaseTranscriptionModel):\n    def __init__(\n        self, model_name: str, sample_rate: int, language: str = \"en\", **kwargs\n    ):\n        super().__init__(model_name, sample_rate, language)\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n        self.api_key = api_key\n        self.openai_client = OpenAI()\n        self.model = partial(\n            self.openai_client.audio.transcriptions.create,\n            model=self.model_name,\n            **kwargs,\n        )\n        self.logger = logging.getLogger(__name__)\n        self.samples = []\n\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        normalized_data = data.astype(np.float32) / 32768.0\n        with io.BytesIO() as temp_wav_buffer:\n            wavfile.write(temp_wav_buffer, self.sample_rate, normalized_data)\n            temp_wav_buffer.seek(0)\n            temp_wav_buffer.name = \"temp.wav\"\n            response = self.model(file=temp_wav_buffer, language=self.language)\n        transcription = response.text\n        self.logger.info(\"transcription: %s\", transcription)\n        return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#localwhisper","title":"LocalWhisper","text":"<ul> <li>Local deployment of OpenAI Whisper: GitHub</li> <li>Supports GPU acceleration</li> <li>Same configuration interface as OpenAIWhisper</li> </ul> LocalWhisper"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.LocalWhisper","title":"<code>rai_s2s.asr.models.local_whisper.LocalWhisper</code>","text":"<p>               Bases: <code>BaseTranscriptionModel</code></p> <p>A transcription model using OpenAI's Whisper, running locally.</p> <p>This class loads a Whisper model and performs speech-to-text transcription on audio data. It supports GPU acceleration if available.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Whisper model to load.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the input audio, in Hz.</p> required <code>language</code> <code>str</code> <p>The language of the transcription output. Default is \"en\" (English).</p> <code>'en'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for loading the Whisper model.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>whisper</code> <code>Whisper</code> <p>The loaded Whisper model for transcription.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for logging transcription results.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>class LocalWhisper(BaseTranscriptionModel):\n    \"\"\"\n    A transcription model using OpenAI's Whisper, running locally.\n\n    This class loads a Whisper model and performs speech-to-text transcription\n    on audio data. It supports GPU acceleration if available.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the Whisper model to load.\n    sample_rate : int\n        The sample rate of the input audio, in Hz.\n    language : str, optional\n        The language of the transcription output. Default is \"en\" (English).\n    **kwargs : dict, optional\n        Additional keyword arguments for loading the Whisper model.\n\n    Attributes\n    ----------\n    whisper : whisper.Whisper\n        The loaded Whisper model for transcription.\n    logger : logging.Logger\n        Logger instance for logging transcription results.\n    \"\"\"\n\n    def __init__(\n        self, model_name: str, sample_rate: int, language: str = \"en\", **kwargs\n    ):\n        super().__init__(model_name, sample_rate, language)\n        self.decode_options = {\n            \"language\": language,  # Set language to English\n            \"task\": \"transcribe\",  # Set task to transcribe (not translate)\n            \"fp16\": False,  # Use FP32 instead of FP16 for better precision\n            \"without_timestamps\": True,  # Don't include timestamps in output\n            \"suppress_tokens\": [-1],  # Default tokens to suppress\n            \"suppress_blank\": True,  # Suppress blank outputs\n            \"beam_size\": 5,  # Beam size for beam search\n        }\n        if torch.cuda.is_available():\n            self.whisper = whisper.load_model(self.model_name, device=\"cuda\", **kwargs)\n        else:\n            self.whisper = whisper.load_model(self.model_name, **kwargs)\n\n        self.logger = logging.getLogger(__name__)\n\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        \"\"\"\n        Transcribes speech from the given audio data using Whisper.\n\n        This method normalizes the input audio, processes it using the Whisper model,\n        and returns the transcribed text.\n\n        Parameters\n        ----------\n        data : NDArray[np.int16]\n            A NumPy array containing the raw audio waveform data.\n\n        Returns\n        -------\n        str\n            The transcribed text from the audio input.\n        \"\"\"\n        normalized_data = data.astype(np.float32) / 32768.0\n\n        result = whisper.transcribe(\n            self.whisper, normalized_data, **self.decode_options\n        )\n        transcription = result[\"text\"]\n        self.logger.info(\"transcription: %s\", transcription)\n        transcription = cast(str, transcription)\n        self.latest_transcription = transcription\n        return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.LocalWhisper.transcribe","title":"<code>transcribe(data)</code>","text":"<p>Transcribes speech from the given audio data using Whisper.</p> <p>This method normalizes the input audio, processes it using the Whisper model, and returns the transcribed text.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[int16]</code> <p>A NumPy array containing the raw audio waveform data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transcribed text from the audio input.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n    \"\"\"\n    Transcribes speech from the given audio data using Whisper.\n\n    This method normalizes the input audio, processes it using the Whisper model,\n    and returns the transcribed text.\n\n    Parameters\n    ----------\n    data : NDArray[np.int16]\n        A NumPy array containing the raw audio waveform data.\n\n    Returns\n    -------\n    str\n        The transcribed text from the audio input.\n    \"\"\"\n    normalized_data = data.astype(np.float32) / 32768.0\n\n    result = whisper.transcribe(\n        self.whisper, normalized_data, **self.decode_options\n    )\n    transcription = result[\"text\"]\n    self.logger.info(\"transcription: %s\", transcription)\n    transcription = cast(str, transcription)\n    self.latest_transcription = transcription\n    return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#fasterwhisper","title":"FasterWhisper","text":"<ul> <li>Optimized Whisper variant: GitHub</li> <li>Designed for high speed and low memory usage</li> <li>Follows the same API as Whisper models</li> </ul> FasterWhisper"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.FasterWhisper","title":"<code>rai_s2s.asr.models.local_whisper.FasterWhisper</code>","text":"<p>               Bases: <code>BaseTranscriptionModel</code></p> <p>A transcription model using Faster Whisper for efficient speech-to-text conversion.</p> <p>This class loads a Faster Whisper model, optimized for speed and efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the Faster Whisper model to load.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the input audio, in Hz.</p> required <code>language</code> <code>str</code> <p>The language of the transcription output. Default is \"en\" (English).</p> <code>'en'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for loading the Faster Whisper model.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>WhisperModel</code> <p>The loaded Faster Whisper model instance.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for logging transcription results.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>class FasterWhisper(BaseTranscriptionModel):\n    \"\"\"\n    A transcription model using Faster Whisper for efficient speech-to-text conversion.\n\n    This class loads a Faster Whisper model, optimized for speed and efficiency.\n\n    Parameters\n    ----------\n    model_name : str\n        The name of the Faster Whisper model to load.\n    sample_rate : int\n        The sample rate of the input audio, in Hz.\n    language : str, optional\n        The language of the transcription output. Default is \"en\" (English).\n    **kwargs : dict, optional\n        Additional keyword arguments for loading the Faster Whisper model.\n\n    Attributes\n    ----------\n    model : WhisperModel\n        The loaded Faster Whisper model instance.\n    logger : logging.Logger\n        Logger instance for logging transcription results.\n    \"\"\"\n\n    def __init__(\n        self, model_name: str, sample_rate: int, language: str = \"en\", **kwargs\n    ):\n        super().__init__(model_name, sample_rate, language)\n        self.model = WhisperModel(model_name, **kwargs)\n        self.logger = logging.getLogger(__name__)\n\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        \"\"\"\n        Transcribes speech from the given audio data using Faster Whisper.\n\n        This method normalizes the input audio, processes it using the Faster Whisper model,\n        and returns the transcribed text.\n\n        Parameters\n        ----------\n        data : NDArray[np.int16]\n            A NumPy array containing the raw audio waveform data.\n\n        Returns\n        -------\n        str\n            The transcribed text from the audio input.\n        \"\"\"\n        normalized_data = data.astype(np.float32) / 32768.0\n        segments, _ = self.model.transcribe(normalized_data)\n        transcription = \" \".join(segment.text for segment in segments)\n        self.logger.info(\"transcription: %s\", transcription)\n        return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.asr.models.local_whisper.FasterWhisper.transcribe","title":"<code>transcribe(data)</code>","text":"<p>Transcribes speech from the given audio data using Faster Whisper.</p> <p>This method normalizes the input audio, processes it using the Faster Whisper model, and returns the transcribed text.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[int16]</code> <p>A NumPy array containing the raw audio waveform data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The transcribed text from the audio input.</p> Source code in <code>rai_s2s/asr/models/local_whisper.py</code> <pre><code>def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n    \"\"\"\n    Transcribes speech from the given audio data using Faster Whisper.\n\n    This method normalizes the input audio, processes it using the Faster Whisper model,\n    and returns the transcribed text.\n\n    Parameters\n    ----------\n    data : NDArray[np.int16]\n        A NumPy array containing the raw audio waveform data.\n\n    Returns\n    -------\n    str\n        The transcribed text from the audio input.\n    \"\"\"\n    normalized_data = data.astype(np.float32) / 32768.0\n    segments, _ = self.model.transcribe(normalized_data)\n    transcription = \" \".join(segment.text for segment in segments)\n    self.logger.info(\"transcription: %s\", transcription)\n    return transcription\n</code></pre>"},{"location":"speech_to_speech/models/overview/#elevenlabs","title":"ElevenLabs","text":"<ul> <li>Cloud-based TTS model: Website</li> <li>Requires the environment variable <code>ELEVENLABS_API_KEY</code> with a valid key</li> </ul> ElevenLabs"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS","title":"<code>rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS</code>","text":"<p>               Bases: <code>TTSModel</code></p> <p>A text-to-speech (TTS) model interface for ElevenLabs.</p> <p>Parameters:</p> Name Type Description Default <code>voice</code> <code>str</code> <p>The voice model to use.</p> required <code>base_url</code> <code>str</code> <p>The API endpoint for the ElevenLabs API, by default None.</p> <code>None</code> Source code in <code>rai_s2s/tts/models/elevenlabs_tts.py</code> <pre><code>class ElevenLabsTTS(TTSModel):\n    \"\"\"\n    A text-to-speech (TTS) model interface for ElevenLabs.\n\n    Parameters\n    ----------\n    voice : str, optional\n        The voice model to use.\n    base_url : str, optional\n        The API endpoint for the ElevenLabs API, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        voice: str,\n        base_url: str | None = None,\n    ):\n        api_key = os.getenv(key=\"ELEVENLABS_API_KEY\")\n        if api_key is None:\n            raise TTSModelError(\"ELEVENLABS_API_KEY environment variable is not set.\")\n\n        self.client = ElevenLabs(base_url=base_url, api_key=api_key)\n        self.voice_settings = VoiceSettings(\n            stability=0.7,\n            similarity_boost=0.5,\n        )\n\n        voices = self.client.voices.get_all().voices\n        voice_id = next((v.voice_id for v in voices if v.name == voice), None)\n        if voice_id is None:\n            raise TTSModelError(f\"Voice {voice} not found\")\n        self.voice = Voice(voice_id=voice_id, settings=self.voice_settings)\n\n    def get_speech(self, text: str) -&gt; AudioSegment:\n        \"\"\"\n        Converts text into speech using the ElevenLabs API.\n\n        Parameters\n        ----------\n        text : str\n            The input text to be converted into speech.\n\n        Returns\n        -------\n        AudioSegment\n            The generated speech as an `AudioSegment` object.\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the ElevenLabs API is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n        try:\n            response = self.client.generate(\n                text=text,\n                voice=self.voice,\n                optimize_streaming_latency=4,\n            )\n            audio_data = b\"\".join(response)\n        except Exception as e:\n            raise TTSModelError(f\"Error occurred while fetching audio: {e}\") from e\n\n        # Load audio into memory (ElevenLabs returns MP3)\n        audio_segment = AudioSegment.from_mp3(BytesIO(audio_data))\n        return audio_segment\n\n    def get_tts_params(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Returns TTS sampling rate and channels.\n\n        The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n        Returns\n        -------\n        Tuple[int, int]\n            sample rate, channels\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the ElevenLabs API is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n        data = self.get_speech(\"A\")\n        return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS.get_speech","title":"<code>get_speech(text)</code>","text":"<p>Converts text into speech using the ElevenLabs API.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be converted into speech.</p> required <p>Returns:</p> Type Description <code>AudioSegment</code> <p>The generated speech as an <code>AudioSegment</code> object.</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the ElevenLabs API is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/elevenlabs_tts.py</code> <pre><code>def get_speech(self, text: str) -&gt; AudioSegment:\n    \"\"\"\n    Converts text into speech using the ElevenLabs API.\n\n    Parameters\n    ----------\n    text : str\n        The input text to be converted into speech.\n\n    Returns\n    -------\n    AudioSegment\n        The generated speech as an `AudioSegment` object.\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the ElevenLabs API is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n    try:\n        response = self.client.generate(\n            text=text,\n            voice=self.voice,\n            optimize_streaming_latency=4,\n        )\n        audio_data = b\"\".join(response)\n    except Exception as e:\n        raise TTSModelError(f\"Error occurred while fetching audio: {e}\") from e\n\n    # Load audio into memory (ElevenLabs returns MP3)\n    audio_segment = AudioSegment.from_mp3(BytesIO(audio_data))\n    return audio_segment\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.elevenlabs_tts.ElevenLabsTTS.get_tts_params","title":"<code>get_tts_params()</code>","text":"<p>Returns TTS sampling rate and channels.</p> <p>The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>sample rate, channels</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the ElevenLabs API is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/elevenlabs_tts.py</code> <pre><code>def get_tts_params(self) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns TTS sampling rate and channels.\n\n    The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n    Returns\n    -------\n    Tuple[int, int]\n        sample rate, channels\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the ElevenLabs API is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n    data = self.get_speech(\"A\")\n    return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#opentts","title":"OpenTTS","text":"<ul> <li>Open source TTS solution: GitHub</li> <li>Easy setup via Docker:</li> </ul> <pre><code> docker run -it -p 5500:5500 synesthesiam/opentts:en --no-espeak\n</code></pre> <ul> <li>Provides a TTS server running on port 5500</li> <li>Supports multiple voices and configurations</li> </ul> OpenTTS"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.open_tts.OpenTTS","title":"<code>rai_s2s.tts.models.open_tts.OpenTTS</code>","text":"<p>               Bases: <code>TTSModel</code></p> <p>A text-to-speech (TTS) model interface for OpenTTS.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint for the OpenTTS server, by default \"http://localhost:5500/api/tts\".</p> <code>'http://localhost:5500/api/tts'</code> <code>voice</code> <code>str</code> <p>The voice model to use, by default \"larynx:blizzard_lessac-glow_tts\".</p> <code>'larynx:blizzard_lessac-glow_tts'</code> Source code in <code>rai_s2s/tts/models/open_tts.py</code> <pre><code>class OpenTTS(TTSModel):\n    \"\"\"\n    A text-to-speech (TTS) model interface for OpenTTS.\n\n    Parameters\n    ----------\n    url : str, optional\n        The API endpoint for the OpenTTS server, by default \"http://localhost:5500/api/tts\".\n    voice : str, optional\n        The voice model to use, by default \"larynx:blizzard_lessac-glow_tts\".\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str = \"http://localhost:5500/api/tts\",\n        voice: str = \"larynx:blizzard_lessac-glow_tts\",\n    ):\n        self.url = url\n        self.voice = voice\n\n    def get_speech(self, text: str) -&gt; AudioSegment:\n        \"\"\"\n        Converts text into speech using the OpenTTS API.\n\n        Parameters\n        ----------\n        text : str\n            The input text to be converted into speech.\n\n        Returns\n        -------\n        AudioSegment\n            The generated speech as an `AudioSegment` object.\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the OpenTTS server is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n        params = {\n            \"voice\": self.voice,\n            \"text\": text,\n        }\n        try:\n            response = requests.get(self.url, params=params)\n        except requests.exceptions.RequestException as e:\n            raise TTSModelError(\n                f\"Error occurred while fetching audio: {e}, check if OpenTTS server is running correctly.\"\n            ) from e\n\n        content_type = response.headers.get(\"Content-Type\", \"\")\n\n        if \"audio\" not in content_type:\n            raise TTSModelError(\"Response does not contain audio data\")\n\n        # Load audio into memory\n        audio_bytes = BytesIO(response.content)\n        sample_rate, data = read(audio_bytes)\n        if data.dtype == np.int32:\n            data = (data / 2**16).astype(np.int16)  # Scale down from int32\n        elif data.dtype == np.uint8:\n            data = (data - 128).astype(np.int16) * 256  # Convert uint8 to int16\n        elif data.dtype == np.float32:\n            data = (\n                (data * 32768).clip(-32768, 32767).astype(np.int16)\n            )  # Convert float32 to int16\n\n        audio = AudioSegment(\n            data.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1\n        )\n        if self.sample_rate == -1:\n            return audio\n        else:\n            return self._resample(audio)\n\n    def get_tts_params(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Returns TTS samling rate and channels.\n\n        The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n        Returns\n        -------\n        Tuple[int, int]\n            sample rate, channels\n\n        Raises\n        ------\n        TTSModelError\n            If there is an issue with the request or the OpenTTS server is unreachable.\n            If the response does not contain valid audio data.\n        \"\"\"\n\n        data = self.get_speech(\"A\")\n        return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.open_tts.OpenTTS.get_speech","title":"<code>get_speech(text)</code>","text":"<p>Converts text into speech using the OpenTTS API.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be converted into speech.</p> required <p>Returns:</p> Type Description <code>AudioSegment</code> <p>The generated speech as an <code>AudioSegment</code> object.</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the OpenTTS server is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/open_tts.py</code> <pre><code>def get_speech(self, text: str) -&gt; AudioSegment:\n    \"\"\"\n    Converts text into speech using the OpenTTS API.\n\n    Parameters\n    ----------\n    text : str\n        The input text to be converted into speech.\n\n    Returns\n    -------\n    AudioSegment\n        The generated speech as an `AudioSegment` object.\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the OpenTTS server is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n    params = {\n        \"voice\": self.voice,\n        \"text\": text,\n    }\n    try:\n        response = requests.get(self.url, params=params)\n    except requests.exceptions.RequestException as e:\n        raise TTSModelError(\n            f\"Error occurred while fetching audio: {e}, check if OpenTTS server is running correctly.\"\n        ) from e\n\n    content_type = response.headers.get(\"Content-Type\", \"\")\n\n    if \"audio\" not in content_type:\n        raise TTSModelError(\"Response does not contain audio data\")\n\n    # Load audio into memory\n    audio_bytes = BytesIO(response.content)\n    sample_rate, data = read(audio_bytes)\n    if data.dtype == np.int32:\n        data = (data / 2**16).astype(np.int16)  # Scale down from int32\n    elif data.dtype == np.uint8:\n        data = (data - 128).astype(np.int16) * 256  # Convert uint8 to int16\n    elif data.dtype == np.float32:\n        data = (\n            (data * 32768).clip(-32768, 32767).astype(np.int16)\n        )  # Convert float32 to int16\n\n    audio = AudioSegment(\n        data.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1\n    )\n    if self.sample_rate == -1:\n        return audio\n    else:\n        return self._resample(audio)\n</code></pre>"},{"location":"speech_to_speech/models/overview/#rai_s2s.tts.models.open_tts.OpenTTS.get_tts_params","title":"<code>get_tts_params()</code>","text":"<p>Returns TTS samling rate and channels.</p> <p>The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>sample rate, channels</p> <p>Raises:</p> Type Description <code>TTSModelError</code> <p>If there is an issue with the request or the OpenTTS server is unreachable. If the response does not contain valid audio data.</p> Source code in <code>rai_s2s/tts/models/open_tts.py</code> <pre><code>def get_tts_params(self) -&gt; Tuple[int, int]:\n    \"\"\"\n    Returns TTS samling rate and channels.\n\n    The information is retrieved by running a sample transcription request, to ensure that the information will be accurate for generation.\n\n    Returns\n    -------\n    Tuple[int, int]\n        sample rate, channels\n\n    Raises\n    ------\n    TTSModelError\n        If there is an issue with the request or the OpenTTS server is unreachable.\n        If the response does not contain valid audio data.\n    \"\"\"\n\n    data = self.get_speech(\"A\")\n    return data.frame_rate, 1\n</code></pre>"},{"location":"speech_to_speech/models/overview/#custom-models","title":"Custom Models","text":""},{"location":"speech_to_speech/models/overview/#voice-detection-models","title":"Voice Detection Models","text":"<p>To implement a custom VAD or Wake Word model, inherit from <code>rai_asr.base.BaseVoiceDetectionModel</code> and implement the following methods:</p> <pre><code>class MyDetectionModel(BaseVoiceDetectionModel):\n    def detect(self, audio_data: NDArray, input_parameters: dict[str, Any]) -&gt; Tuple[bool, dict[str, Any]]:\n        ...\n\n    def reset(self):\n        ...\n</code></pre>"},{"location":"speech_to_speech/models/overview/#transcription-models","title":"Transcription Models","text":"<p>To implement a custom transcription model, inherit from <code>rai_asr.base.BaseTranscriptionModel</code> and implement:</p> <pre><code>class MyTranscriptionModel(BaseTranscriptionModel):\n    def transcribe(self, data: NDArray[np.int16]) -&gt; str:\n        ...\n</code></pre>"},{"location":"speech_to_speech/models/overview/#tts-models","title":"TTS Models","text":"<p>To create a custom TTS model, inherit from <code>rai_tts.models.base.TTSModel</code> and implement the required interface:</p> <pre><code>class MyTTSModel(TTSModel):\n    def get_speech(self, text: str) -&gt; AudioSegment:\n        ...\n        return AudioSegment()\n\n    def get_tts_params(self) -&gt; Tuple[int, int]:\n        ...\n        return sample_rate, channels\n</code></pre>"},{"location":"tutorials/benchmarking/","title":"Benchmarking","text":"<p>Note</p> <p>If you aren't familiar with our benchmark package, please read RAI Bench first.</p> <p>Currently, we offer 2 predefined benchmarks:</p> <ul> <li>Manipulation_O3DE</li> <li>Tool_Calling_Agent</li> </ul> <p>If you want to test multiple models across different benchmark configurations, go to Testing Models.</p> <p>If your goal is creating custom tasks and scenarios, visit Creating Custom Tasks.</p>"},{"location":"tutorials/benchmarking/#manipulation-o3de","title":"Manipulation O3DE","text":"<ul> <li>Follow the main setup Basic Setup and setup from Manipulation demo Setup</li> <li>To see available options run:     <pre><code>python src/rai_bench/rai_bench/examples/manipulation_o3de.py --help\n</code></pre></li> <li> <p>Example usage:</p> <pre><code>python src/rai_bench/rai_bench/examples/manipulation_o3de.py --model-name qwen2.5:7b --vendor ollama --levels trivial\n</code></pre> <p>Note</p> <p>When using Ollama, be sure to pull the model first.</p> <p>Warning</p> <p>Running all scenarios will take a while. If you want to just try it out, we recommend choosing just one level of difficulty.</p> </li> </ul>"},{"location":"tutorials/benchmarking/#tool-calling-agent","title":"Tool Calling Agent","text":"<ul> <li>This benchmark does not require any additional setup besides the main one Basic Setup</li> <li>To see available options run:     <pre><code>python src/rai_bench/rai_bench/examples/tool_calling_agent.py --help\n</code></pre></li> <li>Example usage:</li> </ul> <pre><code>python src/rai_bench/rai_bench/examples/tool_calling_agent.py --model-name qwen2.5:7b --vendor ollama --extra-tool-calls 5 --task-types basic  --n-shots 5 --prompt-detail descriptive --complexities easy\n</code></pre>"},{"location":"tutorials/benchmarking/#testing-models","title":"Testing Models","text":"<p>The best way of benchmarking your models is using the <code>src/rai_bench/rai_bench/examples/benchmarking_models.py</code></p> <p>Feel free to modify the benchmark configs to suit your needs, you can choose every possible set of params and the benchmark will be run tasks with every combination:</p> <pre><code>if __name__ == \"__main__\":\n    # Define models you want to benchmark\n    model_names = [\"qwen3:4b\", \"llama3.2:3b\"]\n    vendors = [\"ollama\", \"ollama\"]\n\n    # Define benchmarks that will be used\n    mani_conf = ManipulationO3DEBenchmarkConfig(\n        o3de_config_path=\"src/rai_bench/rai_bench/manipulation_o3de/predefined/configs/o3de_config.yaml\",\n        levels=[  # define what difficulty of tasks to include in benchmark\n            \"trivial\",\n            \"easy\",\n        ],\n        repeats=1,  # how many times to repeat\n    )\n    tool_conf = ToolCallingAgentBenchmarkConfig(\n        extra_tool_calls=[0, 5],  # how many extra tool calls allowed to still pass\n        task_types=[  # what types of tasks to include\n            \"basic\",\n            \"custom_interfaces\",\n        ],\n        N_shots=[0, 2],  # examples in system prompt\n        prompt_detail=[\"brief\", \"descriptive\"],  # how descriptive should task prompt be\n        repeats=1,\n    )\n\n    out_dir = \"src/rai_bench/rai_bench/experiments\"\n    test_models(\n        model_names=model_names,\n        vendors=vendors,\n        benchmark_configs=[mani_conf, tool_conf],\n        out_dir=out_dir,\n        # if you want to pass any additinal args to model\n        additional_model_args=[\n            {\"reasoning\": False},\n            {},\n        ],\n    )\n</code></pre> <p>Based on the example above the <code>Tool Calling</code> benchmark will run basic and custom_interfaces tasks with every configuration of [extra_tool_calls x N_shots x prompt_detail] provided which will result in almost 500 tasks. Manipulation benchmark will run all specified task level once as there is no additional params. Reapeat is set to 1 in both configs so there will be no additional runs.</p> <p>Note</p> <p>When using ollama vendor make sure to pull used models first</p>"},{"location":"tutorials/benchmarking/#viewing-results","title":"Viewing Results","text":"<p>From every benchmark run, there will be results saved in the provided output directory:</p> <ul> <li>Logs - in <code>benchmark.log</code> file</li> <li>results_summary.csv - for overall metrics</li> <li>results.csv - for detailed results of every task/scenario</li> </ul> <p>When using <code>test_models</code>, the output directories will be saved as <code>&lt;run_datetime&gt;/&lt;benchmark_name&gt;/&lt;model&gt;/&lt;repeat&gt;/...</code> and this format can be visualized with our Streamlit script:</p> <pre><code>streamlit run src/rai_bench/rai_bench/examples/visualise_streamlit.py\n</code></pre>"},{"location":"tutorials/benchmarking/#creating-custom-tasks","title":"Creating Custom Tasks","text":""},{"location":"tutorials/benchmarking/#manipulation-o3de-scenarios","title":"Manipulation O3DE Scenarios","text":"<p>To create your own Scenarios, you will need a Scene Config and Task - check out example <code>src/rai_bench/rai_bench/examples/custom_scenario.py</code>. You can combine already existing Scene and existing Task to create a new Scenario like:</p> <pre><code>import logging\nfrom pathlib import Path\nfrom typing import List, Sequence, Tuple, Union\n\nfrom rclpy.impl.rcutils_logger import RcutilsLogger\n\nfrom rai_bench.manipulation_o3de.benchmark import Scenario\nfrom rai_bench.manipulation_o3de.interfaces import (\n    ManipulationTask,\n)\nfrom rai_bench.manipulation_o3de.tasks import PlaceObjectAtCoordTask\nfrom rai_sim.simulation_bridge import Entity, SceneConfig\n\nloggers_type = Union[RcutilsLogger, logging.Logger]\n\n### Define your scene setup ####################3\npath_to_your_config = (\n    \"src/rai_bench/rai_bench/manipulation_o3de/predefined/configs/1a.yaml\"\n)\nscene_config = SceneConfig.load_base_config(Path(path_to_your_config))\n\n# configure existing Task with different params\ntarget_coords = (0.1, 0.1)\ndisp = 0.1\ntask = PlaceObjectAtCoordTask(\n    obj_type=\"apple\",\n    target_position=target_coords,\n    allowable_displacement=disp,\n)\n\nScenario(task=task, scene_config=scene_config, scene_config_path=path_to_your_config)\n</code></pre> <p>But you can also create them from scratch. Creating a Scene Config is very easy, just declare entities in a YAML file like:</p> <pre><code>entities:\n  - name: apple1\n    prefab_name: apple # make sure that this prefab exists in simulation\n      pose:\n          translation:\n              x: 0.0\n              y: 0.5\n              z: 0.05\n          rotation:\n              x: 0.0\n              y: 0.0\n              z: 0.0\n              w: 1.0\n</code></pre> <p>Creating your own Task will require slightly more effort. Let's start with something simple - a Task that will require throwing given objects off the table:</p> <pre><code>class ThrowObjectsOffTableTask(ManipulationTask):\n    def __init__(self, obj_types: List[str], logger: loggers_type | None = None):\n        super().__init__(logger=logger)\n        # obj_types is a list of objects that are subject of the task\n        # In this case, it will mean which objects should be thrown off the table\n        # can be any objects\n        self.obj_types = obj_types\n\n    @property\n    def task_prompt(self) -&gt; str:\n        # define prompt\n        obj_names = \", \".join(obj + \"s\" for obj in self.obj_types).replace(\"_\", \" \")\n        # 0.0 z is the level of table, so any coord below that means it is off the table\n        return f\"Manipulate objects, so that all of the {obj_names} are dropped outside of the table (for example y&lt;-0.75).\"\n\n    def check_if_required_objects_present(self, simulation_config: SceneConfig) -&gt; bool:\n        # Validate if any required objects are present in sim config\n        # if there is not a single object of provided type, there is no point in running\n        # this task of given scene config\n        count = sum(\n            1 for ent in simulation_config.entities if ent.prefab_name in self.obj_types\n        )\n        return count &gt; 1\n\n    def calculate_correct(self, entities: Sequence[Entity]) -&gt; Tuple[int, int]:\n        selected_type_objects = self.filter_entities_by_object_type(\n            entities=entities, object_types=self.obj_types\n        )\n\n        # check how many objects are below table, that will be our metric\n        correct = sum(\n            1 for ent in selected_type_objects if ent.pose.pose.position.z &lt; 0.0\n        )\n\n        incorrect: int = len(selected_type_objects) - correct\n        return correct, incorrect\n\n\n# configure existing Task with different params\ntarget_coords = (0.1, 0.1)\ndisp = 0.1\ntask = ThrowObjectsOffTableTask(\n    obj_types=[\"apple\"],\n)\n\nsuper_scenario = Scenario(\n    task=task, scene_config=scene_config, scene_config_path=path_to_your_config\n)\n</code></pre> <p>As <code>obj_types</code> is parameterizable, it enables various variants of this Task. In combination with a lot of simulation configs available, it means that a single Task can provide dozens of scenarios.</p> <p>Then yo test it simply run:</p> <pre><code>##### Now you can run it in benchmark ##################\nif __name__ == \"__main__\":\n    from pathlib import Path\n\n    from rai_bench import (\n        define_benchmark_logger,\n    )\n    from rai_bench.manipulation_o3de import run_benchmark\n    from rai_bench.utils import get_llm_for_benchmark\n\n    experiment_dir = Path(out_dir=\"src/rai_bench/experiments/custom_task/\")\n\n    experiment_dir.mkdir(parents=True, exist_ok=True)\n    bench_logger = define_benchmark_logger(out_dir=experiment_dir)\n\n    llm = get_llm_for_benchmark(\n        model_name=\"gpt-4o\",\n        vendor=\"openai\",\n    )\n\n    run_benchmark(\n        llm=llm,\n        out_dir=experiment_dir,\n        # use your scenario\n        scenarios=[super_scenario],\n        bench_logger=bench_logger,\n    )\n</code></pre> <p>Congratulations, you just created and launched your first Scenario from scratch!</p>"},{"location":"tutorials/benchmarking/#tool-calling-tasks","title":"Tool Calling Tasks","text":"<p>To create a Tool Calling Task, you will need to define Subtasks, Validators, and Task itself. Check the example <code>src/rai_bench/rai_bench/examples/custom_task.py</code>. Let's create a basic task that requires using a tool to receive a message from a specific topic.</p> <pre><code>from typing import List\n\nfrom langchain_core.tools import BaseTool\n\nfrom rai_bench.tool_calling_agent.interfaces import Task, TaskArgs\nfrom rai_bench.tool_calling_agent.mocked_tools import (\n    MockGetROS2TopicsNamesAndTypesTool,\n    MockReceiveROS2MessageTool,\n)\nfrom rai_bench.tool_calling_agent.subtasks import (\n    CheckArgsToolCallSubTask,\n)\nfrom rai_bench.tool_calling_agent.validators import (\n    OrderedCallsValidator,\n)\n\n\n# This Task will check if robot can receive msessage from specified topic\nclass GetROS2RobotPositionTask(Task):\n    complexity = \"easy\"\n    type = \"custom\"\n\n    @property\n    def available_tools(self) -&gt; List[BaseTool]:\n        # define topics that will be seen by agent\n        TOPICS = [\n            \"/robot_position\",\n            \"/attached_collision_object\",\n            \"/clock\",\n            \"/collision_object\",\n        ]\n\n        TOPICS_STRING = [\n            \"topic: /attached_collision_object\\ntype: moveit_msgs/msg/AttachedCollisionObject\\n\",\n            \"topic: /clock\\ntype: rosgraph_msgs/msg/Clock\\n\",\n            \"topic: /collision_object\\ntype: moveit_msgs/msg/CollisionObject\\n\",\n            \"topic: /robot_position\\n type: sensor_msgs/msg/RobotPosition\",\n        ]\n        # define which tools will be available for agent\n        return [\n            MockGetROS2TopicsNamesAndTypesTool(\n                mock_topics_names_and_types=TOPICS_STRING\n            ),\n            MockReceiveROS2MessageTool(available_topics=TOPICS),\n        ]\n\n    def get_system_prompt(self) -&gt; str:\n        return \"You are a ROS 2 expert that want to solve tasks. You have access to various tools that allow you to query the ROS 2 system.\"\n\n    def get_base_prompt(self) -&gt; str:\n        return \"Get the position of the robot.\"\n\n    def get_prompt(self) -&gt; str:\n        # Create versions for different levels\n        if self.prompt_detail == \"brief\":\n            return self.get_base_prompt()\n        else:\n            return (\n                f\"{self.get_base_prompt()} \"\n                \"You can discover what topics are currently active.\"\n            )\n\n    @property\n    def optional_tool_calls_number(self) -&gt; int:\n        # Listing topics before getting any message is fine\n        return 1\n\n\n# define subtask\nreceive_robot_pos_subtask = CheckArgsToolCallSubTask(\n    expected_tool_name=\"receive_ros2_message\",\n    expected_args={\"topic\": \"/robot_position\"},\n    expected_optional_args={\n        \"timeout_sec\": int  # if there is not exact value expected, you can pass type\n    },\n)\n# use OrderedCallValidator as there is only 1 subtask to check\ntopics_ord_val = OrderedCallsValidator(subtasks=[receive_robot_pos_subtask])\n\n\n# optionally pass number of extra tool calls\nargs = TaskArgs(extra_tool_calls=0)\nsuper_task = GetROS2RobotPositionTask(validators=[topics_ord_val], task_args=args)\n</code></pre> <p>Then run it with:</p> <pre><code>##### Now you can run it in benchmark ##################\nif __name__ == \"__main__\":\n    from pathlib import Path\n\n    from rai_bench import (\n        define_benchmark_logger,\n    )\n    from rai_bench.tool_calling_agent import (\n        run_benchmark,\n    )\n    from rai_bench.utils import get_llm_for_benchmark\n\n    experiment_dir = Path(\"src/rai_bench/rai_bench/experiments/custom_task\")\n    experiment_dir.mkdir(parents=True, exist_ok=True)\n    bench_logger = define_benchmark_logger(out_dir=experiment_dir)\n\n    super_task.set_logger(bench_logger)\n\n    llm = get_llm_for_benchmark(\n        model_name=\"gpt-4o\",\n        vendor=\"openai\",\n    )\n\n    run_benchmark(\n        llm=llm,\n        out_dir=experiment_dir,\n        tasks=[super_task],\n        bench_logger=bench_logger,\n    )\n</code></pre>"},{"location":"tutorials/create_robots_whoami/","title":"Robot's identity within RAI","text":"<p>RAI Agent needs to understand what kind of robot it is running on. This includes its looks, purpose, ethical code, equipment, capabilities and documentation. To configure RAI for your robot, provide contents for your robot's so called <code>whoami</code> package.</p>"},{"location":"tutorials/create_robots_whoami/#configuration-example-franka-emika-panda-arm","title":"Configuration example - Franka Emika Panda arm","text":"<ol> <li> <p>Setup the repository using quick setup guide</p> </li> <li> <p>Fill in the <code>panda/</code> folder with data:</p> <p>2.1. Save this image into <code>panda/images</code></p> <p>2.2. Save this document in <code>panda/documentation</code></p> <p>2.3. Save this urdf in <code>panda/urdf</code></p> </li> <li> <p>Build the embodiment info using <code>build_whoami.py</code>:</p> <pre><code>python src/rai_whoami/rai_whoami/build_whoami.py panda/ --build-vector-db\n</code></pre> <p>Vector database</p> <p>Building the vector database with cloud vendors might lead to costs. Consider using the local <code>ollama</code> provider for this task. The embedding model can be configured in <code>config.toml</code> (<code>ollama</code> works locally, see docs/setup/vendors.md).</p> </li> <li> <p>Examine the generated files</p> </li> </ol> <p>After running the build command, inspect the generated files in the <code>panda/generated</code> directory. The folder should contain a info.json file containing:</p> <ul> <li><code>rules</code>: List of rules</li> <li><code>capabilities</code>: List of capabilities</li> <li><code>behaviors</code>: List of behaviors</li> <li><code>description</code>: Description of the robot</li> <li><code>images</code>: Base64 encoded images</li> </ul>"},{"location":"tutorials/create_robots_whoami/#testing-ros-2","title":"Testing (ROS 2)","text":"<p>You can test the generated package by using the RAI Whoami services:</p> <ol> <li>Using the RAI Whoami services:</li> </ol> <p>Run the RAI Whoami services:</p> <pre><code>python src/rai_whoami/rai_whoami/scripts/ros2_embodiment_service.py panda/ &amp;\npython src/rai_whoami/rai_whoami/scripts/ros2_vector_store_retrieval_service.py panda/\n</code></pre> <p>With the services running, you can query the robot's identity and vector database:</p> <pre><code># Get robot's identity\nros2 service call /rai_whoami_embodiment_info_service rai_interfaces/srv/EmbodimentInfo\n\n# Query the vector database\nros2 service call /rai_whoami_documentation_service rai_interfaces/srv/VectorStoreRetrieval \"query: 'maximum load'\"\n</code></pre> <p>If your service calls succeed and you can access the embodiment info and vector database, your robot's whoami package has been properly initialized.</p> <ol> <li>Alternatively, you can use the RAI Whoami tools directly in your Python code:</li> </ol> <pre><code>from rai_whoami import EmbodimentInfo\nfrom rai_whoami.tools import QueryDatabaseTool\n\n# Load embodiment info\ninfo = EmbodimentInfo.from_directory(\"panda/generated\")\n\n# Create a system prompt for your LLM\nsystem_prompt = info.to_langchain()\n\n# Use the vector database tool\nquery_tool = QueryDatabaseTool(root_dir=\"panda/generated\")\nquery_tool._run(query=\"maximum load\")\n</code></pre>"},{"location":"tutorials/overview/","title":"RAI Tutorials Overview","text":"<p>This directory contains a collection of tutorials that guide you through various aspects of the RAI (Robot AI) framework. Each tutorial focuses on different components and use cases of the system.</p>"},{"location":"tutorials/overview/#available-tutorials","title":"Available Tutorials","text":""},{"location":"tutorials/overview/#1-walkthrough","title":"1. Walkthrough","text":"<p>A step-by-step guide to creating and deploying a custom RAI agent on a ROS 2 enabled robot. This tutorial covers:</p> <ul> <li>Creating a custom RAI Agent from scratch</li> <li>Implementing platform-specific tools for robot control</li> <li>Building an optimized system prompt using rai whoami</li> <li>Deploying and interacting with the agent</li> </ul>"},{"location":"tutorials/overview/#2-create-robots-whoami","title":"2. Create Robot's Whoami","text":"<p>Learn how to configure RAI to understand your robot's identity, including its appearance, purpose, ethical code, equipment, capabilities, and documentation. This tutorial covers:</p> <ul> <li>Setting up the robot's <code>whoami</code> package</li> <li>Building embodiment information</li> <li>Testing the configuration with ROS 2 services</li> <li>Using the whoami tools in Python code</li> </ul>"},{"location":"tutorials/overview/#3-tools","title":"3. Tools","text":"<p>A comprehensive guide to tool development and usage in RAI. This tutorial covers:</p> <ul> <li>Understanding the fundamental concepts of tools in LangChain</li> <li>Creating custom tools using both <code>BaseTool</code> class and <code>@tool</code> decorator</li> <li>Implementing single-modal and multimodal tools</li> <li>Developing ROS 2 specific tools</li> <li>Tool initialization and configuration</li> <li>Using tools in both distributed and local setups</li> </ul>"},{"location":"tutorials/overview/#4-voice-interface","title":"4. Voice Interface","text":"<p>Learn how to implement human-robot interaction through voice commands. This tutorial covers:</p> <ul> <li>Setting up Automatic Speech Recognition (ASR) agent</li> <li>Configuring Text-to-Speech (TTS) agent</li> <li>Running a complete speech-to-speech communication example</li> </ul>"},{"location":"tutorials/overview/#5-benchmarking","title":"5. Benchmarking","text":"<p>Guide to running benchmarks, testing your models, creating new tasks and visualising results. This tutorial covers:</p> <ul> <li>Running benchmarks with predefined tasks.</li> <li>Interpreting and visualizing the results.</li> <li>Creating your own tasks and scenarios!</li> </ul>"},{"location":"tutorials/overview/#getting-started","title":"Getting Started","text":"<p>To get started with RAI, we recommend following these tutorials in the following order:</p> <ol> <li>Begin with the Walkthrough to understand the basic concepts and create your first agent</li> <li>Learn about Tools to understand how to extend your agent's capabilities</li> <li>Configure your robot's identity using Create Robot's Whoami</li> <li>Add voice interaction capabilities using the Voice Interface tutorial</li> <li>Test your models with Benchmarks</li> </ol> <p>Each tutorial includes practical examples and code snippets to help you implement the concepts in your own projects.</p>"},{"location":"tutorials/tools/","title":"Tool use and development","text":"<p>Tools are a fundamental concept in LangChain that allow AI models to interact with external systems and perform specific operations. Think of tools as callable functions that bridge the gap between natural language understanding and system execution.</p> <p>RAI offers a comprehensive set of pre-built tools, including both general-purpose and ROS 2-specific tools here. However, in some cases, you may need to develop custom tools tailored to specific robots or applications. This guide demonstrates how to create custom tools in RAI using the LangChain framework.</p>"},{"location":"tutorials/tools/#how-llms-understand-tools","title":"How LLMs Understand Tools","text":"<p>When an LLM is given access to tools, it needs to understand:</p> <ol> <li>What tools are available</li> <li>What each tool does</li> <li>What inputs each tool accepts</li> </ol> <p>This understanding is achieved through the tool's metadata fields, which are automatically processed by LangChain and made available to the LLM. The LLM uses this information to:</p> <ul> <li>Determine which tool to use for a given task</li> <li>Format the correct input arguments</li> <li>Interpret the tool's output</li> </ul> <p>The key fields that enable this understanding are:</p> <ul> <li><code>name</code>: A unique identifier for the tool, does not have to be equal to the function/class name</li> <li><code>description</code>: A natural language explanation of what the tool does</li> <li><code>args_schema</code>: A structured definition of the tool's input parameters</li> </ul> <p>RAI supports two primary approaches for implementing tools, each with distinct advantages:</p>"},{"location":"tutorials/tools/#basetool-class","title":"<code>BaseTool</code> Class","text":"<ul> <li>Offers full control over tool behavior and lifecycle</li> <li>Allows configuration parameters</li> <li>Supports stateful operations (e.g., maintaining ROS 2 connector instances)</li> </ul>"},{"location":"tutorials/tools/#tool-decorator","title":"<code>@tool</code> Decorator","text":"<ul> <li>Provides a lightweight, functional approach</li> <li>Ideal for stateless operations</li> <li>Minimizes boilerplate code</li> <li>Suited for simple, single-purpose tools</li> </ul> <p>Use the <code>BaseTool</code> class when state management, or extensive configuration is required. Choose the <code>@tool</code> decorator for simple, stateless functionality where conciseness is preferred.</p>"},{"location":"tutorials/tools/#creating-a-custom-tool","title":"Creating a Custom Tool","text":"<p>LangChain tools typically return either a string or a tuple containing a string and an artifact.</p> <p>RAI extends LangChain's tool capabilities by supporting multimodal tools\u2014tools that return not only text but also other content types, such as images, audio, or structured data. This is achieved using a special object called <code>MultimodalArtifact</code> along with a custom <code>ToolRunner</code> class.</p>"},{"location":"tutorials/tools/#single-modal-tool-text-output","title":"Single-Modal Tool (Text Output)","text":"<p>Here's an example of a single-modal tool implemented using class inheritance:</p> <p>Class-based tools</p> <p>Class-based tools provide more control and flexibility compared to function-based tools:</p> <ul> <li>Allow passing additional parameters (e.g., connectors, configuration)</li> <li>Support stateful operations</li> </ul> <pre><code>from langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\nfrom typing import Type\n\n\nclass GrabObjectToolInput(BaseModel):\n    \"\"\"Input schema for the GrabObjectTool.\n\n    This schema defines the expected input parameters for the tool.\n    The Field class provides additional metadata that helps the LLM understand\n    the parameter's purpose and constraints.\n    Fields must include a type annotation.\n    \"\"\"\n    object_name: str = Field(description=\"The name of the object to grab\")\n\n\nclass GrabObjectTool(BaseTool):\n    \"\"\"Tool for grabbing objects using a robot.\n\n    The following fields are crucial for LLM understanding:\n\n    name: A unique identifier that the LLM uses to reference this tool\n    description: A natural language explanation that helps the LLM understand when to use this tool\n    args_schema: Links to the input schema, helping the LLM understand required parameters\n    \"\"\"\n    name: str = \"grab_object\"\n    description: str = \"Grabs a specified object using the robot's manipulator\"\n    args_schema: Type[GrabObjectToolInput] = GrabObjectToolInput\n\n    robot_arm: RobotArm # custom parameter for dependency injection\n\n    def _run(self, object_name: str) -&gt; str:\n        \"\"\"Execute the object grabbing operation.\n\n        The LLM will receive this output and can use it to:\n        1. Determine if the operation was successful\n        2. Extract relevant information\n        3. Decide on next steps\n        \"\"\"\n        try:\n            status = self.robot_arm.grab_object(object_name)\n            return f\"Successfully grabbed object: {object_name}, status: {status}\"\n        except Exception as e:\n            return f\"Failed to grab object: {object_name}, error: {str(e)}\"\n</code></pre> <p>Alternatively, using the <code>@tool</code> decorator for simpler, stateless operations:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef grab_object(object_name: str) -&gt; str:  # function name is equal to name in class API\n    \"\"\"Grabs a specified object using the robot's manipulator.\"\"\"  # equal to description in class API\n    try:\n        status = robot_arm.grab_object(object_name)\n        return f\"Successfully grabbed object: {object_name}, status: {status}\"\n    except Exception as e:\n        return f\"Failed to grab object: {object_name}, error: {str(e)}\"\n</code></pre>"},{"location":"tutorials/tools/#multimodal-tool-text-image-output","title":"Multimodal Tool (Text + Image Output)","text":"<p>RAI supports multimodal tools through the <code>rai.agents.langchain.core.ToolRunner</code> class. These tools must use this runner either directly or via agents such as create_react_runnable to handle multimedia output correctly.</p> <pre><code>from langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\nfrom typing import Type, Tuple\nfrom rai.messages import MultimodalArtifact\n\n\nclass Get360ImageToolInput(BaseModel):\n    \"\"\"Input schema for the Get360ImageTool.\"\"\"\n    topic: str = Field(description=\"The topic name for the 360 image\")\n\n\nclass Get360ImageTool(BaseTool):\n    \"\"\"Tool for retrieving 360-degree images.\"\"\"\n    name: str = \"get_360_image\"\n    description: str = \"Retrieves a 360-degree image from the specified topic\"\n    args_schema: Type[Get360ImageToolInput] = Get360ImageToolInput\n    response_format: str = \"content_and_artifact\"\n\n    def _run(self, topic: str) -&gt; Tuple[str, MultimodalArtifact]:\n        try:\n            image = robot.get_360_image(topic)\n            return \"Successfully retrieved 360 image\", MultimodalArtifact(images=[image])\n        except Exception as e:\n            return f\"Failed to retrieve image: {str(e)}\", MultimodalArtifact(images=[])\n</code></pre>"},{"location":"tutorials/tools/#ros-2-tools","title":"ROS 2 Tools","text":"<p>RAI includes a base class for ROS 2 tools, supporting configuration of readable, writable, and forbidden topics/actions/services, as well as ROS 2 connector.</p> <pre><code>from rai.tools.ros2.base import BaseROS2Tool\nfrom pydantic import BaseModel, Field\nfrom typing import Type, cast\nfrom sensor_msgs.msg import PointCloud2\n\n\nclass GetROS2LidarDataToolInput(BaseModel):\n    \"\"\"Input schema for the GetROS2LidarDataTool.\"\"\"\n    topic: str = Field(description=\"The topic name for the LiDAR data\")\n\n\nclass GetROS2LidarDataTool(BaseROS2Tool):\n    \"\"\"Tool for retrieving and processing LiDAR data.\"\"\"\n    name: str = \"get_ros2_lidar_data\"\n    description: str = \"Retrieves and processes LiDAR data from the specified topic\"\n    args_schema: Type[GetROS2LidarDataToolInput] = GetROS2LidarDataToolInput\n\n    def _run(self, topic: str) -&gt; str:\n        try:\n            lidar_data = self.connector.receive_message(topic)\n            msg = cast(PointCloud2, lidar_data.payload)\n            # Process the LiDAR data\n            return f\"Successfully processed LiDAR data. Detected objects: ...\"\n        except Exception as e:\n            return f\"Failed to process LiDAR data: {str(e)}\"\n</code></pre> For more information on BaseROS2Tool refer to the source code"},{"location":"tutorials/tools/#rai.tools.ros2.base.BaseROS2Tool","title":"<code>rai.tools.ros2.base.BaseROS2Tool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>Base class for all ROS2 tools.</p> <p>Attributes:</p> Name Type Description <code>connector</code> <code>ROS2Connector</code> <p>The connector to the ROS 2 system.</p> <code>readable</code> <code>Optional[List[str]]</code> <p>The topics that can be read. If the list is not provided, all topics can be read.</p> <code>writable</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that can be written. If the list is not provided, all topics can be written.</p> <code>forbidden</code> <code>Optional[List[str]]</code> <p>The names (topics/actions/services) that are forbidden to read and write.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>The description of the tool.</p> Source code in <code>rai/tools/ros2/base.py</code> <pre><code>class BaseROS2Tool(BaseTool):\n    \"\"\"\n    Base class for all ROS2 tools.\n\n    Attributes\n    ----------\n    connector : ROS2Connector\n        The connector to the ROS 2 system.\n    readable : Optional[List[str]]\n        The topics that can be read. If the list is not provided, all topics can be read.\n    writable : Optional[List[str]]\n        The names (topics/actions/services) that can be written. If the list is not provided, all topics can be written.\n    forbidden : Optional[List[str]]\n        The names (topics/actions/services) that are forbidden to read and write.\n    name : str\n        The name of the tool.\n    description : str\n        The description of the tool.\n    \"\"\"\n\n    connector: ROS2Connector\n    readable: Optional[List[str]] = None\n    writable: Optional[List[str]] = None\n    forbidden: Optional[List[str]] = None\n\n    name: str = \"\"\n    description: str = \"\"\n\n    def is_readable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.readable is None:\n            return True\n        return topic in self.readable\n\n    def is_writable(self, topic: str) -&gt; bool:\n        if self.forbidden is not None and topic in self.forbidden:\n            return False\n        if self.writable is None:\n            return True\n        return topic in self.writable\n</code></pre>"},{"location":"tutorials/tools/#tool-initialization","title":"Tool Initialization","text":"<p>Tools can be initialized with parameters such as a connector, enabling custom configurations for ROS 2 environments.</p> <pre><code>from rai.communication.ros2 import ROS2Connector\nfrom rai.tools.ros2 import (\n    GetROS2ImageTool,\n    GetROS2TopicsNamesAndTypesTool,\n    PublishROS2MessageTool,\n)\n\ndef initialize_tools(connector: ROS2Connector):\n    \"\"\"Initialize and configure ROS 2 tools.\n\n    Returns:\n        list: A list of configured tools.\n    \"\"\"\n    readable_names = [\"/color_image5\", \"/depth_image5\", \"/color_camera_info5\"]\n    forbidden_names = [\"cmd_vel\"]\n    writable_names = [\"/to_human\"]\n\n    return [\n        GetROS2ImageTool(\n            connector=connector, readable=readable_names, forbidden=forbidden_names\n        ),\n        GetROS2TopicsNamesAndTypesTool(\n            connector=connector,\n            readable=readable_names,\n            forbidden=forbidden_names,\n            writable=writable_names,\n        ),\n        PublishROS2MessageTool(\n            connector=connector, writable=writable_names, forbidden=forbidden_names\n        ),\n    ]\n</code></pre>"},{"location":"tutorials/tools/#using-tools-in-a-rai-agent-distributed-setup","title":"Using Tools in a RAI Agent (Distributed Setup)","text":"<pre><code>from rai.agents import ReActAgent\nfrom rai.communication import ROS2Connector, ROS2HRIConnector\nfrom rai.tools.ros2 import ROS2Toolkit\nfrom rai.communication.ros2 import ROS2Context\nfrom rai import AgentRunner\n\n@ROS2Context()\ndef main() -&gt; None:\n    \"\"\"Initialize and run the RAI agent with configured tools.\"\"\"\n    connector = ROS2HRIConnector()\n    ros2_connector = ROS2Connector()\n    agent = ReActAgent(\n        connectors={\"/to_human\": connector},\n        tools=initialize_tools(connector=ros2_connector),\n    )\n    runner = AgentRunner([agent])\n    runner.run_and_wait_for_shutdown()\n\n# Example:\n# ros2 topic pub /from_human rai_interfaces/msg/HRIMessage \"{\\\"text\\\": \\\"What do you see?\\\"}\"\n# ros2 topic echo /to_human rai_interfaces/msg/HRIMessage\n</code></pre>"},{"location":"tutorials/tools/#using-tools-in-langchainlanggraph-agent-local-setup","title":"Using Tools in LangChain/LangGraph Agent (Local Setup)","text":"<pre><code>from rai.agents.langchain import create_react_runnable\nfrom langchain.schema import HumanMessage\nfrom rai.communication.ros2 import ROS2Context\n\n@ROS2Context()\ndef main():\n    ros2_connector = ROS2Connector()\n    agent = create_react_runnable(\n        tools=initialize_tools(connector=ros2_connector),\n        system_prompt=\"You are a helpful assistant that can answer questions and help with tasks.\",\n    )\n    state = {'messages': []}\n    while True:\n        input_text = input(\"Enter a prompt: \")\n        state['messages'].append(HumanMessage(content=input_text))\n        response = agent.invoke(state)\n        print(response)\n</code></pre>"},{"location":"tutorials/tools/#related-topics","title":"Related Topics","text":"<ul> <li>Connectors</li> <li>ROS2Connector</li> <li>ROS2HRIConnector</li> </ul>"},{"location":"tutorials/voice_interface/","title":"Human Robot Interface via Voice","text":"<p>RAI provides two ROS enabled agents for Speech to Speech communication.</p>"},{"location":"tutorials/voice_interface/#automatic-speech-recognition-agent","title":"Automatic Speech Recognition Agent","text":"<p>See <code>examples/s2s/asr.py</code> for an example usage.</p> <p>The agent requires configuration of <code>sounddevice</code> and <code>ros2</code> connectors as well as a required voice activity detection (eg. <code>SileroVAD</code>) and transcription model e.g. (<code>LocalWhisper</code>), as well as optionally additional models to decide if the transcription should start (e.g. <code>OpenWakeWord</code>).</p> <p>The Agent publishes information on two topics:</p> <p><code>/from_human</code>: <code>rai_interfaces/msg/HRIMessages</code> - containing transcriptions of the recorded speech</p> <p><code>/voice_commands</code>: <code>std_msgs/msg/String</code> - containing control commands, to inform the consumer if speech is currently detected (<code>{\"data\": \"pause\"}</code>), was detected, and now it stopped (<code>{\"data\": \"play\"}</code>), and if speech was transcribed (<code>{\"data\": \"stop\"}</code>).</p> <p>The Agent utilises sounddevice module to access user's microphone, by default the <code>\"default\"</code> sound device is used. To get information about available sounddevices use:</p> <pre><code>python -c \"import sounddevice; print(sounddevice.query_devices())\"\n</code></pre> <p>The device can be identifed by name and passed to the configuration.</p>"},{"location":"tutorials/voice_interface/#texttospeechagent","title":"TextToSpeechAgent","text":"<p>See <code>examples/s2s/tts.py</code> for an example usage.</p> <p>The agent requires configuration of <code>sounddevice</code> and <code>ros2</code> connectors as well as a required TextToSpeech model (e.g. <code>OpenTTS</code>). The Agent listens for information on two topics:</p> <p><code>/to_human</code>: <code>rai_interfaces/msg/HRIMessages</code> - containing responses to be played to human. These responses are then transcribed and put into the playback queue.</p> <p><code>/voice_commands</code>: <code>std_msgs/msg/String</code> - containing control commands, to pause current playback (<code>{\"data\": \"pause\"}</code>), start/continue playback (<code>{\"data\": \"play\"}</code>), or stop the playback and drop the current playback queue (<code>{\"data\": \"play\"}</code>).</p> <p>The Agent utilises sounddevice module to access user's speaker, by default the <code>\"default\"</code> sound device is used. To get a list of names of available sound devices use:</p> <pre><code>python -c 'import sounddevice as sd; print([x[\"name\"] for x in list(sd.query_devices())])'\n</code></pre> <p>The device can be identifed by name and passed to the configuration.</p>"},{"location":"tutorials/voice_interface/#opentts","title":"OpenTTS","text":"<p>To run OpenTTS (and the example) a docker server containing the model must be running.</p> <p>To start it run:</p> <pre><code>docker run -it -p 5500:5500 synesthesiam/opentts:en --no-espeak\n</code></pre>"},{"location":"tutorials/voice_interface/#running-example","title":"Running example","text":"<p>To run the provided example of S2S configuration with a minimal LLM-based agent run in 4 separate terminals:</p> <pre><code>$ docker run -it -p 5500:5500 synesthesiam/opentts:en --no-espeak\n$ python ./examples/s2s/asr.py\n$ python ./examples/s2s/tts.py\n$ python ./examples/s2s/conversational.py\n</code></pre>"},{"location":"tutorials/walkthrough/","title":"RAI Walkthrough - from zero to a custom agent","text":""},{"location":"tutorials/walkthrough/#overview","title":"Overview","text":"<p>This guide demonstrates how to create and deploy a RAI agent on a ROS 2 enabled robot. You'll learn how to:</p> <ol> <li>Create a new, custom RAI Agent from scratch</li> <li>Implement a platform-specific tool for robot control</li> <li>Build a optimized system prompt using rai whoami</li> <li>Deploy and interact with the agent</li> </ol>"},{"location":"tutorials/walkthrough/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have:</p> <ul> <li>ROS 2 (humble or jazzy) installed and properly set up</li> <li>RAI installed and configured</li> </ul>"},{"location":"tutorials/walkthrough/#creating-a-custom-agent","title":"Creating a Custom Agent","text":"<p>In this section, we'll create a new agent from scratch using the ReAct agent. While pre-built ROS 2-compatible agents exist in RAI, this example will help you understand the underlying architecture and customization options.</p> <p>The underlying ReAct agent combines language models with a set of tools to solve complex problems. Our Agent implementation adds robot status monitoring, which controls agent execution based on the robot's state.</p> PandaAgent implementation<pre><code>from typing import List\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import BaseTool\nfrom rai.agents import BaseAgent\nfrom rai.agents.langchain import (\n    HRICallbackHandler,\n    ReActAgentState,\n    create_react_runnable,\n)\nfrom rai.communication.ros2 import (\n    ROS2Connector,\n    ROS2HRIConnector,\n    ROS2HRIMessage,\n    ROS2Message,\n)\nfrom rai.messages.multimodal import SystemMultimodalMessage\n\n\nclass PandaAgent(BaseAgent):\n    def __init__(\n        self,\n        connector: ROS2Connector,\n        tools: List[BaseTool],\n        system_prompt: SystemMultimodalMessage,\n        input_topic: str,\n        output_topic: str,\n    ):\n        super().__init__()\n        self.connector = connector\n\n        # Initialize the ROS 2 HRI (Human-Robot Interface) connector\n        # This handles bidirectional communication with human-facing topics\n        self.hri_connector = ROS2HRIConnector()\n\n        # Create a ReAct agent with the provided tools and system prompt\n        self.agent = create_react_runnable(\n            tools=tools,\n            system_prompt=system_prompt,\n        )\n        # Initialize the agent's state with an empty message history\n        self.state = ReActAgentState(messages=[])\n\n        # Set up the callback handler to route agent outputs to ROS 2 topics\n        self.callback_handler = HRICallbackHandler(\n            connectors={output_topic: self.hri_connector},\n        )\n\n        # Register the input callback for distributed operation\n        # For simpler setups, you can directly call self.agent.invoke() in a loop\n        self.hri_connector.register_callback(input_topic, self.input_callback)\n\n        # Monitor the robot's status topic to control agent execution\n        # The agent will only process messages when the arm is ready\n        self.arm_ready = False\n        self.connector.register_callback(\"/panda_status\", self.monitor_status_topic)\n\n    def monitor_status_topic(self, msg: ROS2Message):\n        \"\"\"Update the arm readiness status based on the status message.\"\"\"\n        if msg.payload.status == \"ready\":\n            self.arm_ready = True\n        else:\n            self.arm_ready = False\n\n    def input_callback(self, message: ROS2HRIMessage):\n        \"\"\"Process incoming messages when the arm is ready.\"\"\"\n        if self.arm_ready:\n            self.state[\"messages\"].append(message.to_langchain())\n            self.agent.invoke(\n                self.state, config=RunnableConfig(callbacks=[self.callback_handler])\n            )\n        else:\n            self.logger.warning(\"Arm is not ready, skipping the message\")\n\n    def run(self):\n        \"\"\"Main execution loop for the agent.\n\n        This implementation does not require any background processing, so the method is intentionally left empty.\n        \"\"\"\n        pass\n\n    def stop(self):\n        \"\"\"Clean up resources and shut down connections.\"\"\"\n        self.connector.shutdown()\n        self.hri_connector.shutdown()\n</code></pre>"},{"location":"tutorials/walkthrough/#implementing-platform-specific-tools","title":"Implementing Platform-Specific Tools","text":"<p>This section demonstrates how to implement a custom tool for safely shutting down the robot arm. The tool showcases how to integrate platform-specific functionality into the RAI framework.</p> <p>For comprehensive information about tool implementation in RAI, refer to the tools documentation.</p> Tools specific to the robot<pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel, Field\nfrom rai.communication.ros2 import ROS2Message\nfrom rai.tools.ros2.base import BaseROS2Tool\n\n\n# Define the input schema for the tool\nclass ShutdownArmToolInput(BaseModel):\n    level: Literal[\"soft\", \"hard\"] = Field(\n        default=\"soft\",\n        description=\"The level of shutdown to perform\",\n    )\n\n\n# Define the tool\nclass ShutdownArmTool(BaseROS2Tool):\n    \"\"\"Tool for safely shutting down the robot arm.\"\"\"\n    name: str = \"shutdown_arm\"\n    description: str = \"Shutdown the arm\"\n    args_schema: Type[ShutdownArmToolInput] = ShutdownArmToolInput\n\n    def _run(self, level: Literal[\"soft\", \"hard\"]) -&gt; str:\n        \"\"\"Execute the arm shutdown command.\"\"\"\n        response = self.connector.service_call(\n            ROS2Message(\n                payload={\"level\": level}\n            ),\n            target=\"/panda_arm/shutdown\",\n            msg_type=\"rai_interfaces/ShutdownArm\",\n        )\n        if response.payload.success:\n            return \"Arm shutdown successful\"\n        else:\n            return \"Arm shutdown failed\"\n</code></pre>"},{"location":"tutorials/walkthrough/#building-the-system-prompt","title":"Building the System Prompt","text":"<p>The <code>rai whoami</code> utility generates a system prompt based on your robot's specifications. It requires a directory containing:</p> <ul> <li>Documentation</li> <li>Images</li> <li>URDF files</li> </ul> <p>Directories are optional</p> <p>While each directory is optional, providing comprehensive information about your robot will result in a more accurate and effective system prompt.</p> <p>For extended guide on <code>rai whoami</code> see Create Robot's Whoami</p>"},{"location":"tutorials/walkthrough/#setting-up-the-robot-directory","title":"Setting Up the Robot Directory","text":"<p>Create a <code>panda/</code> directory with the following structure:</p> <ol> <li> <p>Images</p> <ul> <li>Save this image in <code>panda/images</code></li> </ul> </li> <li> <p>Documentation</p> <ul> <li>Save   this document   in <code>panda/documentation</code></li> </ul> </li> <li> <p>URDF</p> <ul> <li>Save   this URDF   in <code>panda/urdf</code></li> </ul> </li> </ol>"},{"location":"tutorials/walkthrough/#building-the-whoami","title":"Building the Whoami","text":"<p>Run the following command to build the whoami:</p> <pre><code>python src/rai_whoami/rai_whoami/build_whoami.py panda/ --build-vector-db\n</code></pre>"},{"location":"tutorials/walkthrough/#running-the-agent","title":"Running the Agent","text":"<pre><code>from rai_whoami.models import EmbodimentInfo\nfrom rai.agents import wait_for_shutdown\nfrom rai.communication.ros2 import ROS2Context, ROS2Connector\nfrom rai.tools.ros2 import ROS2Toolkit\n\n\n@ROS2Context()  # Initializes ROS 2 context and ensures proper cleanup on exit\ndef main():\n    # Load the robot's embodiment information from the whoami directory\n    whoami = EmbodimentInfo.from_directory(\"panda/\")\n    connector = ROS2Connector()\n\n    # Initialize tools with ROS 2 communication capabilities\n    # BaseROS2Tools require a ROS2Connector instance for communication\n    tools = [\n        ShutdownArmTool(connector=connector),\n        *ROS2Toolkit(connector=connector).get_tools(),\n    ]\n\n    # Create and configure the agent with all necessary components\n    agent = PandaAgent(\n        connector=connector,\n        tools=tools,\n        system_prompt=whoami.to_langchain(),\n        input_topic=\"/from_human\",\n        output_topic=\"/to_human\",\n    )\n\n    # Start the agent and wait for shutdown signal (Ctrl+C)\n    wait_for_shutdown([agent])\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/walkthrough/#communicating-with-the-agent","title":"Communicating with the Agent","text":"<p>The agent exposes two main communication channels:</p> <ol> <li> <p>Sending Commands</p> <pre><code>ros2 topic pub /from_human rai_interfaces/msg/HRIMessage \"{\\\"text\\\": \\\"Move the arm to 0, 0, 0?\\\"}\"\n</code></pre> </li> <li> <p>Receiving Responses</p> <pre><code>ros2 topic echo /to_human rai_interfaces/msg/HRIMessage\n</code></pre> </li> </ol>"}]}